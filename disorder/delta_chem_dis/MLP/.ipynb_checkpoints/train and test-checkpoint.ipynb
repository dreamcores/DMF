{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6e665f-e083-4d42-8c4d-b2846146aab1",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4324e-68a4-4f9c-9b38-abfdcbf27026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset1(Dataset):\n",
    "    def __init__(self, input_file='conductance_datasets.npy', label_file='Y_labels.npy'):\n",
    "        datas = np.load(input_file)\n",
    "        self.ori_in_shape = datas.shape\n",
    "        labels = np.load(label_file)\n",
    "        self.out_shape = labels.shape\n",
    "        datas = datas.reshape(labels.shape[0], -1)\n",
    "        self.in_shape = datas.shape\n",
    "        self.datas = datas\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input\": torch.from_numpy(self.datas[idx]).float(), \"label\":  torch.from_numpy(self.labels[idx]).float()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = MyDataset1(input_file='../conductance_datasets.npy', label_file='../Y_labels.npy')\n",
    "    train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    for i, d in enumerate(train_dataloader):\n",
    "        print(i, d['input'].shape, d['label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb8eba-3c2b-4e0a-a811-88a3317266a5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86dc10e-a8e6-439e-870a-ec219cd59368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self, seq_num=4500, out_dim=5, hidden_dim=1024) -> None:\n",
    "        super(MyNet, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(seq_num, hidden_dim), torch.nn.ReLU6(), torch.nn.BatchNorm1d(hidden_dim),\n",
    "                                       torch.nn.Linear(hidden_dim, hidden_dim // 2), torch.nn.ReLU6(), torch.nn.BatchNorm1d(hidden_dim // 2),\n",
    "                                       torch.nn.Linear(hidden_dim // 2, hidden_dim // 4), torch.nn.ReLU6(), torch.nn.BatchNorm1d(hidden_dim // 4),\n",
    "                                       torch.nn.Linear(hidden_dim // 4, hidden_dim // 8), torch.nn.ReLU6(), torch.nn.BatchNorm1d(hidden_dim // 8),\n",
    "                                       torch.nn.Linear(hidden_dim // 8, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x.squeeze())\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    net = MyNet(seq_num=4500).cuda()\n",
    "    # input = torch.rand([16, 4500]).cuda()\n",
    "    # out = net(input)\n",
    "    # print(out.shape)\n",
    "\n",
    "    # 输出各层的参数数量\n",
    "    for name, param in net.named_parameters():\n",
    "        print(f\"Layer: {name}, Parameters: {param.numel()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cc707-b33a-47ec-a6b5-ccfbb81a01dd",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "216d735d-b160-4d80-baa0-0883e890561c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1.251953363418579 1.1906276941299438 0.0001\n",
      "save model\n",
      "Epoch 1: 0.9653822779655457 1.1876380443572998 0.0001\n",
      "save model\n",
      "Epoch 2: 0.8310081958770752 1.1843574047088623 0.0001\n",
      "save model\n",
      "Epoch 3: 0.7438316345214844 1.1806236505508423 0.0001\n",
      "save model\n",
      "Epoch 4: 0.6741254925727844 1.1764767169952393 0.0001\n",
      "save model\n",
      "Epoch 5: 0.6137790083885193 1.171654462814331 0.0001\n",
      "save model\n",
      "Epoch 6: 0.5664411187171936 1.1660360097885132 0.0001\n",
      "save model\n",
      "Epoch 7: 0.5265085697174072 1.1596918106079102 0.0001\n",
      "save model\n",
      "Epoch 8: 0.49061957001686096 1.152283787727356 0.0001\n",
      "save model\n",
      "Epoch 9: 0.45945292711257935 1.1435526609420776 0.0001\n",
      "save model\n",
      "Epoch 10: 0.4300873875617981 1.1335148811340332 0.0001\n",
      "save model\n",
      "Epoch 11: 0.4030851423740387 1.122032880783081 0.0001\n",
      "save model\n",
      "Epoch 12: 0.3788587152957916 1.1085395812988281 0.0001\n",
      "save model\n",
      "Epoch 13: 0.357928067445755 1.0928807258605957 0.0001\n",
      "save model\n",
      "Epoch 14: 0.33894872665405273 1.0758581161499023 0.0001\n",
      "save model\n",
      "Epoch 15: 0.32180720567703247 1.0576547384262085 0.0001\n",
      "save model\n",
      "Epoch 16: 0.3064229190349579 1.0372424125671387 0.0001\n",
      "save model\n",
      "Epoch 17: 0.2919962704181671 1.0143226385116577 0.0001\n",
      "save model\n",
      "Epoch 18: 0.2786347568035126 0.9906882643699646 0.0001\n",
      "save model\n",
      "Epoch 19: 0.26612967252731323 0.9667569994926453 0.0001\n",
      "save model\n",
      "Epoch 20: 0.25512874126434326 0.9421617984771729 0.0001\n",
      "save model\n",
      "Epoch 21: 0.24490201473236084 0.9167411923408508 0.0001\n",
      "save model\n",
      "Epoch 22: 0.23514984548091888 0.8896613121032715 0.0001\n",
      "save model\n",
      "Epoch 23: 0.22610731422901154 0.8607107400894165 0.0001\n",
      "save model\n",
      "Epoch 24: 0.21743114292621613 0.8307856917381287 0.0001\n",
      "save model\n",
      "Epoch 25: 0.20894595980644226 0.7983371019363403 0.0001\n",
      "save model\n",
      "Epoch 26: 0.2009916752576828 0.764735996723175 0.0001\n",
      "save model\n",
      "Epoch 27: 0.19395677745342255 0.7308239340782166 0.0001\n",
      "save model\n",
      "Epoch 28: 0.18687018752098083 0.6964941024780273 0.0001\n",
      "save model\n",
      "Epoch 29: 0.1803549826145172 0.6628029942512512 0.0001\n",
      "save model\n",
      "Epoch 30: 0.17448687553405762 0.6250699758529663 0.0001\n",
      "save model\n",
      "Epoch 31: 0.1689101755619049 0.59518963098526 0.0001\n",
      "save model\n",
      "Epoch 32: 0.16390147805213928 0.5574227571487427 0.0001\n",
      "save model\n",
      "Epoch 33: 0.15864375233650208 0.5319194197654724 0.0001\n",
      "save model\n",
      "Epoch 34: 0.15346355736255646 0.4993722438812256 0.0001\n",
      "save model\n",
      "Epoch 35: 0.14807696640491486 0.47163715958595276 0.0001\n",
      "save model\n",
      "Epoch 36: 0.14313814043998718 0.44805213809013367 0.0001\n",
      "save model\n",
      "Epoch 37: 0.13901764154434204 0.4227343499660492 0.0001\n",
      "save model\n",
      "Epoch 38: 0.1362745612859726 0.40849846601486206 0.0001\n",
      "save model\n",
      "Epoch 39: 0.13647796213626862 0.37520575523376465 0.0001\n",
      "save model\n",
      "Epoch 40: 0.13362079858779907 0.36874115467071533 0.0001\n",
      "save model\n",
      "Epoch 41: 0.1253911554813385 0.33832964301109314 0.0001\n",
      "save model\n",
      "Epoch 42: 0.1221037209033966 0.3220900893211365 0.0001\n",
      "save model\n",
      "Epoch 43: 0.12197080999612808 0.31999680399894714 0.0001\n",
      "save model\n",
      "Epoch 44: 0.11898938566446304 0.28922781348228455 0.0001\n",
      "save model\n",
      "Epoch 45: 0.11533136665821075 0.302871435880661 0.0001\n",
      "Epoch 46: 0.11809418350458145 0.2844730317592621 0.0001\n",
      "save model\n",
      "Epoch 47: 0.11473424732685089 0.262869656085968 0.0001\n",
      "save model\n",
      "Epoch 48: 0.10607887804508209 0.2608118951320648 0.0001\n",
      "save model\n",
      "Epoch 49: 0.10418835282325745 0.25210660696029663 0.0001\n",
      "save model\n",
      "Epoch 50: 0.10450312495231628 0.2516278028488159 0.0001\n",
      "save model\n",
      "Epoch 51: 0.10087379068136215 0.23836000263690948 0.0001\n",
      "save model\n",
      "Epoch 52: 0.09738487005233765 0.23941586911678314 0.0001\n",
      "Epoch 53: 0.09758898615837097 0.23456121981143951 0.0001\n",
      "save model\n",
      "Epoch 54: 0.0934714823961258 0.22723133862018585 0.0001\n",
      "save model\n",
      "Epoch 55: 0.08878583461046219 0.22977426648139954 0.0001\n",
      "Epoch 56: 0.08872979879379272 0.22198925912380219 0.0001\n",
      "save model\n",
      "Epoch 57: 0.09024856239557266 0.2292841225862503 0.0001\n",
      "Epoch 58: 0.08853562921285629 0.222298264503479 0.0001\n",
      "Epoch 59: 0.08686016499996185 0.23456774652004242 0.0001\n",
      "Epoch 60: 0.08719877898693085 0.2107202410697937 0.0001\n",
      "save model\n",
      "Epoch 61: 0.08383610844612122 0.21674911677837372 0.0001\n",
      "Epoch 62: 0.08090204000473022 0.2119358330965042 0.0001\n",
      "Epoch 63: 0.080074243247509 0.21194018423557281 0.0001\n",
      "Epoch 64: 0.08040434122085571 0.21064262092113495 0.0001\n",
      "save model\n",
      "Epoch 65: 0.07854047417640686 0.21486185491085052 0.0001\n",
      "Epoch 66: 0.07742781192064285 0.20662163197994232 0.0001\n",
      "save model\n",
      "Epoch 67: 0.07523557543754578 0.21506215631961823 0.0001\n",
      "Epoch 68: 0.07501707226037979 0.2042117714881897 0.0001\n",
      "save model\n",
      "Epoch 69: 0.07395729422569275 0.20913460850715637 0.0001\n",
      "Epoch 70: 0.0699336975812912 0.20293879508972168 0.0001\n",
      "save model\n",
      "Epoch 71: 0.0718456357717514 0.2116456776857376 0.0001\n",
      "Epoch 72: 0.07378420978784561 0.2017601877450943 0.0001\n",
      "save model\n",
      "Epoch 73: 0.06830182671546936 0.2010219693183899 0.0001\n",
      "save model\n",
      "Epoch 74: 0.06561540067195892 0.20402933657169342 0.0001\n",
      "Epoch 75: 0.065549835562706 0.20448918640613556 0.0001\n",
      "Epoch 76: 0.06519412994384766 0.2026890069246292 0.0001\n",
      "Epoch 77: 0.06296369433403015 0.21163488924503326 0.0001\n",
      "Epoch 78: 0.06747782230377197 0.21131017804145813 0.0001\n",
      "Epoch 79: 0.06727340817451477 0.2022068053483963 0.0001\n",
      "Epoch 80: 0.061583664268255234 0.19487841427326202 0.0001\n",
      "save model\n",
      "Epoch 81: 0.05841413885354996 0.2016156017780304 0.0001\n",
      "Epoch 82: 0.06083289533853531 0.2157236784696579 0.0001\n",
      "Epoch 83: 0.06899035722017288 0.20196892321109772 0.0001\n",
      "Epoch 84: 0.06154659390449524 0.2238352745771408 0.0001\n",
      "Epoch 85: 0.05643421411514282 0.2328113168478012 0.0001\n",
      "Epoch 86: 0.060901444405317307 0.21869218349456787 0.0001\n",
      "Epoch 87: 0.05787908658385277 0.21700029075145721 0.0001\n",
      "Epoch 88: 0.05673472210764885 0.20895516872406006 0.0001\n",
      "Epoch 89: 0.056180987507104874 0.2075052410364151 0.0001\n",
      "Epoch 90: 0.055998045951128006 0.21134425699710846 0.0001\n",
      "Epoch 91: 0.0587807223200798 0.2047896385192871 0.0001\n",
      "Epoch 92: 0.05521661415696144 0.2037963569164276 0.0001\n",
      "Epoch 93: 0.05643254145979881 0.198135107755661 0.0001\n",
      "Epoch 94: 0.05389053747057915 0.19927582144737244 0.0001\n",
      "Epoch 95: 0.05385762080550194 0.20171649754047394 0.0001\n",
      "Epoch 96: 0.05183132737874985 0.19450147449970245 0.0001\n",
      "save model\n",
      "Epoch 97: 0.048726074397563934 0.20608310401439667 0.0001\n",
      "Epoch 98: 0.053105615079402924 0.20400547981262207 0.0001\n",
      "Epoch 99: 0.05472681671380997 0.20195290446281433 0.0001\n",
      "Epoch 100: 0.0510396882891655 0.20010599493980408 0.0001\n",
      "Epoch 101: 0.05083366110920906 0.19480030238628387 0.0001\n",
      "Epoch 102: 0.04780303314328194 0.19561293721199036 0.0001\n",
      "Epoch 103: 0.048695314675569534 0.197938472032547 0.0001\n",
      "Epoch 104: 0.04939434304833412 0.19924838840961456 0.0001\n",
      "Epoch 105: 0.04996054247021675 0.19945824146270752 0.0001\n",
      "Epoch 106: 0.04922518879175186 0.19355131685733795 0.0001\n",
      "save model\n",
      "Epoch 107: 0.04534199833869934 0.1926017701625824 0.0001\n",
      "save model\n",
      "Epoch 108: 0.04213986173272133 0.19299641251564026 0.0001\n",
      "Epoch 109: 0.04315253347158432 0.2000713348388672 0.0001\n",
      "Epoch 110: 0.04842964932322502 0.1933566927909851 0.0001\n",
      "Epoch 111: 0.04571884125471115 0.19368870556354523 0.0001\n",
      "Epoch 112: 0.04329640418291092 0.19432146847248077 0.0001\n",
      "Epoch 113: 0.04388883337378502 0.19716861844062805 0.0001\n",
      "Epoch 114: 0.044008128345012665 0.1941336840391159 0.0001\n",
      "Epoch 115: 0.04549090564250946 0.1980578601360321 0.0001\n",
      "Epoch 116: 0.04276781156659126 0.19768667221069336 0.0001\n",
      "Epoch 117: 0.04192833602428436 0.22129620611667633 0.0001\n",
      "Epoch 118: 0.04598702862858772 0.21907179057598114 0.0001\n",
      "Epoch 119: 0.041081927716732025 0.19792918860912323 0.0001\n",
      "Epoch 120: 0.04287996143102646 0.20202285051345825 0.0001\n",
      "Epoch 121: 0.044055577367544174 0.1969645470380783 0.0001\n",
      "Epoch 122: 0.03894612193107605 0.19466663897037506 0.0001\n",
      "Epoch 123: 0.03982633724808693 0.20205603539943695 0.0001\n",
      "Epoch 124: 0.04018064960837364 0.19109505414962769 0.0001\n",
      "save model\n",
      "Epoch 125: 0.03724781423807144 0.19198428094387054 0.0001\n",
      "Epoch 126: 0.040710363537073135 0.20467598736286163 0.0001\n",
      "Epoch 127: 0.0437215231359005 0.19255954027175903 0.0001\n",
      "Epoch 128: 0.0392533503472805 0.1964428871870041 0.0001\n",
      "Epoch 129: 0.03929651901125908 0.19523102045059204 0.0001\n",
      "Epoch 130: 0.04136721044778824 0.1929830014705658 0.0001\n",
      "Epoch 131: 0.039644163101911545 0.19900506734848022 0.0001\n",
      "Epoch 132: 0.041474319994449615 0.19899645447731018 0.0001\n",
      "Epoch 133: 0.039275914430618286 0.19354073703289032 0.0001\n",
      "Epoch 134: 0.03583158180117607 0.19153741002082825 0.0001\n",
      "Epoch 135: 0.035239774733781815 0.1928206831216812 0.0001\n",
      "Epoch 136: 0.037221893668174744 0.1944391280412674 0.0001\n",
      "Epoch 137: 0.03671989589929581 0.19584515690803528 0.0001\n",
      "Epoch 138: 0.037577565759420395 0.19524788856506348 0.0001\n",
      "Epoch 139: 0.03757762908935547 0.19339017570018768 0.0001\n",
      "Epoch 140: 0.03674451261758804 0.18945010006427765 0.0001\n",
      "save model\n",
      "Epoch 141: 0.03360036388039589 0.1938609629869461 0.0001\n",
      "Epoch 142: 0.037295740097761154 0.1933569312095642 0.0001\n",
      "Epoch 143: 0.03949510306119919 0.19086503982543945 0.0001\n",
      "Epoch 144: 0.03514433652162552 0.19463233649730682 0.0001\n",
      "Epoch 145: 0.03675301745533943 0.19598041474819183 0.0001\n",
      "Epoch 146: 0.03960573673248291 0.18846739828586578 0.0001\n",
      "save model\n",
      "Epoch 147: 0.033406537026166916 0.18968094885349274 0.0001\n",
      "Epoch 148: 0.03136365860700607 0.18883764743804932 0.0001\n",
      "Epoch 149: 0.032544467598199844 0.1904185712337494 0.0001\n",
      "Epoch 150: 0.033819809556007385 0.19034919142723083 0.0001\n",
      "Epoch 151: 0.03395344316959381 0.1926930844783783 0.0001\n",
      "Epoch 152: 0.031813520938158035 0.19155529141426086 0.0001\n",
      "Epoch 153: 0.03262336179614067 0.19091063737869263 0.0001\n",
      "Epoch 154: 0.03138737007975578 0.19215209782123566 0.0001\n",
      "Epoch 155: 0.03398873656988144 0.18988065421581268 0.0001\n",
      "Epoch 156: 0.03224189579486847 0.19181710481643677 0.0001\n",
      "Epoch 157: 0.03063613548874855 0.19508859515190125 0.0001\n",
      "Epoch 158: 0.03174129128456116 0.19061705470085144 0.0001\n",
      "Epoch 159: 0.03129151090979576 0.19043178856372833 0.0001\n",
      "Epoch 160: 0.033191561698913574 0.18823635578155518 0.0001\n",
      "save model\n",
      "Epoch 161: 0.030451839789748192 0.19026973843574524 0.0001\n",
      "Epoch 162: 0.03370431438088417 0.19112113118171692 0.0001\n",
      "Epoch 163: 0.03405585139989853 0.1910073161125183 0.0001\n",
      "Epoch 164: 0.03311273828148842 0.19359418749809265 0.0001\n",
      "Epoch 165: 0.0328776054084301 0.19088317453861237 0.0001\n",
      "Epoch 166: 0.029192473739385605 0.1901792287826538 0.0001\n",
      "Epoch 167: 0.02989473193883896 0.1911569982767105 0.0001\n",
      "Epoch 168: 0.03176817670464516 0.18968485295772552 0.0001\n",
      "Epoch 169: 0.030377056449651718 0.18797613680362701 0.0001\n",
      "save model\n",
      "Epoch 170: 0.031033266335725784 0.18677599728107452 0.0001\n",
      "save model\n",
      "Epoch 171: 0.030809206888079643 0.18977349996566772 0.0001\n",
      "Epoch 172: 0.030390767380595207 0.18810433149337769 0.0001\n",
      "Epoch 173: 0.030219053849577904 0.1914997696876526 0.0001\n",
      "Epoch 174: 0.031984709203243256 0.19000886380672455 0.0001\n",
      "Epoch 175: 0.03140978142619133 0.1876882016658783 0.0001\n",
      "Epoch 176: 0.02860104665160179 0.18710705637931824 0.0001\n",
      "Epoch 177: 0.029413731768727303 0.18631654977798462 0.0001\n",
      "save model\n",
      "Epoch 178: 0.02917923592031002 0.18831071257591248 0.0001\n",
      "Epoch 179: 0.028113415464758873 0.18792878091335297 0.0001\n",
      "Epoch 180: 0.026774870231747627 0.18815447390079498 0.0001\n",
      "Epoch 181: 0.027464689686894417 0.18814371526241302 0.0001\n",
      "Epoch 182: 0.028892571106553078 0.18903371691703796 0.0001\n",
      "Epoch 183: 0.028301909565925598 0.18853312730789185 0.0001\n",
      "Epoch 184: 0.028297768905758858 0.1869252324104309 0.0001\n",
      "Epoch 185: 0.029151611030101776 0.1854356825351715 0.0001\n",
      "save model\n",
      "Epoch 186: 0.027737731114029884 0.18714435398578644 0.0001\n",
      "Epoch 187: 0.027176037430763245 0.1849364936351776 0.0001\n",
      "save model\n",
      "Epoch 188: 0.027867479249835014 0.18805743753910065 0.0001\n",
      "Epoch 189: 0.026115307584404945 0.18663403391838074 0.0001\n",
      "Epoch 190: 0.026589123532176018 0.18726755678653717 0.0001\n",
      "Epoch 191: 0.026733361184597015 0.18689167499542236 0.0001\n",
      "Epoch 192: 0.02731097862124443 0.18749970197677612 0.0001\n",
      "Epoch 193: 0.027427321299910545 0.18718944489955902 0.0001\n",
      "Epoch 194: 0.026473814621567726 0.18724718689918518 0.0001\n",
      "Epoch 195: 0.027120793238282204 0.18662984669208527 0.0001\n",
      "Epoch 196: 0.02668563462793827 0.1916203498840332 0.0001\n",
      "Epoch 197: 0.028080236166715622 0.18879424035549164 0.0001\n",
      "Epoch 198: 0.028040379285812378 0.18703123927116394 0.0001\n",
      "Epoch 199: 0.02524777129292488 0.1870686411857605 0.0001\n",
      "Epoch 200: 0.02644054777920246 0.18969188630580902 0.0001\n",
      "Epoch 201: 0.028032470494508743 0.18861907720565796 0.0001\n",
      "Epoch 202: 0.025069110095500946 0.1865641176700592 0.0001\n",
      "Epoch 203: 0.023562133312225342 0.18783335387706757 0.0001\n",
      "Epoch 204: 0.026600224897265434 0.18701840937137604 0.0001\n",
      "Epoch 205: 0.024622943252325058 0.18566086888313293 0.0001\n",
      "Epoch 206: 0.024828532710671425 0.18658579885959625 0.0001\n",
      "Epoch 207: 0.02618762105703354 0.18510790169239044 0.0001\n",
      "Epoch 208: 0.023276312276721 0.18577155470848083 0.0001\n",
      "Epoch 209: 0.025627920404076576 0.1857411116361618 0.0001\n",
      "Epoch 210: 0.026620274409651756 0.1818450391292572 0.0001\n",
      "save model\n",
      "Epoch 211: 0.023949716240167618 0.1863776594400406 0.0001\n",
      "Epoch 212: 0.0240040086209774 0.18524226546287537 0.0001\n",
      "Epoch 213: 0.02543194405734539 0.18511125445365906 0.0001\n",
      "Epoch 214: 0.023038089275360107 0.1822117418050766 0.0001\n",
      "Epoch 215: 0.023763835430145264 0.18661212921142578 0.0001\n",
      "Epoch 216: 0.02531319111585617 0.18319258093833923 0.0001\n",
      "Epoch 217: 0.024029716849327087 0.1844164878129959 0.0001\n",
      "Epoch 218: 0.022611187770962715 0.18607421219348907 0.0001\n",
      "Epoch 219: 0.024099035188555717 0.18627822399139404 0.0001\n",
      "Epoch 220: 0.02378689870238304 0.18728305399417877 0.0001\n",
      "Epoch 221: 0.02595292031764984 0.18509644269943237 0.0001\n",
      "Epoch 222: 0.025007929652929306 0.18597467243671417 0.0001\n",
      "Epoch 223: 0.0258908923715353 0.19027389585971832 0.0001\n",
      "Epoch 224: 0.02763853780925274 0.18410715460777283 0.0001\n",
      "Epoch 225: 0.021897992119193077 0.18462076783180237 0.0001\n",
      "Epoch 226: 0.02297782711684704 0.1870778650045395 0.0001\n",
      "Epoch 227: 0.026411382481455803 0.18501721322536469 0.0001\n",
      "Epoch 228: 0.022563986480236053 0.18751917779445648 0.0001\n",
      "Epoch 229: 0.023361703380942345 0.1860375702381134 0.0001\n",
      "Epoch 230: 0.025353237986564636 0.18309353291988373 0.0001\n",
      "Epoch 231: 0.021514594554901123 0.18511322140693665 0.0001\n",
      "Epoch 232: 0.024462632834911346 0.18317309021949768 0.0001\n",
      "Epoch 233: 0.02211509272456169 0.185609832406044 0.0001\n",
      "Epoch 234: 0.023169400170445442 0.1846897006034851 0.0001\n",
      "Epoch 235: 0.024043211713433266 0.18445943295955658 0.0001\n",
      "Epoch 236: 0.02253749594092369 0.1844998002052307 0.0001\n",
      "Epoch 237: 0.02292514033615589 0.18715892732143402 0.0001\n",
      "Epoch 238: 0.024261366575956345 0.18683764338493347 0.0001\n",
      "Epoch 239: 0.022708650678396225 0.18589437007904053 0.0001\n",
      "Epoch 240: 0.02273106947541237 0.1870962530374527 0.0001\n",
      "Epoch 241: 0.023078206926584244 0.18677152693271637 0.0001\n",
      "Epoch 242: 0.021993359550833702 0.1851956993341446 0.0001\n",
      "Epoch 243: 0.021229417994618416 0.18654274940490723 0.0001\n",
      "Epoch 244: 0.022055257111787796 0.18580815196037292 0.0001\n",
      "Epoch 245: 0.02304312027990818 0.18329459428787231 0.0001\n",
      "Epoch 246: 0.022662993520498276 0.18743249773979187 0.0001\n",
      "Epoch 247: 0.023924576118588448 0.1842041164636612 0.0001\n",
      "Epoch 248: 0.02308310754597187 0.1837320476770401 0.0001\n",
      "Epoch 249: 0.020134212449193 0.18422242999076843 0.0001\n",
      "Epoch 250: 0.023077495396137238 0.1865805685520172 0.0001\n",
      "Epoch 251: 0.023517398163676262 0.18338020145893097 0.0001\n",
      "Epoch 252: 0.020230254158377647 0.18315958976745605 0.0001\n",
      "Epoch 253: 0.021679963916540146 0.18471188843250275 0.0001\n",
      "Epoch 254: 0.023910874500870705 0.18204420804977417 0.0001\n",
      "Epoch 255: 0.020238708704710007 0.18210971355438232 0.0001\n",
      "Epoch 256: 0.02056022360920906 0.18305037915706635 0.0001\n",
      "Epoch 257: 0.02304176241159439 0.18210232257843018 0.0001\n",
      "Epoch 258: 0.020173821598291397 0.18218165636062622 0.0001\n",
      "Epoch 259: 0.019466591998934746 0.18369098007678986 0.0001\n",
      "Epoch 260: 0.021100368350744247 0.5276103019714355 0.0001\n",
      "Epoch 261: 0.022354431450366974 0.23249667882919312 0.0001\n",
      "Epoch 262: 0.01992058753967285 0.22323165833950043 0.0001\n",
      "Epoch 263: 0.020058901980519295 0.2083992063999176 0.0001\n",
      "Epoch 264: 0.02042950689792633 0.19747601449489594 0.0001\n",
      "Epoch 265: 0.020488906651735306 0.19336025416851044 0.0001\n",
      "Epoch 266: 0.021967748180031776 0.19136381149291992 0.0001\n",
      "Epoch 267: 0.01887679472565651 0.18688105046749115 0.0001\n",
      "Epoch 268: 0.020215705037117004 0.18968816101551056 0.0001\n",
      "Epoch 269: 0.020737215876579285 0.18620333075523376 0.0001\n",
      "Epoch 270: 0.020837392657995224 0.1869887262582779 0.0001\n",
      "Epoch 271: 0.019670048728585243 0.18391378223896027 0.0001\n",
      "Epoch 272: 0.021970484405755997 0.18887485563755035 0.0001\n",
      "Epoch 273: 0.02092314325273037 0.18322131037712097 0.0001\n",
      "Epoch 274: 0.021566689014434814 0.18673673272132874 0.0001\n",
      "Epoch 275: 0.020880047231912613 0.1836089938879013 0.0001\n",
      "Epoch 276: 0.02115282602608204 0.184875950217247 0.0001\n",
      "Epoch 277: 0.019833717495203018 0.18381518125534058 0.0001\n",
      "Epoch 278: 0.019773181527853012 0.18393515050411224 0.0001\n",
      "Epoch 279: 0.020737843587994576 0.1865195631980896 0.0001\n",
      "Epoch 280: 0.019761038944125175 0.18357928097248077 0.0001\n",
      "Epoch 281: 0.01923322305083275 0.18676531314849854 0.0001\n",
      "Epoch 282: 0.0203523151576519 0.18370561301708221 0.0001\n",
      "Epoch 283: 0.02015739306807518 0.18413594365119934 0.0001\n",
      "Epoch 284: 0.019499845802783966 0.18153533339500427 0.0001\n",
      "save model\n",
      "Epoch 285: 0.020723726600408554 0.18361328542232513 0.0001\n",
      "Epoch 286: 0.020862393081188202 0.18268877267837524 0.0001\n",
      "Epoch 287: 0.01978807896375656 0.1853029578924179 0.0001\n",
      "Epoch 288: 0.022413277998566628 0.1835775077342987 0.0001\n",
      "Epoch 289: 0.020638206973671913 0.1833260953426361 0.0001\n",
      "Epoch 290: 0.01862921193242073 0.18285681307315826 0.0001\n",
      "Epoch 291: 0.02026120200753212 0.18384647369384766 0.0001\n",
      "Epoch 292: 0.020902534946799278 0.18248392641544342 0.0001\n",
      "Epoch 293: 0.019034475088119507 0.18151229619979858 0.0001\n",
      "save model\n",
      "Epoch 294: 0.019570868462324142 0.1808430552482605 0.0001\n",
      "save model\n",
      "Epoch 295: 0.019950782880187035 0.18186545372009277 0.0001\n",
      "Epoch 296: 0.01958753727376461 0.18421471118927002 0.0001\n",
      "Epoch 297: 0.020382069051265717 0.1820315420627594 0.0001\n",
      "Epoch 298: 0.02086677774786949 0.18346984684467316 0.0001\n",
      "Epoch 299: 0.019106967374682426 0.18053151667118073 0.0001\n",
      "save model\n",
      "Epoch 300: 0.02003321424126625 0.18281903862953186 0.0001\n",
      "Epoch 301: 0.019384387880563736 0.1825411170721054 0.0001\n",
      "Epoch 302: 0.01838892139494419 0.18178997933864594 0.0001\n",
      "Epoch 303: 0.019344661384820938 0.18167248368263245 0.0001\n",
      "Epoch 304: 0.019124947488307953 0.18229220807552338 0.0001\n",
      "Epoch 305: 0.018917784094810486 0.18450380861759186 0.0001\n",
      "Epoch 306: 0.01935872808098793 0.18142133951187134 0.0001\n",
      "Epoch 307: 0.019827893003821373 0.1845254898071289 0.0001\n",
      "Epoch 308: 0.018499383702874184 0.18230478465557098 0.0001\n",
      "Epoch 309: 0.019635993987321854 0.18208962678909302 0.0001\n",
      "Epoch 310: 0.0179995559155941 0.1813237965106964 0.0001\n",
      "Epoch 311: 0.019249485805630684 0.1838248074054718 0.0001\n",
      "Epoch 312: 0.020508553832769394 0.18119892477989197 0.0001\n",
      "Epoch 313: 0.017749400809407234 0.18512685596942902 0.0001\n",
      "Epoch 314: 0.019533922895789146 0.1815994381904602 0.0001\n",
      "Epoch 315: 0.019578447565436363 0.18248224258422852 0.0001\n",
      "Epoch 316: 0.017456647008657455 0.18035152554512024 0.0001\n",
      "save model\n",
      "Epoch 317: 0.018012885004281998 0.18408522009849548 0.0001\n",
      "Epoch 318: 0.01995980180799961 0.18123628199100494 0.0001\n",
      "Epoch 319: 0.0178398285061121 0.18110287189483643 0.0001\n",
      "Epoch 320: 0.01860128715634346 0.1812676191329956 0.0001\n",
      "Epoch 321: 0.020282579585909843 0.18137209117412567 0.0001\n",
      "Epoch 322: 0.018229013308882713 0.18202075362205505 0.0001\n",
      "Epoch 323: 0.018559085205197334 0.18396753072738647 0.0001\n",
      "Epoch 324: 0.01918862573802471 0.1814395934343338 0.0001\n",
      "Epoch 325: 0.01908058300614357 0.18290454149246216 0.0001\n",
      "Epoch 326: 0.01872595027089119 0.18191805481910706 0.0001\n",
      "Epoch 327: 0.017183642834424973 0.18086068332195282 0.0001\n",
      "Epoch 328: 0.017513427883386612 0.18320372700691223 0.0001\n",
      "Epoch 329: 0.018393097445368767 0.18287767469882965 0.0001\n",
      "Epoch 330: 0.019160309806466103 0.18186916410923004 0.0001\n",
      "Epoch 331: 0.01776440255343914 0.18213728070259094 0.0001\n",
      "Epoch 332: 0.018116803839802742 0.18136325478553772 0.0001\n",
      "Epoch 333: 0.0180865079164505 0.18110038340091705 0.0001\n",
      "Epoch 334: 0.018055172637104988 0.1807965636253357 0.0001\n",
      "Epoch 335: 0.016985973343253136 0.18217231333255768 0.0001\n",
      "Epoch 336: 0.019084710627794266 0.1835446059703827 0.0001\n",
      "Epoch 337: 0.018832094967365265 0.1814790517091751 0.0001\n",
      "Epoch 338: 0.017505504190921783 0.18180714547634125 0.0001\n",
      "Epoch 339: 0.018407635390758514 0.1822255551815033 0.0001\n",
      "Epoch 340: 0.017992813140153885 0.18092414736747742 0.0001\n",
      "Epoch 341: 0.01743779331445694 0.18223582208156586 0.0001\n",
      "Epoch 342: 0.017896803095936775 0.732163667678833 0.0001\n",
      "Epoch 343: 0.02025441825389862 0.24801181256771088 0.0001\n",
      "Epoch 344: 0.016197457909584045 0.2431771159172058 0.0001\n",
      "Epoch 345: 0.01592186838388443 0.22112280130386353 0.0001\n",
      "Epoch 346: 0.01887863501906395 0.2085377424955368 0.0001\n",
      "Epoch 347: 0.01738695055246353 0.2006237804889679 0.0001\n",
      "Epoch 348: 0.017137978225946426 0.19506658613681793 0.0001\n",
      "Epoch 349: 0.016142956912517548 0.18866311013698578 0.0001\n",
      "Epoch 350: 0.01761966571211815 0.19070617854595184 0.0001\n",
      "Epoch 351: 0.017381511628627777 0.18431638181209564 0.0001\n",
      "Epoch 352: 0.01647374778985977 0.1872667372226715 0.0001\n",
      "Epoch 353: 0.017657900229096413 0.1852349042892456 0.0001\n",
      "Epoch 354: 0.01798878237605095 0.185755655169487 0.0001\n",
      "Epoch 355: 0.017372939735651016 0.18225370347499847 0.0001\n",
      "Epoch 356: 0.016764678061008453 0.183989480137825 0.0001\n",
      "Epoch 357: 0.01775023341178894 0.18294523656368256 0.0001\n",
      "Epoch 358: 0.01896667666733265 0.1833072304725647 0.0001\n",
      "Epoch 359: 0.01725398562848568 0.18233174085617065 0.0001\n",
      "Epoch 360: 0.017483409494161606 0.18233433365821838 0.0001\n",
      "Epoch 361: 0.018263190984725952 0.1809757947921753 0.0001\n",
      "Epoch 362: 0.016735265031456947 0.18204671144485474 0.0001\n",
      "Epoch 363: 0.016926342621445656 0.18176637589931488 0.0001\n",
      "Epoch 364: 0.017902467399835587 0.18170180916786194 0.0001\n",
      "Epoch 365: 0.016055669635534286 0.18032880127429962 0.0001\n",
      "save model\n",
      "Epoch 366: 0.016964660957455635 0.18098856508731842 0.0001\n",
      "Epoch 367: 0.017421722412109375 0.180564284324646 0.0001\n",
      "Epoch 368: 0.015611529350280762 0.18070337176322937 0.0001\n",
      "Epoch 369: 0.018386512994766235 0.1818036586046219 0.0001\n",
      "Epoch 370: 0.0173978079110384 0.18197020888328552 0.0001\n",
      "Epoch 371: 0.01787390373647213 0.18060816824436188 0.0001\n",
      "Epoch 372: 0.018221864476799965 0.18000635504722595 0.0001\n",
      "save model\n",
      "Epoch 373: 0.016719140112400055 0.18002921342849731 0.0001\n",
      "Epoch 374: 0.017365023493766785 0.18053248524665833 0.0001\n",
      "Epoch 375: 0.01696949452161789 0.18093043565750122 0.0001\n",
      "Epoch 376: 0.017352810129523277 0.18044528365135193 0.0001\n",
      "Epoch 377: 0.017120972275733948 0.18037965893745422 0.0001\n",
      "Epoch 378: 0.016508864238858223 0.17929765582084656 0.0001\n",
      "save model\n",
      "Epoch 379: 0.017067350447177887 0.18279609084129333 0.0001\n",
      "Epoch 380: 0.017240796238183975 0.17957881093025208 0.0001\n",
      "Epoch 381: 0.017398901283740997 0.1812644600868225 0.0001\n",
      "Epoch 382: 0.01623951643705368 0.18005910515785217 0.0001\n",
      "Epoch 383: 0.0166946854442358 0.18325215578079224 0.0001\n",
      "Epoch 384: 0.01692410372197628 0.18093937635421753 0.0001\n",
      "Epoch 385: 0.01670316606760025 0.18200817704200745 0.0001\n",
      "Epoch 386: 0.01646791771054268 0.1805790215730667 0.0001\n",
      "Epoch 387: 0.017030535265803337 0.18235836923122406 0.0001\n",
      "Epoch 388: 0.01681692712008953 0.17970742285251617 0.0001\n",
      "Epoch 389: 0.01821843907237053 0.18095053732395172 0.0001\n",
      "Epoch 390: 0.015767406672239304 0.18059875071048737 0.0001\n",
      "Epoch 391: 0.017591215670108795 0.18353497982025146 0.0001\n",
      "Epoch 392: 0.017190448939800262 0.17909425497055054 0.0001\n",
      "save model\n",
      "Epoch 393: 0.015986373648047447 0.18078972399234772 0.0001\n",
      "Epoch 394: 0.017105374485254288 0.17891542613506317 0.0001\n",
      "save model\n",
      "Epoch 395: 0.017165532335639 0.17955651879310608 0.0001\n",
      "Epoch 396: 0.016085829585790634 0.17828558385372162 0.0001\n",
      "save model\n",
      "Epoch 397: 0.01735672354698181 0.1790497899055481 0.0001\n",
      "Epoch 398: 0.01619594916701317 0.17869633436203003 0.0001\n",
      "Epoch 399: 0.016042077913880348 0.1792658120393753 0.0001\n",
      "Epoch 400: 0.01679764688014984 0.1796570122241974 0.0001\n",
      "Epoch 401: 0.015648767352104187 0.1801518350839615 0.0001\n",
      "Epoch 402: 0.01707886904478073 0.17974205315113068 0.0001\n",
      "Epoch 403: 0.016421783715486526 0.1790028065443039 0.0001\n",
      "Epoch 404: 0.016680503264069557 0.17922630906105042 0.0001\n",
      "Epoch 405: 0.017541637644171715 0.18013563752174377 0.0001\n",
      "Epoch 406: 0.01617826521396637 0.1807953268289566 0.0001\n",
      "Epoch 407: 0.01685105636715889 0.18142274022102356 0.0001\n",
      "Epoch 408: 0.01572793535888195 0.17929388582706451 0.0001\n",
      "Epoch 409: 0.01652815006673336 0.18084755539894104 0.0001\n",
      "Epoch 410: 0.0158691368997097 0.17896708846092224 0.0001\n",
      "Epoch 411: 0.016913635656237602 0.17985107004642487 0.0001\n",
      "Epoch 412: 0.016246993094682693 0.17740212380886078 0.0001\n",
      "save model\n",
      "Epoch 413: 0.015777893364429474 0.18088023364543915 0.0001\n",
      "Epoch 414: 0.016603631898760796 0.178618922829628 0.0001\n",
      "Epoch 415: 0.015602575615048409 0.18169991672039032 0.0001\n",
      "Epoch 416: 0.016293104737997055 0.18041031062602997 0.0001\n",
      "Epoch 417: 0.01604718342423439 0.17904144525527954 0.0001\n",
      "Epoch 418: 0.01499879453331232 0.18049359321594238 0.0001\n",
      "Epoch 419: 0.016505718231201172 0.18001751601696014 0.0001\n",
      "Epoch 420: 0.01528446190059185 0.17894907295703888 0.0001\n",
      "Epoch 421: 0.016118008643388748 0.17958354949951172 0.0001\n",
      "Epoch 422: 0.017048021778464317 0.17754171788692474 0.0001\n",
      "Epoch 423: 0.014260988682508469 0.17979784309864044 0.0001\n",
      "Epoch 424: 0.016012733802199364 0.17839767038822174 0.0001\n",
      "Epoch 425: 0.0158411655575037 0.18011219799518585 0.0001\n",
      "Epoch 426: 0.016157526522874832 0.17942006886005402 0.0001\n",
      "Epoch 427: 0.015668991953134537 0.17910747230052948 0.0001\n",
      "Epoch 428: 0.01505283173173666 0.18019355833530426 0.0001\n",
      "Epoch 429: 0.015597588382661343 0.17917661368846893 0.0001\n",
      "Epoch 430: 0.015411119908094406 0.17957361042499542 0.0001\n",
      "Epoch 431: 0.015528218820691109 0.1789550483226776 0.0001\n",
      "Epoch 432: 0.01585596799850464 0.1787182092666626 0.0001\n",
      "Epoch 433: 0.014431958086788654 0.18010641634464264 0.0001\n",
      "Epoch 434: 0.016132785007357597 0.17951498925685883 0.0001\n",
      "Epoch 435: 0.01546523068100214 0.17964570224285126 0.0001\n",
      "Epoch 436: 0.015817804262042046 0.17691130936145782 0.0001\n",
      "save model\n",
      "Epoch 437: 0.014890412800014019 0.1786780059337616 0.0001\n",
      "Epoch 438: 0.01535633485764265 0.1767154335975647 0.0001\n",
      "save model\n",
      "Epoch 439: 0.015993494540452957 0.5227994918823242 0.0001\n",
      "Epoch 440: 0.01647855155169964 0.2141258716583252 0.0001\n",
      "Epoch 441: 0.015381397679448128 0.1936156153678894 0.0001\n",
      "Epoch 442: 0.01554170809686184 0.1946057826280594 0.0001\n",
      "Epoch 443: 0.01579129695892334 0.18249279260635376 0.0001\n",
      "Epoch 444: 0.015046225860714912 0.18693822622299194 0.0001\n",
      "Epoch 445: 0.015783485025167465 0.179208442568779 0.0001\n",
      "Epoch 446: 0.015597275458276272 0.18293257057666779 0.0001\n",
      "Epoch 447: 0.014292151667177677 0.17802879214286804 0.0001\n",
      "Epoch 448: 0.01535315252840519 0.18548861145973206 0.0001\n",
      "Epoch 449: 0.01669200323522091 0.17807331681251526 0.0001\n",
      "Epoch 450: 0.01517738588154316 0.18327577412128448 0.0001\n",
      "Epoch 451: 0.016202252358198166 0.17827562987804413 0.0001\n",
      "Epoch 452: 0.015568391419947147 0.18223313987255096 0.0001\n",
      "Epoch 453: 0.014662191271781921 0.1792207658290863 0.0001\n",
      "Epoch 454: 0.01566958986222744 0.18207105994224548 0.0001\n",
      "Epoch 455: 0.015559094026684761 0.17791350185871124 0.0001\n",
      "Epoch 456: 0.015378771349787712 0.17942412197589874 0.0001\n",
      "Epoch 457: 0.01534488145262003 0.1770184487104416 0.0001\n",
      "Epoch 458: 0.014929159544408321 0.1820191740989685 0.0001\n",
      "Epoch 459: 0.015987679362297058 0.1782902330160141 0.0001\n",
      "Epoch 460: 0.015570897608995438 0.18121644854545593 0.0001\n",
      "Epoch 461: 0.014756454154849052 0.17817628383636475 0.0001\n",
      "Epoch 462: 0.016260508447885513 0.1781589835882187 0.0001\n",
      "Epoch 463: 0.014218722470104694 0.1768624484539032 0.0001\n",
      "Epoch 464: 0.014913808554410934 0.18120817840099335 0.0001\n",
      "Epoch 465: 0.016618087887763977 0.17798973619937897 0.0001\n",
      "Epoch 466: 0.014010082930326462 0.180167555809021 0.0001\n",
      "Epoch 467: 0.015293095260858536 0.17877092957496643 0.0001\n",
      "Epoch 468: 0.015645179897546768 0.1803673803806305 0.0001\n",
      "Epoch 469: 0.0150919109582901 0.17822809517383575 0.0001\n",
      "Epoch 470: 0.015918148681521416 0.18123690783977509 0.0001\n",
      "Epoch 471: 0.01520838588476181 0.17682060599327087 0.0001\n",
      "Epoch 472: 0.014394320547580719 0.1812320053577423 0.0001\n",
      "Epoch 473: 0.016357412561774254 0.17737962305545807 0.0001\n",
      "Epoch 474: 0.014600981958210468 0.17989222705364227 0.0001\n",
      "Epoch 475: 0.014501132071018219 0.1770080029964447 0.0001\n",
      "Epoch 476: 0.01621970534324646 0.17800462245941162 0.0001\n",
      "Epoch 477: 0.013814376667141914 0.17665666341781616 0.0001\n",
      "save model\n",
      "Epoch 478: 0.015432274900376797 0.17954608798027039 0.0001\n",
      "Epoch 479: 0.015530290082097054 0.17911401391029358 0.0001\n",
      "Epoch 480: 0.014184074476361275 0.17903928458690643 0.0001\n",
      "Epoch 481: 0.015914954245090485 0.17746958136558533 0.0001\n",
      "Epoch 482: 0.015027309767901897 0.17805297672748566 0.0001\n",
      "Epoch 483: 0.014910181984305382 0.17770875990390778 0.0001\n",
      "Epoch 484: 0.01600988209247589 0.5790399312973022 0.0001\n",
      "Epoch 485: 0.014417673461139202 0.22477290034294128 0.0001\n",
      "Epoch 486: 0.01527464296668768 0.21137379109859467 0.0001\n",
      "Epoch 487: 0.016239674761891365 0.197218120098114 0.0001\n",
      "Epoch 488: 0.013985863886773586 0.2011205554008484 0.0001\n",
      "Epoch 489: 0.014862453565001488 0.1826598197221756 0.0001\n",
      "Epoch 490: 0.01601039618253708 0.19300563633441925 0.0001\n",
      "Epoch 491: 0.014895385131239891 0.17981164157390594 0.0001\n",
      "Epoch 492: 0.013246825896203518 0.18641801178455353 0.0001\n",
      "Epoch 493: 0.014808638021349907 0.17788134515285492 0.0001\n",
      "Epoch 494: 0.01605191081762314 0.181999072432518 0.0001\n",
      "Epoch 495: 0.013248054310679436 0.17801609635353088 0.0001\n",
      "Epoch 496: 0.015430498868227005 0.1828046292066574 0.0001\n",
      "Epoch 497: 0.015137474983930588 0.1775100976228714 0.0001\n",
      "Epoch 498: 0.013618065975606441 0.18234440684318542 0.0001\n",
      "Epoch 499: 0.016431355848908424 0.17640867829322815 0.0001\n",
      "save model\n",
      "Epoch 500: 0.014225100167095661 0.18205726146697998 0.0001\n",
      "Epoch 501: 0.014908330515027046 0.17660881578922272 0.0001\n",
      "Epoch 502: 0.01501726359128952 0.18051326274871826 0.0001\n",
      "Epoch 503: 0.013856211677193642 0.176378071308136 0.0001\n",
      "save model\n",
      "Epoch 504: 0.014714562334120274 0.1802927404642105 0.0001\n",
      "Epoch 505: 0.014997592195868492 0.17650675773620605 0.0001\n",
      "Epoch 506: 0.013906522653996944 0.1778866946697235 0.0001\n",
      "Epoch 507: 0.014856238849461079 0.17752543091773987 0.0001\n",
      "Epoch 508: 0.01432268600910902 0.1779737025499344 0.0001\n",
      "Epoch 509: 0.01404963992536068 0.17817920446395874 0.0001\n",
      "Epoch 510: 0.014769810251891613 0.1776362806558609 0.0001\n",
      "Epoch 511: 0.014951684512197971 0.17726105451583862 0.0001\n",
      "Epoch 512: 0.013923468999564648 0.1765775829553604 0.0001\n",
      "Epoch 513: 0.01419348455965519 0.17704948782920837 0.0001\n",
      "Epoch 514: 0.014840081334114075 0.17637871205806732 0.0001\n",
      "Epoch 515: 0.013653093948960304 0.177299365401268 0.0001\n",
      "Epoch 516: 0.013925788924098015 0.17898285388946533 0.0001\n",
      "Epoch 517: 0.014911346137523651 0.1770557314157486 0.0001\n",
      "Epoch 518: 0.01348906196653843 0.17896617949008942 0.0001\n",
      "Epoch 519: 0.013545120134949684 0.17732825875282288 0.0001\n",
      "Epoch 520: 0.014657176099717617 0.17814384400844574 0.0001\n",
      "Epoch 521: 0.013984709978103638 0.1761627197265625 0.0001\n",
      "save model\n",
      "Epoch 522: 0.014737093821167946 0.1780976504087448 0.0001\n",
      "Epoch 523: 0.01434588897973299 0.17558178305625916 0.0001\n",
      "save model\n",
      "Epoch 524: 0.014146700501441956 0.17890390753746033 0.0001\n",
      "Epoch 525: 0.013822128064930439 0.17616604268550873 0.0001\n",
      "Epoch 526: 0.015302792191505432 0.17840053141117096 0.0001\n",
      "Epoch 527: 0.013801125809550285 0.1753646731376648 0.0001\n",
      "save model\n",
      "Epoch 528: 0.01450123731046915 0.17783290147781372 0.0001\n",
      "Epoch 529: 0.014170140027999878 0.17679081857204437 0.0001\n",
      "Epoch 530: 0.01466586533933878 0.17860811948776245 0.0001\n",
      "Epoch 531: 0.014454453252255917 0.17722399532794952 0.0001\n",
      "Epoch 532: 0.014550503343343735 0.17785251140594482 0.0001\n",
      "Epoch 533: 0.014778192155063152 0.17747099697589874 0.0001\n",
      "Epoch 534: 0.013724462129175663 0.17547768354415894 0.0001\n",
      "Epoch 535: 0.01469856221228838 0.17851406335830688 0.0001\n",
      "Epoch 536: 0.014295612461864948 0.1762504279613495 0.0001\n",
      "Epoch 537: 0.014138284139335155 0.1797695755958557 0.0001\n",
      "Epoch 538: 0.014709127135574818 0.17751576006412506 0.0001\n",
      "Epoch 539: 0.014692362397909164 0.18046578764915466 0.0001\n",
      "Epoch 540: 0.014208084903657436 0.1776227504014969 0.0001\n",
      "Epoch 541: 0.014350567013025284 0.17997148633003235 0.0001\n",
      "Epoch 542: 0.01448118221014738 0.17552082240581512 0.0001\n",
      "Epoch 543: 0.013584992848336697 0.1786852478981018 9.900000000000001e-05\n",
      "Epoch 544: 0.013637942261993885 0.1770864874124527 9.900000000000001e-05\n",
      "Epoch 545: 0.013642880134284496 0.17957356572151184 9.900000000000001e-05\n",
      "Epoch 546: 0.014467560686171055 0.17712673544883728 9.900000000000001e-05\n",
      "Epoch 547: 0.01371213048696518 0.17744044959545135 9.900000000000001e-05\n",
      "Epoch 548: 0.01309360284358263 0.17730943858623505 9.900000000000001e-05\n",
      "Epoch 549: 0.014178171753883362 0.17798954248428345 9.900000000000001e-05\n",
      "Epoch 550: 0.013208170421421528 0.17566938698291779 9.900000000000001e-05\n",
      "Epoch 551: 0.01333621609956026 0.17925837635993958 9.900000000000001e-05\n",
      "Epoch 552: 0.01385418325662613 0.17566020786762238 9.900000000000001e-05\n",
      "Epoch 553: 0.013372921384871006 0.17805509269237518 9.900000000000001e-05\n",
      "Epoch 554: 0.013143468648195267 0.17524093389511108 9.900000000000001e-05\n",
      "save model\n",
      "Epoch 555: 0.013912796042859554 0.17683278024196625 9.900000000000001e-05\n",
      "Epoch 556: 0.013382278382778168 0.1758277267217636 9.900000000000001e-05\n",
      "Epoch 557: 0.014153935946524143 0.17625512182712555 9.900000000000001e-05\n",
      "Epoch 558: 0.014780642464756966 0.17708136141300201 9.900000000000001e-05\n",
      "Epoch 559: 0.013525283895432949 0.17573392391204834 9.900000000000001e-05\n",
      "Epoch 560: 0.014001086354255676 0.17637798190116882 9.900000000000001e-05\n",
      "Epoch 561: 0.013523214496672153 0.17579016089439392 9.900000000000001e-05\n",
      "Epoch 562: 0.01319140288978815 0.17805638909339905 9.900000000000001e-05\n",
      "Epoch 563: 0.014177380129694939 0.17691689729690552 9.900000000000001e-05\n",
      "Epoch 564: 0.014067077077925205 0.17729410529136658 9.900000000000001e-05\n",
      "Epoch 565: 0.014615505933761597 0.17628510296344757 9.900000000000001e-05\n",
      "Epoch 566: 0.014094147831201553 0.17667336761951447 9.900000000000001e-05\n",
      "Epoch 567: 0.014082955196499825 0.17703409492969513 9.900000000000001e-05\n",
      "Epoch 568: 0.013924331404268742 0.1782136708498001 9.900000000000001e-05\n",
      "Epoch 569: 0.013585868291556835 0.17700128257274628 9.900000000000001e-05\n",
      "Epoch 570: 0.013595884665846825 0.17858566343784332 9.900000000000001e-05\n",
      "Epoch 571: 0.013630999252200127 0.17552457749843597 9.900000000000001e-05\n",
      "Epoch 572: 0.013171005062758923 0.17761412262916565 9.900000000000001e-05\n",
      "Epoch 573: 0.014104186557233334 0.1750560998916626 9.900000000000001e-05\n",
      "save model\n",
      "Epoch 574: 0.013430866412818432 0.1778779774904251 9.900000000000001e-05\n",
      "Epoch 575: 0.01336919516324997 0.1758134514093399 9.900000000000001e-05\n",
      "Epoch 576: 0.01363132894039154 0.17692014575004578 9.900000000000001e-05\n",
      "Epoch 577: 0.01334949117153883 0.17581430077552795 9.900000000000001e-05\n",
      "Epoch 578: 0.014031730592250824 0.17779968678951263 9.900000000000001e-05\n",
      "Epoch 579: 0.014204136095941067 0.17636987566947937 9.900000000000001e-05\n",
      "Epoch 580: 0.012928341515362263 0.17687736451625824 9.900000000000001e-05\n",
      "Epoch 581: 0.014580370858311653 0.17586340010166168 9.900000000000001e-05\n",
      "Epoch 582: 0.012685948982834816 0.1767340451478958 9.900000000000001e-05\n",
      "Epoch 583: 0.013565578497946262 0.1763990968465805 9.900000000000001e-05\n",
      "Epoch 584: 0.01428581215441227 0.17641682922840118 9.900000000000001e-05\n",
      "Epoch 585: 0.012093955650925636 0.1763259768486023 9.900000000000001e-05\n",
      "Epoch 586: 0.013918731361627579 0.1775529533624649 9.900000000000001e-05\n",
      "Epoch 587: 0.014139990322291851 0.17586779594421387 9.900000000000001e-05\n",
      "Epoch 588: 0.012964745983481407 0.1778254359960556 9.900000000000001e-05\n",
      "Epoch 589: 0.013989977538585663 0.17532126605510712 9.900000000000001e-05\n",
      "Epoch 590: 0.013268169946968555 0.17780539393424988 9.900000000000001e-05\n",
      "Epoch 591: 0.01278861053287983 0.17656801640987396 9.900000000000001e-05\n",
      "Epoch 592: 0.014365887269377708 0.1779722422361374 9.900000000000001e-05\n",
      "Epoch 593: 0.012608461081981659 0.17616690695285797 9.900000000000001e-05\n",
      "Epoch 594: 0.013698913156986237 0.17845338582992554 9.900000000000001e-05\n",
      "Epoch 595: 0.013850588351488113 0.17590934038162231 9.900000000000001e-05\n",
      "Epoch 596: 0.013190291821956635 0.1769198477268219 9.900000000000001e-05\n",
      "Epoch 597: 0.01364315114915371 0.1766432523727417 9.900000000000001e-05\n",
      "Epoch 598: 0.01431319210678339 0.1752416342496872 9.900000000000001e-05\n",
      "Epoch 599: 0.013352394104003906 0.17624247074127197 9.900000000000001e-05\n",
      "Epoch 600: 0.014197097159922123 0.17551927268505096 9.900000000000001e-05\n",
      "Epoch 601: 0.013138310983777046 0.1760740429162979 9.900000000000001e-05\n",
      "Epoch 602: 0.012842991389334202 0.17764954268932343 9.900000000000001e-05\n",
      "Epoch 603: 0.014875930733978748 0.17550913989543915 9.900000000000001e-05\n",
      "Epoch 604: 0.011996480636298656 0.17670099437236786 9.900000000000001e-05\n",
      "Epoch 605: 0.013855917379260063 0.17685620486736298 9.900000000000001e-05\n",
      "Epoch 606: 0.013754623010754585 0.17572711408138275 9.900000000000001e-05\n",
      "Epoch 607: 0.012357134371995926 0.17783235013484955 9.900000000000001e-05\n",
      "Epoch 608: 0.014271452091634274 0.17505225539207458 9.900000000000001e-05\n",
      "save model\n",
      "Epoch 609: 0.012426022440195084 0.1764923632144928 9.900000000000001e-05\n",
      "Epoch 610: 0.012914055958390236 0.17538820207118988 9.900000000000001e-05\n",
      "Epoch 611: 0.01427198015153408 0.17624187469482422 9.900000000000001e-05\n",
      "Epoch 612: 0.012147026136517525 0.17487238347530365 9.900000000000001e-05\n",
      "save model\n",
      "Epoch 613: 0.013472316786646843 0.17726001143455505 9.900000000000001e-05\n",
      "Epoch 614: 0.014130035415291786 0.17457188665866852 9.900000000000001e-05\n",
      "save model\n",
      "Epoch 615: 0.012410173192620277 0.17701293528079987 9.900000000000001e-05\n",
      "Epoch 616: 0.014032342471182346 0.17555853724479675 9.900000000000001e-05\n",
      "Epoch 617: 0.013386411592364311 0.17653392255306244 9.900000000000001e-05\n",
      "Epoch 618: 0.012983727268874645 0.17710697650909424 9.900000000000001e-05\n",
      "Epoch 619: 0.014130288735032082 0.1753811091184616 9.900000000000001e-05\n",
      "Epoch 620: 0.012368801981210709 0.17665694653987885 9.900000000000001e-05\n",
      "Epoch 621: 0.014132039621472359 0.17585539817810059 9.900000000000001e-05\n",
      "Epoch 622: 0.013627043925225735 0.1755397915840149 9.900000000000001e-05\n",
      "Epoch 623: 0.012654894962906837 0.177403062582016 9.900000000000001e-05\n",
      "Epoch 624: 0.01398555375635624 0.17598792910575867 9.900000000000001e-05\n",
      "Epoch 625: 0.012508061714470387 0.1761021614074707 9.900000000000001e-05\n",
      "Epoch 626: 0.013357571326196194 0.176035538315773 9.900000000000001e-05\n",
      "Epoch 627: 0.013269467279314995 0.1758510172367096 9.900000000000001e-05\n",
      "Epoch 628: 0.012725102715194225 0.17616084218025208 9.900000000000001e-05\n",
      "Epoch 629: 0.013813192024827003 0.17562945187091827 9.900000000000001e-05\n",
      "Epoch 630: 0.01247566007077694 0.17582814395427704 9.900000000000001e-05\n",
      "Epoch 631: 0.012956029735505581 0.17496119439601898 9.900000000000001e-05\n",
      "Epoch 632: 0.013051836751401424 0.17493346333503723 9.900000000000001e-05\n",
      "Epoch 633: 0.011883561499416828 0.17534300684928894 9.900000000000001e-05\n",
      "Epoch 634: 0.013099493458867073 0.1746416985988617 9.900000000000001e-05\n",
      "Epoch 635: 0.012754668481647968 0.17525620758533478 9.900000000000001e-05\n",
      "Epoch 636: 0.01318606548011303 0.17497335374355316 9.900000000000001e-05\n",
      "Epoch 637: 0.01334297563880682 0.17816336452960968 9.900000000000001e-05\n",
      "Epoch 638: 0.013951909728348255 0.1759193390607834 9.900000000000001e-05\n",
      "Epoch 639: 0.013346273452043533 0.17684994637966156 9.900000000000001e-05\n",
      "Epoch 640: 0.012500699609518051 0.1754504293203354 9.900000000000001e-05\n",
      "Epoch 641: 0.013635838404297829 0.1770913004875183 9.900000000000001e-05\n",
      "Epoch 642: 0.012867476791143417 0.1748795062303543 9.900000000000001e-05\n",
      "Epoch 643: 0.013607802800834179 0.17735500633716583 9.900000000000001e-05\n",
      "Epoch 644: 0.01342886034399271 0.17552709579467773 9.900000000000001e-05\n",
      "Epoch 645: 0.013278762809932232 0.17683744430541992 9.900000000000001e-05\n",
      "Epoch 646: 0.012765433639287949 0.17497652769088745 9.900000000000001e-05\n",
      "Epoch 647: 0.012873980216681957 0.17680144309997559 9.900000000000001e-05\n",
      "Epoch 648: 0.012768006883561611 0.17570340633392334 9.900000000000001e-05\n",
      "Epoch 649: 0.013197927735745907 0.1765996813774109 9.900000000000001e-05\n",
      "Epoch 650: 0.012882900424301624 0.17521484196186066 9.900000000000001e-05\n",
      "Epoch 651: 0.013500683009624481 0.17599469423294067 9.900000000000001e-05\n",
      "Epoch 652: 0.013530322350561619 0.17507655918598175 9.900000000000001e-05\n",
      "Epoch 653: 0.013384669087827206 0.17501869797706604 9.900000000000001e-05\n",
      "Epoch 654: 0.01313688326627016 0.1742507815361023 9.900000000000001e-05\n",
      "save model\n",
      "Epoch 655: 0.01295506116002798 0.17540326714515686 9.900000000000001e-05\n",
      "Epoch 656: 0.01296854205429554 0.17570170760154724 9.900000000000001e-05\n",
      "Epoch 657: 0.013179345056414604 0.1762848049402237 9.900000000000001e-05\n",
      "Epoch 658: 0.013017185963690281 0.17541064321994781 9.900000000000001e-05\n",
      "Epoch 659: 0.01278634648770094 0.17651988565921783 9.900000000000001e-05\n",
      "Epoch 660: 0.013634136877954006 0.17549383640289307 9.900000000000001e-05\n",
      "Epoch 661: 0.012633485719561577 0.17639552056789398 9.900000000000001e-05\n",
      "Epoch 662: 0.013265572488307953 0.17591771483421326 9.900000000000001e-05\n",
      "Epoch 663: 0.012415964156389236 0.17552071809768677 9.900000000000001e-05\n",
      "Epoch 664: 0.012611093930900097 0.1768156737089157 9.900000000000001e-05\n",
      "Epoch 665: 0.0140989376232028 0.17625275254249573 9.900000000000001e-05\n",
      "Epoch 666: 0.012523647397756577 0.17647777497768402 9.900000000000001e-05\n",
      "Epoch 667: 0.013420050032436848 0.17570878565311432 9.900000000000001e-05\n",
      "Epoch 668: 0.013014397583901882 0.1761527806520462 9.900000000000001e-05\n",
      "Epoch 669: 0.012460041791200638 0.1757165938615799 9.900000000000001e-05\n",
      "Epoch 670: 0.012852555140852928 0.17688745260238647 9.900000000000001e-05\n",
      "Epoch 671: 0.013416553847491741 0.17593911290168762 9.900000000000001e-05\n",
      "Epoch 672: 0.012656967155635357 0.17652231454849243 9.900000000000001e-05\n",
      "Epoch 673: 0.013055426999926567 0.17442519962787628 9.900000000000001e-05\n",
      "Epoch 674: 0.012849779799580574 0.17751112580299377 9.900000000000001e-05\n",
      "Epoch 675: 0.01323769148439169 0.17494942247867584 9.900000000000001e-05\n",
      "Epoch 676: 0.013200473040342331 0.1766306757926941 9.900000000000001e-05\n",
      "Epoch 677: 0.012128124944865704 0.1737958937883377 9.900000000000001e-05\n",
      "save model\n",
      "Epoch 678: 0.013021199963986874 0.17615756392478943 9.900000000000001e-05\n",
      "Epoch 679: 0.013382082805037498 0.1740080714225769 9.900000000000001e-05\n",
      "Epoch 680: 0.011979499831795692 0.17636579275131226 9.900000000000001e-05\n",
      "Epoch 681: 0.013744242489337921 0.17441879212856293 9.900000000000001e-05\n",
      "Epoch 682: 0.013051281683146954 0.17504049837589264 9.900000000000001e-05\n",
      "Epoch 683: 0.012256178073585033 0.17632189393043518 9.900000000000001e-05\n",
      "Epoch 684: 0.013898919336497784 0.17485429346561432 9.801e-05\n",
      "Epoch 685: 0.011984532698988914 0.17607024312019348 9.801e-05\n",
      "Epoch 686: 0.01230609230697155 0.33545222878456116 9.801e-05\n",
      "Epoch 687: 0.013867742381989956 0.18315891921520233 9.801e-05\n",
      "Epoch 688: 0.0117622260004282 0.17747218906879425 9.801e-05\n",
      "Epoch 689: 0.013247959315776825 0.17766417562961578 9.801e-05\n",
      "Epoch 690: 0.012075946666300297 0.17620445787906647 9.801e-05\n",
      "Epoch 691: 0.012426206842064857 0.1783500760793686 9.801e-05\n",
      "Epoch 692: 0.012296154163777828 0.17503748834133148 9.801e-05\n",
      "Epoch 693: 0.01215273980051279 0.1767047494649887 9.801e-05\n",
      "Epoch 694: 0.012171076610684395 0.17584417760372162 9.801e-05\n",
      "Epoch 695: 0.012553350999951363 0.17766183614730835 9.801e-05\n",
      "Epoch 696: 0.012302307412028313 0.17519249022006989 9.801e-05\n",
      "Epoch 697: 0.012588643468916416 0.17786799371242523 9.801e-05\n",
      "Epoch 698: 0.013160700909793377 0.17476847767829895 9.801e-05\n",
      "Epoch 699: 0.011425405740737915 0.17737025022506714 9.801e-05\n",
      "Epoch 700: 0.013369298540055752 0.17437690496444702 9.801e-05\n",
      "Epoch 701: 0.012733126059174538 0.17609654366970062 9.801e-05\n",
      "Epoch 702: 0.0127106923609972 0.17483265697956085 9.801e-05\n",
      "Epoch 703: 0.013569899834692478 0.17422741651535034 9.801e-05\n",
      "Epoch 704: 0.011605965904891491 0.1750674694776535 9.801e-05\n",
      "Epoch 705: 0.012973650358617306 0.17493002116680145 9.801e-05\n",
      "Epoch 706: 0.012910936959087849 0.17578794062137604 9.801e-05\n",
      "Epoch 707: 0.011347065679728985 0.1748131960630417 9.801e-05\n",
      "Epoch 708: 0.013372991234064102 0.17604979872703552 9.801e-05\n",
      "Epoch 709: 0.012193591333925724 0.17426888644695282 9.801e-05\n",
      "Epoch 710: 0.012476647272706032 0.17668136954307556 9.801e-05\n",
      "Epoch 711: 0.012904850766062737 0.1732407808303833 9.801e-05\n",
      "save model\n",
      "Epoch 712: 0.011719861067831516 0.1767658144235611 9.801e-05\n",
      "Epoch 713: 0.013130690902471542 0.17436298727989197 9.801e-05\n",
      "Epoch 714: 0.01264171116054058 0.1760714054107666 9.801e-05\n",
      "Epoch 715: 0.012262605130672455 0.17457491159439087 9.801e-05\n",
      "Epoch 716: 0.012804162688553333 0.17541106045246124 9.801e-05\n",
      "Epoch 717: 0.012514850124716759 0.17466770112514496 9.801e-05\n",
      "Epoch 718: 0.012303520925343037 0.17625942826271057 9.801e-05\n",
      "Epoch 719: 0.012803441844880581 0.17551271617412567 9.801e-05\n",
      "Epoch 720: 0.011829415336251259 0.17497490346431732 9.801e-05\n",
      "Epoch 721: 0.012592995539307594 0.1745259165763855 9.801e-05\n",
      "Epoch 722: 0.012384205125272274 0.17429029941558838 9.801e-05\n",
      "Epoch 723: 0.01200474239885807 0.17448113858699799 9.801e-05\n",
      "Epoch 724: 0.012988602742552757 0.1747896373271942 9.801e-05\n",
      "Epoch 725: 0.012377072125673294 0.1738518476486206 9.801e-05\n",
      "Epoch 726: 0.012273936532437801 0.17633014917373657 9.801e-05\n",
      "Epoch 727: 0.013514038175344467 0.1738712042570114 9.801e-05\n",
      "Epoch 728: 0.011388669721782207 0.1753668189048767 9.801e-05\n",
      "Epoch 729: 0.013338739052414894 0.17441970109939575 9.801e-05\n",
      "Epoch 730: 0.012607060372829437 0.17462457716464996 9.801e-05\n",
      "Epoch 731: 0.011593326926231384 0.17466095089912415 9.801e-05\n",
      "Epoch 732: 0.012867270968854427 0.17536798119544983 9.801e-05\n",
      "Epoch 733: 0.012485607527196407 0.1745266318321228 9.801e-05\n",
      "Epoch 734: 0.011305529624223709 0.17622579634189606 9.801e-05\n",
      "Epoch 735: 0.012681982479989529 0.17405769228935242 9.801e-05\n",
      "Epoch 736: 0.012091300450265408 0.1754772961139679 9.801e-05\n",
      "Epoch 737: 0.01187435444444418 0.17443037033081055 9.801e-05\n",
      "Epoch 738: 0.012792418710887432 0.17546555399894714 9.801e-05\n",
      "Epoch 739: 0.011803407222032547 0.17462223768234253 9.801e-05\n",
      "Epoch 740: 0.012936933897435665 0.17557330429553986 9.801e-05\n",
      "Epoch 741: 0.012456023134291172 0.1739584356546402 9.801e-05\n",
      "Epoch 742: 0.01193181797862053 0.17531011998653412 9.801e-05\n",
      "Epoch 743: 0.012858066707849503 0.1746203601360321 9.801e-05\n",
      "Epoch 744: 0.011880677193403244 0.17485368251800537 9.801e-05\n",
      "Epoch 745: 0.013074103742837906 0.17471036314964294 9.801e-05\n",
      "Epoch 746: 0.012081884779036045 0.17469632625579834 9.801e-05\n",
      "Epoch 747: 0.012287658639252186 0.17451903223991394 9.801e-05\n",
      "Epoch 748: 0.011926446110010147 0.175382599234581 9.801e-05\n",
      "Epoch 749: 0.012576289474964142 0.17437690496444702 9.801e-05\n",
      "Epoch 750: 0.01173474732786417 0.17460897564888 9.801e-05\n",
      "Epoch 751: 0.012131309136748314 0.1747007668018341 9.801e-05\n",
      "Epoch 752: 0.012297802604734898 0.17499090731143951 9.801e-05\n",
      "Epoch 753: 0.011921924538910389 0.17447829246520996 9.801e-05\n",
      "Epoch 754: 0.012740355916321278 0.176100492477417 9.801e-05\n",
      "Epoch 755: 0.012342954985797405 0.17363132536411285 9.801e-05\n",
      "Epoch 756: 0.011928465217351913 0.1768336147069931 9.801e-05\n",
      "Epoch 757: 0.012869412079453468 0.17396898567676544 9.801e-05\n",
      "Epoch 758: 0.012408359907567501 0.17643383145332336 9.801e-05\n",
      "Epoch 759: 0.011938152834773064 0.17454591393470764 9.801e-05\n",
      "Epoch 760: 0.012350346893072128 0.17631132900714874 9.801e-05\n",
      "Epoch 761: 0.01171259954571724 0.17377863824367523 9.801e-05\n",
      "Epoch 762: 0.012210075743496418 0.1755131483078003 9.801e-05\n",
      "Epoch 763: 0.01260471437126398 0.17330175638198853 9.801e-05\n",
      "Epoch 764: 0.011727404780685902 0.17518368363380432 9.801e-05\n",
      "Epoch 765: 0.012589868158102036 0.1739286631345749 9.801e-05\n",
      "Epoch 766: 0.012413389049470425 0.17450346052646637 9.801e-05\n",
      "Epoch 767: 0.01132216863334179 0.1742175966501236 9.801e-05\n",
      "Epoch 768: 0.012964428402483463 0.17475034296512604 9.801e-05\n",
      "Epoch 769: 0.012319115921854973 0.1740199476480484 9.801e-05\n",
      "Epoch 770: 0.011462603695690632 0.17502005398273468 9.801e-05\n",
      "Epoch 771: 0.012725968845188618 0.1737104207277298 9.801e-05\n",
      "Epoch 772: 0.011431105434894562 0.17490838468074799 9.801e-05\n",
      "Epoch 773: 0.012768538668751717 0.17495334148406982 9.801e-05\n",
      "Epoch 774: 0.01214941218495369 0.173990860581398 9.801e-05\n",
      "Epoch 775: 0.011856812983751297 0.17559581995010376 9.801e-05\n",
      "Epoch 776: 0.013316073454916477 0.17362838983535767 9.801e-05\n",
      "Epoch 777: 0.011533030308783054 0.17593061923980713 9.801e-05\n",
      "Epoch 778: 0.012699144892394543 0.17373600602149963 9.801e-05\n",
      "Epoch 779: 0.011883145198225975 0.17519472539424896 9.801e-05\n",
      "Epoch 780: 0.011734690517187119 0.17332248389720917 9.801e-05\n",
      "Epoch 781: 0.012331086210906506 0.1751430332660675 9.801e-05\n",
      "Epoch 782: 0.011572884395718575 0.17343302071094513 9.801e-05\n",
      "Epoch 783: 0.012089488096535206 0.17405842244625092 9.801e-05\n",
      "Epoch 784: 0.011936542578041553 0.1731593906879425 9.801e-05\n",
      "save model\n",
      "Epoch 785: 0.01176059152930975 0.17488156259059906 9.70299e-05\n",
      "Epoch 786: 0.011921034194529057 0.17376603186130524 9.70299e-05\n",
      "Epoch 787: 0.011279826052486897 0.17493021488189697 9.70299e-05\n",
      "Epoch 788: 0.012249213643372059 0.17398545145988464 9.70299e-05\n",
      "Epoch 789: 0.011622326448559761 0.17432603240013123 9.70299e-05\n",
      "Epoch 790: 0.011891369707882404 0.17316573858261108 9.70299e-05\n",
      "Epoch 791: 0.01146304327994585 0.17472140491008759 9.70299e-05\n",
      "Epoch 792: 0.01199306733906269 0.173746258020401 9.70299e-05\n",
      "Epoch 793: 0.011780351400375366 0.17379753291606903 9.70299e-05\n",
      "Epoch 794: 0.011450112797319889 0.1743001639842987 9.70299e-05\n",
      "Epoch 795: 0.0121909249573946 0.17439821362495422 9.70299e-05\n",
      "Epoch 796: 0.012071765027940273 0.1733686625957489 9.70299e-05\n",
      "Epoch 797: 0.012529932893812656 0.17393895983695984 9.70299e-05\n",
      "Epoch 798: 0.012282677926123142 0.17306917905807495 9.70299e-05\n",
      "save model\n",
      "Epoch 799: 0.011500249616801739 0.1744513064622879 9.70299e-05\n",
      "Epoch 800: 0.012356622144579887 0.17284364998340607 9.70299e-05\n",
      "save model\n",
      "Epoch 801: 0.011652989313006401 0.1740313023328781 9.70299e-05\n",
      "Epoch 802: 0.011795018799602985 0.1724434494972229 9.70299e-05\n",
      "save model\n",
      "Epoch 803: 0.011980821378529072 0.1744212657213211 9.70299e-05\n",
      "Epoch 804: 0.011511423625051975 0.1724289506673813 9.70299e-05\n",
      "save model\n",
      "Epoch 805: 0.011904465034604073 0.17442011833190918 9.70299e-05\n",
      "Epoch 806: 0.011928867548704147 0.17286981642246246 9.70299e-05\n",
      "Epoch 807: 0.0113423066213727 0.17422425746917725 9.70299e-05\n",
      "Epoch 808: 0.011732809245586395 0.17380261421203613 9.70299e-05\n",
      "Epoch 809: 0.011520826257765293 0.17527827620506287 9.70299e-05\n",
      "Epoch 810: 0.012477516196668148 0.17359140515327454 9.70299e-05\n",
      "Epoch 811: 0.011598245240747929 0.17449884116649628 9.70299e-05\n",
      "Epoch 812: 0.011590755544602871 0.17402175068855286 9.70299e-05\n",
      "Epoch 813: 0.012218499556183815 0.17482759058475494 9.70299e-05\n",
      "Epoch 814: 0.01155956368893385 0.17500966787338257 9.70299e-05\n",
      "Epoch 815: 0.012338207103312016 0.17412492632865906 9.70299e-05\n",
      "Epoch 816: 0.011658561415970325 0.17414478957653046 9.70299e-05\n",
      "Epoch 817: 0.012059538625180721 0.17288748919963837 9.70299e-05\n",
      "Epoch 818: 0.01167331263422966 0.17358601093292236 9.70299e-05\n",
      "Epoch 819: 0.011195840314030647 0.1732974499464035 9.70299e-05\n",
      "Epoch 820: 0.011969354003667831 0.17348012328147888 9.70299e-05\n",
      "Epoch 821: 0.011201520450413227 0.17340612411499023 9.70299e-05\n",
      "Epoch 822: 0.011554361321032047 0.17495013773441315 9.70299e-05\n",
      "Epoch 823: 0.011728550307452679 0.17387008666992188 9.70299e-05\n",
      "Epoch 824: 0.01116482075303793 0.17407342791557312 9.70299e-05\n",
      "Epoch 825: 0.01159953698515892 0.17406006157398224 9.70299e-05\n",
      "Epoch 826: 0.011430707760155201 0.17364051938056946 9.70299e-05\n",
      "Epoch 827: 0.011557865887880325 0.17481350898742676 9.70299e-05\n",
      "Epoch 828: 0.012143285945057869 0.17312510311603546 9.70299e-05\n",
      "Epoch 829: 0.011210285127162933 0.17551203072071075 9.70299e-05\n",
      "Epoch 830: 0.012268416583538055 0.1732833981513977 9.70299e-05\n",
      "Epoch 831: 0.011035230942070484 0.17495203018188477 9.70299e-05\n",
      "Epoch 832: 0.011261173523962498 0.17330202460289001 9.70299e-05\n",
      "Epoch 833: 0.01248194370418787 0.17527832090854645 9.70299e-05\n",
      "Epoch 834: 0.011051950976252556 0.17390702664852142 9.70299e-05\n",
      "Epoch 835: 0.011889660730957985 0.17498409748077393 9.70299e-05\n",
      "Epoch 836: 0.011786338873207569 0.17262783646583557 9.70299e-05\n",
      "Epoch 837: 0.011305167339742184 0.17429295182228088 9.70299e-05\n",
      "Epoch 838: 0.012007866986095905 0.17293664813041687 9.70299e-05\n",
      "Epoch 839: 0.011162861250340939 0.17434142529964447 9.70299e-05\n",
      "Epoch 840: 0.011786174960434437 0.17338278889656067 9.70299e-05\n",
      "Epoch 841: 0.011221831664443016 0.17437021434307098 9.70299e-05\n",
      "Epoch 842: 0.011751572601497173 0.17395362257957458 9.70299e-05\n",
      "Epoch 843: 0.011499616317451 0.17409373819828033 9.70299e-05\n",
      "Epoch 844: 0.011455136351287365 0.17400707304477692 9.70299e-05\n",
      "Epoch 845: 0.011383149772882462 0.17415682971477509 9.70299e-05\n",
      "Epoch 846: 0.0120253199711442 0.17367729544639587 9.70299e-05\n",
      "Epoch 847: 0.011327248066663742 0.17377634346485138 9.70299e-05\n",
      "Epoch 848: 0.011891464702785015 0.17313411831855774 9.70299e-05\n",
      "Epoch 849: 0.011346889659762383 0.1738215684890747 9.70299e-05\n",
      "Epoch 850: 0.01143626868724823 0.1733415722846985 9.70299e-05\n",
      "Epoch 851: 0.011564632877707481 0.1733742356300354 9.70299e-05\n",
      "Epoch 852: 0.011435006745159626 0.17348366975784302 9.70299e-05\n",
      "Epoch 853: 0.01142954919487238 0.1734551191329956 9.70299e-05\n",
      "Epoch 854: 0.011467686854302883 0.17297862470149994 9.70299e-05\n",
      "Epoch 855: 0.011139178648591042 0.1734665185213089 9.70299e-05\n",
      "Epoch 856: 0.011884824372828007 0.17300297319889069 9.70299e-05\n",
      "Epoch 857: 0.011387077160179615 0.17362700402736664 9.70299e-05\n",
      "Epoch 858: 0.011363294906914234 0.17300982773303986 9.70299e-05\n",
      "Epoch 859: 0.011955556459724903 0.17394156754016876 9.70299e-05\n",
      "Epoch 860: 0.011166357435286045 0.17402683198451996 9.70299e-05\n",
      "Epoch 861: 0.01171216368675232 0.17535722255706787 9.70299e-05\n",
      "Epoch 862: 0.011469648219645023 0.1735515296459198 9.70299e-05\n",
      "Epoch 863: 0.011586395092308521 0.17433133721351624 9.70299e-05\n",
      "Epoch 864: 0.012186240404844284 0.1726527065038681 9.70299e-05\n",
      "Epoch 865: 0.011570116505026817 0.17353200912475586 9.70299e-05\n",
      "Epoch 866: 0.011402202770113945 0.17303887009620667 9.70299e-05\n",
      "Epoch 867: 0.011914960108697414 0.17353329062461853 9.70299e-05\n",
      "Epoch 868: 0.011198588646948338 0.1737567037343979 9.70299e-05\n",
      "Epoch 869: 0.0118164187297225 0.1737229973077774 9.70299e-05\n",
      "Epoch 870: 0.011093389242887497 0.1735720932483673 9.70299e-05\n",
      "Epoch 871: 0.011070475913584232 0.17459739744663239 9.70299e-05\n",
      "Epoch 872: 0.011827635578811169 0.17322608828544617 9.70299e-05\n",
      "Epoch 873: 0.011165582574903965 0.1746750921010971 9.70299e-05\n",
      "Epoch 874: 0.011553773656487465 0.17331980168819427 9.70299e-05\n",
      "Epoch 875: 0.01181834377348423 0.1741076111793518 9.70299e-05\n",
      "Epoch 876: 0.011156684719026089 0.17275679111480713 9.70299e-05\n",
      "Epoch 877: 0.011667592450976372 0.17407551407814026 9.70299e-05\n",
      "Epoch 878: 0.011919607408344746 0.1731487661600113 9.70299e-05\n",
      "Epoch 879: 0.011108145117759705 0.173716738820076 9.70299e-05\n",
      "Epoch 880: 0.012065787799656391 0.17409582436084747 9.70299e-05\n",
      "Epoch 881: 0.011467944830656052 0.173171728849411 9.70299e-05\n",
      "Epoch 882: 0.011530016548931599 0.17367860674858093 9.70299e-05\n",
      "Epoch 883: 0.011563239619135857 0.17233572900295258 9.70299e-05\n",
      "save model\n",
      "Epoch 884: 0.01134652178734541 0.172990620136261 9.70299e-05\n",
      "Epoch 885: 0.011209244839847088 0.17270858585834503 9.70299e-05\n",
      "Epoch 886: 0.01190780196338892 0.1730230748653412 9.605960100000001e-05\n",
      "Epoch 887: 0.010665789246559143 0.17283639311790466 9.605960100000001e-05\n",
      "Epoch 888: 0.01151898130774498 0.1735716015100479 9.605960100000001e-05\n",
      "Epoch 889: 0.011058343574404716 0.17371949553489685 9.605960100000001e-05\n",
      "Epoch 890: 0.011359306052327156 0.1731739640235901 9.605960100000001e-05\n",
      "Epoch 891: 0.011558608151972294 0.17362673580646515 9.605960100000001e-05\n",
      "Epoch 892: 0.01080792024731636 0.17321231961250305 9.605960100000001e-05\n",
      "Epoch 893: 0.011448991484940052 0.17428386211395264 9.605960100000001e-05\n",
      "Epoch 894: 0.010952633805572987 0.17373764514923096 9.605960100000001e-05\n",
      "Epoch 895: 0.011068254709243774 0.17371444404125214 9.605960100000001e-05\n",
      "Epoch 896: 0.011365695856511593 0.17389842867851257 9.605960100000001e-05\n",
      "Epoch 897: 0.011078358627855778 0.1737293303012848 9.605960100000001e-05\n",
      "Epoch 898: 0.011580991558730602 0.17439252138137817 9.605960100000001e-05\n",
      "Epoch 899: 0.011377964168787003 0.1729433238506317 9.605960100000001e-05\n",
      "Epoch 900: 0.010960032232105732 0.17373569309711456 9.605960100000001e-05\n",
      "Epoch 901: 0.011719776317477226 0.174096941947937 9.605960100000001e-05\n",
      "Epoch 902: 0.010669863782823086 0.17404328286647797 9.605960100000001e-05\n",
      "Epoch 903: 0.011351866647601128 0.1740301549434662 9.605960100000001e-05\n",
      "Epoch 904: 0.01107108499854803 0.17375096678733826 9.605960100000001e-05\n",
      "Epoch 905: 0.01121616456657648 0.17382767796516418 9.605960100000001e-05\n",
      "Epoch 906: 0.011437412351369858 0.17367196083068848 9.605960100000001e-05\n",
      "Epoch 907: 0.010817979462444782 0.17411865293979645 9.605960100000001e-05\n",
      "Epoch 908: 0.011484184302389622 0.17347122728824615 9.605960100000001e-05\n",
      "Epoch 909: 0.011244631372392178 0.7178796529769897 9.605960100000001e-05\n",
      "Epoch 910: 0.012098516337573528 0.18690167367458344 9.605960100000001e-05\n",
      "Epoch 911: 0.012174752540886402 0.186136856675148 9.605960100000001e-05\n",
      "Epoch 912: 0.01174655370414257 0.17892929911613464 9.605960100000001e-05\n",
      "Epoch 913: 0.010786845348775387 0.17991852760314941 9.605960100000001e-05\n",
      "Epoch 914: 0.011609038338065147 0.17582817375659943 9.605960100000001e-05\n",
      "Epoch 915: 0.011521165259182453 0.1754518747329712 9.605960100000001e-05\n",
      "Epoch 916: 0.010681750252842903 0.17565476894378662 9.605960100000001e-05\n",
      "Epoch 917: 0.01207788847386837 0.17465272545814514 9.605960100000001e-05\n",
      "Epoch 918: 0.010914941318333149 0.17373457551002502 9.605960100000001e-05\n",
      "Epoch 919: 0.010401925072073936 0.17387928068637848 9.605960100000001e-05\n",
      "Epoch 920: 0.012609695084393024 0.1734769493341446 9.605960100000001e-05\n",
      "Epoch 921: 0.010428354144096375 0.17417867481708527 9.605960100000001e-05\n",
      "Epoch 922: 0.011147472076117992 0.17429690062999725 9.605960100000001e-05\n",
      "Epoch 923: 0.011897019110620022 0.17413996160030365 9.605960100000001e-05\n",
      "Epoch 924: 0.01064890157431364 0.17323067784309387 9.605960100000001e-05\n",
      "Epoch 925: 0.011266643181443214 0.17345348000526428 9.605960100000001e-05\n",
      "Epoch 926: 0.011206007562577724 0.17274168133735657 9.605960100000001e-05\n",
      "Epoch 927: 0.011169232428073883 0.17354455590248108 9.605960100000001e-05\n",
      "Epoch 928: 0.010877967812120914 0.17288239300251007 9.605960100000001e-05\n",
      "Epoch 929: 0.010786797851324081 0.17459890246391296 9.605960100000001e-05\n",
      "Epoch 930: 0.01153651811182499 0.17290741205215454 9.605960100000001e-05\n",
      "Epoch 931: 0.010787653736770153 0.17414435744285583 9.605960100000001e-05\n",
      "Epoch 932: 0.011015582829713821 0.17270058393478394 9.605960100000001e-05\n",
      "Epoch 933: 0.011441811919212341 0.17379401624202728 9.605960100000001e-05\n",
      "Epoch 934: 0.010683964006602764 0.17244283854961395 9.605960100000001e-05\n",
      "Epoch 935: 0.011310957372188568 0.17438070476055145 9.605960100000001e-05\n",
      "Epoch 936: 0.011554849334061146 0.17283925414085388 9.605960100000001e-05\n",
      "Epoch 937: 0.011271379888057709 0.17356325685977936 9.605960100000001e-05\n",
      "Epoch 938: 0.011193204671144485 0.17233435809612274 9.605960100000001e-05\n",
      "save model\n",
      "Epoch 939: 0.010991161689162254 0.17418895661830902 9.605960100000001e-05\n",
      "Epoch 940: 0.011681430973112583 0.17199184000492096 9.605960100000001e-05\n",
      "save model\n",
      "Epoch 941: 0.010792011395096779 0.1736106425523758 9.605960100000001e-05\n",
      "Epoch 942: 0.011348010040819645 0.1721104383468628 9.605960100000001e-05\n",
      "Epoch 943: 0.011099273338913918 0.17435026168823242 9.605960100000001e-05\n",
      "Epoch 944: 0.011016901582479477 0.1727374941110611 9.605960100000001e-05\n",
      "Epoch 945: 0.011463942006230354 0.17402440309524536 9.605960100000001e-05\n",
      "Epoch 946: 0.010625319555401802 0.1728314459323883 9.605960100000001e-05\n",
      "Epoch 947: 0.01111721433699131 0.1748802214860916 9.605960100000001e-05\n",
      "Epoch 948: 0.011331642977893353 0.17305412888526917 9.605960100000001e-05\n",
      "Epoch 949: 0.010995040647685528 0.17437191307544708 9.605960100000001e-05\n",
      "Epoch 950: 0.01099455077201128 0.17289236187934875 9.605960100000001e-05\n",
      "Epoch 951: 0.010839924216270447 0.17352932691574097 9.605960100000001e-05\n",
      "Epoch 952: 0.010898569598793983 0.17245404422283173 9.605960100000001e-05\n",
      "Epoch 953: 0.010816204361617565 0.1737184226512909 9.605960100000001e-05\n",
      "Epoch 954: 0.01070608664304018 0.17317457497119904 9.605960100000001e-05\n",
      "Epoch 955: 0.0112386429682374 0.1731288582086563 9.605960100000001e-05\n",
      "Epoch 956: 0.010614998638629913 0.17394059896469116 9.605960100000001e-05\n",
      "Epoch 957: 0.011109053157269955 0.1727161556482315 9.605960100000001e-05\n",
      "Epoch 958: 0.01086602732539177 0.17313344776630402 9.605960100000001e-05\n",
      "Epoch 959: 0.010823052376508713 0.1727714240550995 9.605960100000001e-05\n",
      "Epoch 960: 0.011525509878993034 0.17285460233688354 9.605960100000001e-05\n",
      "Epoch 961: 0.010595301166176796 0.17264483869075775 9.605960100000001e-05\n",
      "Epoch 962: 0.011825419031083584 0.17381376028060913 9.605960100000001e-05\n",
      "Epoch 963: 0.011102964170277119 0.17177873849868774 9.605960100000001e-05\n",
      "save model\n",
      "Epoch 964: 0.011125295422971249 0.17356449365615845 9.605960100000001e-05\n",
      "Epoch 965: 0.010973446071147919 0.17210882902145386 9.605960100000001e-05\n",
      "Epoch 966: 0.010764206759631634 0.17363962531089783 9.605960100000001e-05\n",
      "Epoch 967: 0.011241924948990345 0.1728149652481079 9.605960100000001e-05\n",
      "Epoch 968: 0.01145328301936388 0.1736302226781845 9.605960100000001e-05\n",
      "Epoch 969: 0.010641718283295631 0.1731245070695877 9.605960100000001e-05\n",
      "Epoch 970: 0.011875960044562817 0.17380598187446594 9.605960100000001e-05\n",
      "Epoch 971: 0.010624032467603683 0.1728396862745285 9.605960100000001e-05\n",
      "Epoch 972: 0.011087819002568722 0.17336848378181458 9.605960100000001e-05\n",
      "Epoch 973: 0.011532939970493317 0.17227889597415924 9.605960100000001e-05\n",
      "Epoch 974: 0.010431678965687752 0.17305728793144226 9.605960100000001e-05\n",
      "Epoch 975: 0.011386943981051445 0.17218902707099915 9.605960100000001e-05\n",
      "Epoch 976: 0.010707003064453602 0.1725269854068756 9.605960100000001e-05\n",
      "Epoch 977: 0.010804317891597748 0.1723189800977707 9.605960100000001e-05\n",
      "Epoch 978: 0.011353281326591969 0.17294996976852417 9.605960100000001e-05\n",
      "Epoch 979: 0.010212890803813934 0.17268800735473633 9.605960100000001e-05\n",
      "Epoch 980: 0.011175686493515968 0.1726321429014206 9.605960100000001e-05\n",
      "Epoch 981: 0.010835007764399052 0.17230650782585144 9.605960100000001e-05\n",
      "Epoch 982: 0.010488619096577168 0.1725015640258789 9.605960100000001e-05\n",
      "Epoch 983: 0.010598575696349144 0.17202408611774445 9.605960100000001e-05\n",
      "Epoch 984: 0.01082482561469078 0.17219363152980804 9.605960100000001e-05\n",
      "Epoch 985: 0.010564280673861504 0.1724673956632614 9.605960100000001e-05\n",
      "Epoch 986: 0.010998168028891087 0.17184226214885712 9.605960100000001e-05\n",
      "Epoch 987: 0.01066985446959734 0.1724991351366043 9.605960100000001e-05\n",
      "Epoch 988: 0.01111701037734747 0.1726442128419876 9.605960100000001e-05\n",
      "Epoch 989: 0.010890470817685127 0.1733352243900299 9.605960100000001e-05\n",
      "Epoch 990: 0.011146754957735538 0.1722356379032135 9.605960100000001e-05\n",
      "Epoch 991: 0.010959716513752937 0.17339767515659332 9.605960100000001e-05\n",
      "Epoch 992: 0.011398760601878166 0.17238745093345642 9.605960100000001e-05\n",
      "Epoch 993: 0.011037320829927921 0.17375925183296204 9.605960100000001e-05\n",
      "Epoch 994: 0.011075153946876526 0.17252971231937408 9.605960100000001e-05\n",
      "Epoch 995: 0.011303176172077656 0.173392653465271 9.605960100000001e-05\n",
      "Epoch 996: 0.011090273968875408 0.1728670299053192 9.605960100000001e-05\n",
      "Epoch 997: 0.011376850306987762 0.17270419001579285 9.605960100000001e-05\n",
      "Epoch 998: 0.01089341752231121 0.17294900119304657 9.605960100000001e-05\n",
      "Epoch 999: 0.011750713922083378 0.17197442054748535 9.605960100000001e-05\n"
     ]
    }
   ],
   "source": [
    "from model import MyNet\n",
    "from dataset import MyDataset1\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "def train():\n",
    "    train_dataset = MyDataset1(input_file='/home/cusps/桌面/ML_disorder/data0/train/train_data.npy', \n",
    "                         label_file='/home/cusps/桌面/ML_disorder/data0/train/train_labels.npy')\n",
    "\n",
    "    vali_dataset = MyDataset1(input_file='/home/cusps/桌面/ML_disorder/data0/vali/vali_data.npy', \n",
    "                         label_file='/home/cusps/桌面/ML_disorder/data0/vali/vali_labels.npy')\n",
    "    \n",
    "    net = MyNet(seq_num=train_dataset.in_shape[1], out_dim=train_dataset.out_shape[1], hidden_dim=1024).cuda()\n",
    "    \n",
    "    # net.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "    # net.train()\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16384 * 2, shuffle=True)\n",
    "    vali_dataloader = DataLoader(vali_dataset, batch_size=16384 * 2, shuffle=True)\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.99, patience=100, threshold=1e-4, \n",
    "                                                           threshold_mode='rel',cooldown=100, min_lr=1e-20)\n",
    "    epoch = 1000\n",
    "\n",
    "    record_loss = []\n",
    "    for eid in range(epoch):\n",
    "        all_loss = []\n",
    "        vali_loss = []\n",
    "\n",
    "        net_train = net.train()\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            out = net(input)\n",
    "            # loss = torch.nn.MSELoss()(out, batch['label'].cuda())\n",
    "            loss = torch.nn.functional.l1_loss(out, batch['label'].cuda())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            all_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "            \n",
    "        net_vali = net.eval()\n",
    "        for j, batch in enumerate(vali_dataloader):\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            out = net(input)\n",
    "            # loss = torch.nn.MSELoss()(out, batch['label'].cuda())\n",
    "            loss = torch.nn.functional.l1_loss(out, batch['label'].cuda())\n",
    "            vali_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "            \n",
    "        print(f\"Epoch {eid}: {np.mean(all_loss)} {np.mean(vali_loss)} {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "        scheduler.step(np.mean(all_loss))\n",
    "        writer.add_scalar(\"loss: \",np.mean(all_loss), global_step=eid)\n",
    "        writer.add_scalar(\"learn rate: \",optimizer.state_dict()['param_groups'][0]['lr'], global_step=eid)\n",
    "\n",
    "        scheduler.step(np.mean(vali_loss))\n",
    "        \n",
    "        record_loss.append(np.mean(vali_loss))\n",
    "        writer.add_scalar(\"validation loss: \",np.mean(vali_loss), global_step=eid)\n",
    "\n",
    "\n",
    "        \n",
    "        if  record_loss[eid] == min(record_loss):\n",
    "            torch.save(net.state_dict(), 'model_weights.pth')\n",
    "            print(\"save model\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834bcbd-57b2-4bdc-ad78-436e0ef3c1b6",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "334b6ee5-778f-45b0-93a0-4e3e9304704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test l1 Loss: 0.18247239291667938\n"
     ]
    }
   ],
   "source": [
    "from model import MyNet\n",
    "from dataset import MyDataset1\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def test():\n",
    "    # 加载测试数据集\n",
    "    test_dataset = MyDataset1(input_file='/home/cusps/桌面/ML_disorder/data0/test/test_data.npy',\n",
    "                              label_file='/home/cusps/桌面/ML_disorder/data0/test/test_labels.npy')\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16384, shuffle=False)\n",
    "\n",
    "    # 初始化模型\n",
    "    net = MyNet(seq_num=test_dataset.in_shape[1], out_dim=test_dataset.out_shape[1], hidden_dim=1024).cuda().eval()\n",
    "    \n",
    "    # 加载训练好的权重\n",
    "    net.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "    \n",
    "    # 测试模型\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            predictions = net(input)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(batch['label'].numpy())\n",
    "    \n",
    "    # 合并所有批次的预测结果和真实标签\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "\n",
    "    # mse_loss = np.mean((all_predictions - all_labels) ** 2)\n",
    "    # print(f\"Test MSE Loss: {mse_loss}\")\n",
    "\n",
    "    l1_loss = np.mean(np.abs(all_predictions - all_labels))\n",
    "    print(f\"Test l1 Loss: {l1_loss}\")\n",
    "    \n",
    "    \n",
    "    # 可选：保存预测结果\n",
    "    np.save('test_predictions.npy', all_predictions)\n",
    "    np.save('test_labels.npy', all_labels)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c8878a-daf9-4619-8890-a6d19596243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5452303 0.9997776 0.9869819\n"
     ]
    }
   ],
   "source": [
    "'''inner product fidelity'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "exact_label = np.load(\"test_labels.npy\")\n",
    "prediction_label = np.load(\"test_predictions.npy\")\n",
    "\n",
    "\n",
    "F_list = []\n",
    "def calculate_F(A, B):\n",
    "    return np.dot(A, B)/np.sqrt(np.dot(A, A) * np.dot(B, B))\n",
    "\n",
    "for i in range(exact_label.shape[0]):\n",
    "    F = calculate_F(exact_label[i], prediction_label[i])\n",
    "    F_list.append(F)\n",
    "\n",
    "print(min(F_list),max(F_list),np.mean(F_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1fbbc5-e51f-4fad-9bc8-34bdb3355b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''所有F的分布'''\n",
    "\n",
    "%matplotlib tk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(F_list)),F_list,'o')\n",
    "plt.xlim(0,1000)\n",
    "plt.title('F distribution')\n",
    "plt.ylabel('F value')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0bf235d-cd08-4027-9724-051e30dac9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.9564235620200634\n"
     ]
    }
   ],
   "source": [
    "'''R2'''\n",
    "import numpy as np\n",
    "\n",
    "def calculate_r2(label_list, pred_list):\n",
    "\n",
    "    label_array = np.array(label_list)\n",
    "    pred_array = np.array(pred_list)\n",
    "\n",
    "    ss_res = np.sum((label_array - pred_array) ** 2)\n",
    "    ss_tot = np.sum((label_array - np.mean(label_array, axis=0) ) ** 2)\n",
    "\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "r2 = calculate_r2(exact_label, prediction_label)\n",
    "print(f\"R2 = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab76a6-4928-4bf8-9984-4b5946099b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV files\n",
    "train_loss = pd.read_csv('loss/trainloss.csv')\n",
    "x1 = train_loss.iloc[:, 1]  \n",
    "y1 = train_loss.iloc[:, 2]  \n",
    "\n",
    "\n",
    "vali_loss = pd.read_csv('loss/validationloss.csv')\n",
    "x2 = vali_loss.iloc[:, 1]  \n",
    "y2 = vali_loss.iloc[:, 2]  \n",
    "\n",
    "\n",
    "# 绘制图形\n",
    "plt.plot(x1, y1,label='train')\n",
    "plt.plot(x2, y2,label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f46e6-8532-47c7-b4bb-9a85ebb185b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''测试无disorder的表现情况'''\n",
    "\n",
    "from model import MyNet\n",
    "from dataset import MyDataset1\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def test():\n",
    "\n",
    "    # 初始化模型\n",
    "    net = MyNet(seq_num=test_dataset.in_shape[1], out_dim=test_dataset.out_shape[1], hidden_dim=1024).cuda().eval()\n",
    "    \n",
    "    # 加载训练好的权重\n",
    "    net.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "    \n",
    "    # 测试模型\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            predictions = net(input)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(batch['label'].numpy())\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
