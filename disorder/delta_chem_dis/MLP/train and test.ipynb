{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6e665f-e083-4d42-8c4d-b2846146aab1",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0a4324e-68a4-4f9c-9b38-abfdcbf27026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 5, 15, 15, 4)\n",
      "(8000, 2, 5)\n",
      "(8000, 4500)\n",
      "(8000, 10)\n",
      "0 torch.Size([8000, 4500]) torch.Size([8000, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset1(Dataset):\n",
    "    def __init__(self, input_file='conductance_datasets.npy', label_file='Y_labels.npy'):\n",
    "        datas = np.load(input_file)\n",
    "        self.ori_in_shape = datas.shape\n",
    "        datas = datas.reshape(datas.shape[0], -1)\n",
    "        self.in_shape = datas.shape\n",
    "        self.datas = datas\n",
    "\n",
    "\n",
    "        labels = np.load(label_file)\n",
    "        self.ori_out_shape = labels.shape\n",
    "        labels = labels.reshape(labels.shape[0], -1)\n",
    "        self.out_shape = labels.shape\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input\": torch.from_numpy(self.datas[idx]).float(), \"label\":  torch.from_numpy(self.labels[idx]).float()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = \"/home/cusps/python/ML_nanowire/disorder/delta_chem_dis/delta_chem_dis\"\n",
    "\n",
    "    dataset = MyDataset1(input_file = path + '/train/train_data.npy', label_file= path + '/train/train_labels.npy')\n",
    "    # dataset = MyDataset1(input_file = path + '/test/test_data.npy', label_file= path + '/test/test_labels.npy')\n",
    "    # dataset = MyDataset1(input_file = path + '/vali/vali_data.npy', label_file= path + '/vali/vali_labels.npy')\n",
    "\n",
    "    print(dataset.ori_in_shape)\n",
    "    print(dataset.ori_out_shape)\n",
    "\n",
    "    print(dataset.in_shape)\n",
    "    print(dataset.out_shape)\n",
    "\n",
    "    data_dataloader = DataLoader(dataset, batch_size=10000, shuffle=True)\n",
    "    for i, d in enumerate(data_dataloader):\n",
    "        print(i, d['input'].shape, d['label'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb8eba-3c2b-4e0a-a811-88a3317266a5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86dc10e-a8e6-439e-870a-ec219cd59368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self, seq_num=4500, out_dim=10, hidden_dim=1024) -> None:\n",
    "        super(MyNet, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(seq_num, hidden_dim), torch.nn.ReLU(), torch.nn.BatchNorm1d(hidden_dim),#torch.nn.Dropout(0.1),\n",
    "                                       torch.nn.Linear(hidden_dim, hidden_dim // 2), torch.nn.ReLU(), torch.nn.BatchNorm1d(hidden_dim // 2),#torch.nn.Dropout(0.1),\n",
    "                                       torch.nn.Linear(hidden_dim // 2, hidden_dim // 4), torch.nn.ReLU(), torch.nn.BatchNorm1d(hidden_dim // 4),#torch.nn.Dropout(0.1),\n",
    "                                       torch.nn.Linear(hidden_dim // 4, hidden_dim // 8), torch.nn.ReLU(), torch.nn.BatchNorm1d(hidden_dim // 8),\n",
    "                                       torch.nn.Linear(hidden_dim // 8, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x.squeeze())\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = MyNet().cuda()\n",
    "    input = torch.rand([16, 4500]).cuda()\n",
    "    out = net(input)\n",
    "    print(out.shape)\n",
    "\n",
    "    # # 输出各层的参数数量\n",
    "    # for name, param in net.named_parameters():\n",
    "    #     print(f\"Layer: {name}, Parameters: {param.numel()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cc707-b33a-47ec-a6b5-ccfbb81a01dd",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216d735d-b160-4d80-baa0-0883e890561c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.45995262265205383 0.1287875473499298 0.0001\n",
      "save model\n",
      "Epoch 1: 0.3186526298522949 0.12868452072143555 0.0001\n",
      "save model\n",
      "Epoch 2: 0.25823432207107544 0.1289598047733307 0.0001\n",
      "Epoch 3: 0.21425428986549377 0.12925595045089722 0.0001\n",
      "Epoch 4: 0.1820845752954483 0.12959882616996765 0.0001\n",
      "Epoch 5: 0.15886089205741882 0.12997519969940186 0.0001\n",
      "Epoch 6: 0.1413014680147171 0.13036158680915833 0.0001\n",
      "Epoch 7: 0.12713557481765747 0.13069196045398712 0.0001\n",
      "Epoch 8: 0.11492162942886353 0.13091078400611877 0.0001\n",
      "Epoch 9: 0.1045125424861908 0.13094885647296906 0.0001\n",
      "Epoch 10: 0.09576442837715149 0.13075608015060425 0.0001\n",
      "Epoch 11: 0.08870066702365875 0.13034303486347198 0.0001\n",
      "Epoch 12: 0.08275318145751953 0.12972614169120789 0.0001\n",
      "Epoch 13: 0.07766813784837723 0.12891417741775513 0.0001\n",
      "Epoch 14: 0.07312433421611786 0.12788966298103333 0.0001\n",
      "save model\n",
      "Epoch 15: 0.0690755844116211 0.12666384875774384 0.0001\n",
      "save model\n",
      "Epoch 16: 0.0654187947511673 0.12524613738059998 0.0001\n",
      "save model\n",
      "Epoch 17: 0.0621301494538784 0.12361283600330353 0.0001\n",
      "save model\n",
      "Epoch 18: 0.05922308191657066 0.12175361067056656 0.0001\n",
      "save model\n",
      "Epoch 19: 0.056657664477825165 0.11967329680919647 0.0001\n",
      "save model\n",
      "Epoch 20: 0.054330162703990936 0.11732172220945358 0.0001\n",
      "save model\n",
      "Epoch 21: 0.05216941237449646 0.11473546922206879 0.0001\n",
      "save model\n",
      "Epoch 22: 0.0501704216003418 0.11192107945680618 0.0001\n",
      "save model\n",
      "Epoch 23: 0.04831879213452339 0.10888823866844177 0.0001\n",
      "save model\n",
      "Epoch 24: 0.04663340374827385 0.10563258826732635 0.0001\n",
      "save model\n",
      "Epoch 25: 0.045080993324518204 0.1021515354514122 0.0001\n",
      "save model\n",
      "Epoch 26: 0.043624863028526306 0.09857792407274246 0.0001\n",
      "save model\n",
      "Epoch 27: 0.04225798696279526 0.09499713778495789 0.0001\n",
      "save model\n",
      "Epoch 28: 0.04098202660679817 0.09139162302017212 0.0001\n",
      "save model\n",
      "Epoch 29: 0.0397874116897583 0.08772920072078705 0.0001\n",
      "save model\n",
      "Epoch 30: 0.03866078332066536 0.08407599478960037 0.0001\n",
      "save model\n",
      "Epoch 31: 0.037602800875902176 0.08046059310436249 0.0001\n",
      "save model\n",
      "Epoch 32: 0.03660828620195389 0.07692891359329224 0.0001\n",
      "save model\n",
      "Epoch 33: 0.03566667437553406 0.07350847125053406 0.0001\n",
      "save model\n",
      "Epoch 34: 0.03476951643824577 0.07026723772287369 0.0001\n",
      "save model\n",
      "Epoch 35: 0.03391750529408455 0.06726802885532379 0.0001\n",
      "save model\n",
      "Epoch 36: 0.03310828655958176 0.06453127413988113 0.0001\n",
      "save model\n",
      "Epoch 37: 0.03232954069972038 0.062078170478343964 0.0001\n",
      "save model\n",
      "Epoch 38: 0.03157990425825119 0.059873100370168686 0.0001\n",
      "save model\n",
      "Epoch 39: 0.030871709808707237 0.05786683037877083 0.0001\n",
      "save model\n",
      "Epoch 40: 0.030180595815181732 0.056027915328741074 0.0001\n",
      "save model\n",
      "Epoch 41: 0.029533065855503082 0.05432623252272606 0.0001\n",
      "save model\n",
      "Epoch 42: 0.028908001258969307 0.05271388962864876 0.0001\n",
      "save model\n",
      "Epoch 43: 0.02830684185028076 0.05119752511382103 0.0001\n",
      "save model\n",
      "Epoch 44: 0.027728984132409096 0.049782346934080124 0.0001\n",
      "save model\n",
      "Epoch 45: 0.02717370167374611 0.048500318080186844 0.0001\n",
      "save model\n",
      "Epoch 46: 0.026638781651854515 0.04734668508172035 0.0001\n",
      "save model\n",
      "Epoch 47: 0.026120027527213097 0.04632271081209183 0.0001\n",
      "save model\n",
      "Epoch 48: 0.025619378313422203 0.04544449597597122 0.0001\n",
      "save model\n",
      "Epoch 49: 0.025135856121778488 0.04470815882086754 0.0001\n",
      "save model\n",
      "Epoch 50: 0.02466791681945324 0.04408114776015282 0.0001\n",
      "save model\n",
      "Epoch 51: 0.024213355034589767 0.043536536395549774 0.0001\n",
      "save model\n",
      "Epoch 52: 0.023772358894348145 0.0430518202483654 0.0001\n",
      "save model\n",
      "Epoch 53: 0.023346783593297005 0.0425763875246048 0.0001\n",
      "save model\n",
      "Epoch 54: 0.022934291511774063 0.04213840886950493 0.0001\n",
      "save model\n",
      "Epoch 55: 0.022531654685735703 0.04173286631703377 0.0001\n",
      "save model\n",
      "Epoch 56: 0.022140689194202423 0.04136141389608383 0.0001\n",
      "save model\n",
      "Epoch 57: 0.021759692579507828 0.04102049022912979 0.0001\n",
      "save model\n",
      "Epoch 58: 0.021389424800872803 0.04070687294006348 0.0001\n",
      "save model\n",
      "Epoch 59: 0.021025827154517174 0.040427546948194504 0.0001\n",
      "save model\n",
      "Epoch 60: 0.020675180479884148 0.04017242416739464 0.0001\n",
      "save model\n",
      "Epoch 61: 0.020330479368567467 0.03994673117995262 0.0001\n",
      "save model\n",
      "Epoch 62: 0.01999671570956707 0.03975965827703476 0.0001\n",
      "save model\n",
      "Epoch 63: 0.019669875502586365 0.039606254547834396 0.0001\n",
      "save model\n",
      "Epoch 64: 0.019351011142134666 0.03948444873094559 0.0001\n",
      "save model\n",
      "Epoch 65: 0.019040729850530624 0.03937710449099541 0.0001\n",
      "save model\n",
      "Epoch 66: 0.01873853988945484 0.039281003177165985 0.0001\n",
      "save model\n",
      "Epoch 67: 0.01844676397740841 0.03919284790754318 0.0001\n",
      "save model\n",
      "Epoch 68: 0.0181538388133049 0.0391080379486084 0.0001\n",
      "save model\n",
      "Epoch 69: 0.017871690914034843 0.03901723772287369 0.0001\n",
      "save model\n",
      "Epoch 70: 0.017595453187823296 0.038923393934965134 0.0001\n",
      "save model\n",
      "Epoch 71: 0.017325110733509064 0.0388268418610096 0.0001\n",
      "save model\n",
      "Epoch 72: 0.017060980200767517 0.03872711583971977 0.0001\n",
      "save model\n",
      "Epoch 73: 0.016802096739411354 0.03862623870372772 0.0001\n",
      "save model\n",
      "Epoch 74: 0.01654868572950363 0.03853866457939148 0.0001\n",
      "save model\n",
      "Epoch 75: 0.016339100897312164 0.03847026452422142 0.0001\n",
      "save model\n",
      "Epoch 76: 0.016057319939136505 0.03840900957584381 0.0001\n",
      "save model\n",
      "Epoch 77: 0.015819445252418518 0.03835269808769226 0.0001\n",
      "save model\n",
      "Epoch 78: 0.01558554358780384 0.03830614686012268 0.0001\n",
      "save model\n",
      "Epoch 79: 0.015359907411038876 0.038270145654678345 0.0001\n",
      "save model\n",
      "Epoch 80: 0.015132405795156956 0.03824680671095848 0.0001\n",
      "save model\n",
      "Epoch 81: 0.014912321232259274 0.0382232666015625 0.0001\n",
      "save model\n",
      "Epoch 82: 0.014696198515594006 0.038199011236429214 0.0001\n",
      "save model\n",
      "Epoch 83: 0.014484594576060772 0.0381801500916481 0.0001\n",
      "save model\n",
      "Epoch 84: 0.014276989735662937 0.038159191608428955 0.0001\n",
      "save model\n",
      "Epoch 85: 0.014073298312723637 0.038150712847709656 0.0001\n",
      "save model\n",
      "Epoch 86: 0.013873610645532608 0.038152918219566345 0.0001\n",
      "Epoch 87: 0.013678028248250484 0.0381428562104702 0.0001\n",
      "save model\n",
      "Epoch 88: 0.013485509902238846 0.038131386041641235 0.0001\n",
      "save model\n",
      "Epoch 89: 0.013297097757458687 0.038135942071676254 0.0001\n",
      "Epoch 90: 0.013111375272274017 0.0381467230618 0.0001\n",
      "Epoch 91: 0.012929558753967285 0.03814288228750229 0.0001\n",
      "Epoch 92: 0.012750600464642048 0.03813735768198967 0.0001\n",
      "Epoch 93: 0.012575676664710045 0.03813629597425461 0.0001\n",
      "Epoch 94: 0.01240247581154108 0.03812811151146889 0.0001\n",
      "save model\n",
      "Epoch 95: 0.012233089655637741 0.038112103939056396 0.0001\n",
      "save model\n",
      "Epoch 96: 0.012065216898918152 0.0381094291806221 0.0001\n",
      "save model\n",
      "Epoch 97: 0.011902163736522198 0.03812745213508606 0.0001\n",
      "Epoch 98: 0.011739722453057766 0.03813709318637848 0.0001\n",
      "Epoch 99: 0.011581906117498875 0.038122840225696564 0.0001\n",
      "Epoch 100: 0.011425399221479893 0.03811445087194443 0.0001\n",
      "Epoch 101: 0.011272608302533627 0.03812139853835106 0.0001\n",
      "Epoch 102: 0.011121369898319244 0.03813515231013298 0.0001\n",
      "Epoch 103: 0.010974167846143246 0.038143713027238846 0.0001\n",
      "Epoch 104: 0.01082757581025362 0.03814718499779701 0.0001\n",
      "Epoch 105: 0.0106851477175951 0.03814319521188736 0.0001\n",
      "Epoch 106: 0.01054394617676735 0.03813663125038147 0.0001\n",
      "Epoch 107: 0.010405249893665314 0.038133300840854645 0.0001\n",
      "Epoch 108: 0.010268769226968288 0.03811072185635567 0.0001\n",
      "Epoch 109: 0.010134998708963394 0.03808406740427017 0.0001\n",
      "save model\n",
      "Epoch 110: 0.010002078488469124 0.03805717080831528 0.0001\n",
      "save model\n",
      "Epoch 111: 0.009872755035758018 0.03802572190761566 0.0001\n",
      "save model\n",
      "Epoch 112: 0.009744781069457531 0.03801052272319794 0.0001\n",
      "save model\n",
      "Epoch 113: 0.009618405252695084 0.03800273686647415 0.0001\n",
      "save model\n",
      "Epoch 114: 0.009495412930846214 0.03798165172338486 0.0001\n",
      "save model\n",
      "Epoch 115: 0.00937360618263483 0.037958793342113495 0.0001\n",
      "save model\n",
      "Epoch 116: 0.009253498166799545 0.03795242682099342 0.0001\n",
      "save model\n",
      "Epoch 117: 0.00913587212562561 0.03794137015938759 0.0001\n",
      "save model\n",
      "Epoch 118: 0.009019565768539906 0.03853610157966614 0.0001\n",
      "Epoch 119: 0.008976387791335583 0.03786247596144676 0.0001\n",
      "save model\n",
      "Epoch 120: 0.00879556406289339 0.037816062569618225 0.0001\n",
      "save model\n",
      "Epoch 121: 0.008683898486196995 0.03777023032307625 0.0001\n",
      "save model\n",
      "Epoch 122: 0.008574392646551132 0.03775256127119064 0.0001\n",
      "save model\n",
      "Epoch 123: 0.00846700370311737 0.0377655029296875 0.0001\n",
      "Epoch 124: 0.008360981941223145 0.037725016474723816 0.0001\n",
      "save model\n",
      "Epoch 125: 0.008257254958152771 0.037667643278837204 0.0001\n",
      "save model\n",
      "Epoch 126: 0.008154266513884068 0.037652019411325455 0.0001\n",
      "save model\n",
      "Epoch 127: 0.008053619414567947 0.03763915225863457 0.0001\n",
      "save model\n",
      "Epoch 128: 0.007953431457281113 0.037634264677762985 0.0001\n",
      "save model\n",
      "Epoch 129: 0.007855621166527271 0.03761075809597969 0.0001\n",
      "save model\n",
      "Epoch 130: 0.007759266998618841 0.03756806626915932 0.0001\n",
      "save model\n",
      "Epoch 131: 0.007663583382964134 0.03754374384880066 0.0001\n",
      "save model\n",
      "Epoch 132: 0.007569967303425074 0.037528105080127716 0.0001\n",
      "save model\n",
      "Epoch 133: 0.007478794082999229 0.03748735785484314 0.0001\n",
      "save model\n",
      "Epoch 134: 0.007386947516351938 0.03746655955910683 0.0001\n",
      "save model\n",
      "Epoch 135: 0.007297806441783905 0.038760822266340256 0.0001\n",
      "Epoch 136: 0.007262736558914185 0.037408482283353806 0.0001\n",
      "save model\n",
      "Epoch 137: 0.007124950177967548 0.037370871752500534 0.0001\n",
      "save model\n",
      "Epoch 138: 0.007040153257548809 0.03734935075044632 0.0001\n",
      "save model\n",
      "Epoch 139: 0.006955217570066452 0.03732366859912872 0.0001\n",
      "save model\n",
      "Epoch 140: 0.0068725524470210075 0.037296075373888016 0.0001\n",
      "save model\n",
      "Epoch 141: 0.006790316663682461 0.03728284314274788 0.0001\n",
      "save model\n",
      "Epoch 142: 0.00670985784381628 0.03726658970117569 0.0001\n",
      "save model\n",
      "Epoch 143: 0.006630309857428074 0.03726094961166382 0.0001\n",
      "save model\n",
      "Epoch 144: 0.006551900878548622 0.0372774600982666 0.0001\n",
      "Epoch 145: 0.006474475841969252 0.037250567227602005 0.0001\n",
      "save model\n",
      "Epoch 146: 0.006398191209882498 0.03722267225384712 0.0001\n",
      "save model\n",
      "Epoch 147: 0.0063235885463654995 0.0372222438454628 0.0001\n",
      "save model\n",
      "Epoch 148: 0.006249985191971064 0.03720276430249214 0.0001\n",
      "save model\n",
      "Epoch 149: 0.0061771986074745655 0.03719508275389671 0.0001\n",
      "save model\n",
      "Epoch 150: 0.006105695851147175 0.03718094527721405 0.0001\n",
      "save model\n",
      "Epoch 151: 0.0060347034595906734 0.037136249244213104 0.0001\n",
      "save model\n",
      "Epoch 152: 0.005965180229395628 0.03712005913257599 0.0001\n",
      "save model\n",
      "Epoch 153: 0.005896630696952343 0.03710579872131348 0.0001\n",
      "save model\n",
      "Epoch 154: 0.00582922762259841 0.037081122398376465 0.0001\n",
      "save model\n",
      "Epoch 155: 0.005762637127190828 0.03708868473768234 0.0001\n",
      "Epoch 156: 0.005696884356439114 0.0370904803276062 0.0001\n",
      "Epoch 157: 0.005632535554468632 0.037069957703351974 0.0001\n",
      "save model\n",
      "Epoch 158: 0.005568286869674921 0.03707413375377655 0.0001\n",
      "Epoch 159: 0.005505762994289398 0.037060800939798355 0.0001\n",
      "save model\n",
      "Epoch 160: 0.005443902220577002 0.03700223192572594 0.0001\n",
      "save model\n",
      "Epoch 161: 0.005382824689149857 0.03952379524707794 0.0001\n",
      "Epoch 162: 0.0053783501498401165 0.043402042239904404 0.0001\n",
      "Epoch 163: 0.0053033833391964436 0.04178496450185776 0.0001\n",
      "Epoch 164: 0.005234902258962393 0.04019024595618248 0.0001\n",
      "Epoch 165: 0.005168789532035589 0.03902071341872215 0.0001\n",
      "Epoch 166: 0.005106509663164616 0.03826389089226723 0.0001\n",
      "Epoch 167: 0.00504740746691823 0.037897344678640366 0.0001\n",
      "Epoch 168: 0.004990197718143463 0.04010936617851257 0.0001\n",
      "Epoch 169: 0.00497088860720396 0.04164879396557808 0.0001\n",
      "Epoch 170: 0.004906031768769026 0.03960072994232178 0.0001\n",
      "Epoch 171: 0.004843326285481453 0.03858691453933716 0.0001\n",
      "Epoch 172: 0.00478405924513936 0.0380781926214695 0.0001\n",
      "Epoch 173: 0.004728355444967747 0.03772320970892906 0.0001\n",
      "Epoch 174: 0.0046743727289140224 0.037504907697439194 0.0001\n",
      "Epoch 175: 0.004621047526597977 0.03736603260040283 0.0001\n",
      "Epoch 176: 0.004569988697767258 0.037207670509815216 0.0001\n",
      "Epoch 177: 0.004519468639045954 0.03709601238369942 0.0001\n",
      "Epoch 178: 0.004469267092645168 0.03704524412751198 0.0001\n",
      "Epoch 179: 0.00442007789388299 0.036991070955991745 0.0001\n",
      "save model\n",
      "Epoch 180: 0.004371109884232283 0.036940500140190125 0.0001\n",
      "save model\n",
      "Epoch 181: 0.004322797060012817 0.03693654015660286 0.0001\n",
      "save model\n",
      "Epoch 182: 0.004276138264685869 0.03687090054154396 0.0001\n",
      "save model\n",
      "Epoch 183: 0.004229314625263214 0.03679327666759491 0.0001\n",
      "save model\n",
      "Epoch 184: 0.004183372016996145 0.03675596043467522 0.0001\n",
      "save model\n",
      "Epoch 185: 0.004138650372624397 0.03670991584658623 0.0001\n",
      "save model\n",
      "Epoch 186: 0.004093411844223738 0.036675311625003815 0.0001\n",
      "save model\n",
      "Epoch 187: 0.004049764480441809 0.03669454902410507 0.0001\n",
      "Epoch 188: 0.00400595972314477 0.036687031388282776 0.0001\n",
      "Epoch 189: 0.003962947521358728 0.03664807602763176 0.0001\n",
      "save model\n",
      "Epoch 190: 0.003921074792742729 0.03662361204624176 0.0001\n",
      "save model\n",
      "Epoch 191: 0.0038792353589087725 0.036617837846279144 0.0001\n",
      "save model\n",
      "Epoch 192: 0.003837532363831997 0.03660375252366066 0.0001\n",
      "save model\n",
      "Epoch 193: 0.003796821925789118 0.03660712391138077 0.0001\n",
      "Epoch 194: 0.003756333841010928 0.036607928574085236 0.0001\n",
      "Epoch 195: 0.003716522827744484 0.0365963876247406 0.0001\n",
      "save model\n",
      "Epoch 196: 0.003677739528939128 0.036593999713659286 0.0001\n",
      "save model\n",
      "Epoch 197: 0.0036389417946338654 0.03658733516931534 0.0001\n",
      "save model\n",
      "Epoch 198: 0.003601216711103916 0.03659743070602417 0.0001\n",
      "Epoch 199: 0.0035629866179078817 0.03657890483736992 0.0001\n",
      "save model\n",
      "Epoch 200: 0.0035256927367299795 0.03654898703098297 0.0001\n",
      "save model\n",
      "Epoch 201: 0.003488920396193862 0.03655038774013519 0.0001\n",
      "Epoch 202: 0.003452564822509885 0.037384215742349625 0.0001\n",
      "Epoch 203: 0.0034512996207922697 0.04741542413830757 0.0001\n",
      "Epoch 204: 0.003436060156673193 0.045354679226875305 0.0001\n",
      "Epoch 205: 0.003382498864084482 0.047236744314432144 0.0001\n",
      "Epoch 206: 0.003362779039889574 0.04348236694931984 0.0001\n",
      "Epoch 207: 0.0033126838970929384 0.04093917831778526 0.0001\n",
      "Epoch 208: 0.0032731120008975267 0.03981520235538483 0.0001\n",
      "Epoch 209: 0.0032335456926375628 0.03907214105129242 0.0001\n",
      "Epoch 210: 0.0031976273749023676 0.03848489746451378 0.0001\n",
      "Epoch 211: 0.0031625174451619387 0.038062598556280136 0.0001\n",
      "Epoch 212: 0.003127200296148658 0.037723880261182785 0.0001\n",
      "Epoch 213: 0.003094960004091263 0.03741730749607086 0.0001\n",
      "Epoch 214: 0.0030628328677266836 0.037219658493995667 0.0001\n",
      "Epoch 215: 0.0030302770901471376 0.037070758640766144 0.0001\n",
      "Epoch 216: 0.00299852411262691 0.03694136440753937 0.0001\n",
      "Epoch 217: 0.0029680514708161354 0.03684177249670029 0.0001\n",
      "Epoch 218: 0.002935386262834072 0.03675122931599617 0.0001\n",
      "Epoch 219: 0.0029062428511679173 0.03668440505862236 0.0001\n",
      "Epoch 220: 0.002875748323276639 0.03665025532245636 0.0001\n",
      "Epoch 221: 0.0028461709152907133 0.036605555564165115 0.0001\n",
      "Epoch 222: 0.002816941821947694 0.03654074668884277 0.0001\n",
      "save model\n",
      "Epoch 223: 0.0027873918879777193 0.036502886563539505 0.0001\n",
      "save model\n",
      "Epoch 224: 0.002758387941867113 0.03652564436197281 0.0001\n",
      "Epoch 225: 0.002730340464040637 0.03652755916118622 0.0001\n",
      "Epoch 226: 0.0027020622510463 0.036493998020887375 0.0001\n",
      "save model\n",
      "Epoch 227: 0.00267453002743423 0.03647492453455925 0.0001\n",
      "save model\n",
      "Epoch 228: 0.0026472369208931923 0.036465782672166824 0.0001\n",
      "save model\n",
      "Epoch 229: 0.002620463026687503 0.036487456411123276 0.0001\n",
      "Epoch 230: 0.002594549208879471 0.03647606819868088 0.0001\n",
      "Epoch 231: 0.0025675813667476177 0.03646985813975334 0.0001\n",
      "Epoch 232: 0.002540991175919771 0.036479540169239044 0.0001\n",
      "Epoch 233: 0.0025161574594676495 0.03647400066256523 0.0001\n",
      "Epoch 234: 0.002491001272574067 0.03649073839187622 0.0001\n",
      "Epoch 235: 0.002466046018525958 0.03649516403675079 0.0001\n",
      "Epoch 236: 0.0024415648076683283 0.03645645081996918 0.0001\n",
      "save model\n",
      "Epoch 237: 0.0024187483359128237 0.036491040140390396 0.0001\n",
      "Epoch 238: 0.0023947677109390497 0.036466602236032486 0.0001\n",
      "Epoch 239: 0.0023698662407696247 0.03645019233226776 0.0001\n",
      "save model\n",
      "Epoch 240: 0.002345643937587738 0.03647428750991821 0.0001\n",
      "Epoch 241: 0.0023234530817717314 0.036465875804424286 0.0001\n",
      "Epoch 242: 0.002302417764440179 0.03648705407977104 0.0001\n",
      "Epoch 243: 0.002278384752571583 0.03648732975125313 0.0001\n",
      "Epoch 244: 0.0022545503452420235 0.03650076687335968 0.0001\n",
      "Epoch 245: 0.002234038896858692 0.0365268774330616 0.0001\n",
      "Epoch 246: 0.002212339546531439 0.03649432957172394 0.0001\n",
      "Epoch 247: 0.0021901424042880535 0.03649049997329712 0.0001\n",
      "Epoch 248: 0.0021687212865799665 0.036474280059337616 0.0001\n",
      "Epoch 249: 0.002147877123206854 0.03647502511739731 0.0001\n",
      "Epoch 250: 0.0021279642824083567 0.03649555519223213 0.0001\n",
      "Epoch 251: 0.0021074882242828608 0.03649796172976494 0.0001\n",
      "Epoch 252: 0.00208655116148293 0.03651835396885872 0.0001\n",
      "Epoch 253: 0.002066694898530841 0.03652181848883629 0.0001\n",
      "Epoch 254: 0.0020467655267566442 0.03650691360235214 0.0001\n",
      "Epoch 255: 0.002027531387284398 0.03650474175810814 0.0001\n",
      "Epoch 256: 0.0020085179712623358 0.036493171006441116 0.0001\n",
      "Epoch 257: 0.001989576267078519 0.03648937866091728 0.0001\n",
      "Epoch 258: 0.001970280660316348 0.036507975310087204 0.0001\n",
      "Epoch 259: 0.0019516010070219636 0.0364861823618412 0.0001\n",
      "Epoch 260: 0.0019331962103024125 0.03650191053748131 0.0001\n",
      "Epoch 261: 0.0019149559084326029 0.0365365706384182 0.0001\n",
      "Epoch 262: 0.00189784518443048 0.03652184456586838 0.0001\n",
      "Epoch 263: 0.001881635282188654 0.03650203347206116 0.0001\n",
      "Epoch 264: 0.0018637103494256735 0.036529138684272766 0.0001\n",
      "Epoch 265: 0.0018457378027960658 0.03654100373387337 0.0001\n",
      "Epoch 266: 0.001829054206609726 0.03652281314134598 0.0001\n",
      "Epoch 267: 0.0018117846921086311 0.03652471303939819 0.0001\n",
      "Epoch 268: 0.001794737414456904 0.03655006363987923 0.0001\n",
      "Epoch 269: 0.0017800331115722656 0.03652171418070793 0.0001\n",
      "Epoch 270: 0.0017636817647144198 0.03651691973209381 0.0001\n",
      "Epoch 271: 0.0017473140032961965 0.03655335679650307 0.0001\n",
      "Epoch 272: 0.0017315397271886468 0.036525219678878784 0.0001\n",
      "Epoch 273: 0.0017153468215838075 0.036530930548906326 0.0001\n",
      "Epoch 274: 0.0017000583466142416 0.03657383471727371 0.0001\n",
      "Epoch 275: 0.0016840120078995824 0.03655814006924629 0.0001\n",
      "Epoch 276: 0.0016688208561390638 0.03652619943022728 0.0001\n",
      "Epoch 277: 0.001654348336160183 0.0365605466067791 0.0001\n",
      "Epoch 278: 0.0016397401923313737 0.0365438349545002 0.0001\n",
      "Epoch 279: 0.0016243434511125088 0.036550868302583694 0.0001\n",
      "Epoch 280: 0.0016092932783067226 0.036536235362291336 0.0001\n",
      "Epoch 281: 0.0015954733826220036 0.03657500818371773 0.0001\n",
      "Epoch 282: 0.0015806937590241432 0.03658399358391762 0.0001\n",
      "Epoch 283: 0.0015672065783292055 0.03657850995659828 0.0001\n",
      "Epoch 284: 0.0015537099679931998 0.03656116873025894 0.0001\n",
      "Epoch 285: 0.0015398628311231732 0.03661726415157318 0.0001\n",
      "Epoch 286: 0.0015265694819390774 0.036577124148607254 0.0001\n",
      "Epoch 287: 0.0015124250203371048 0.03662114217877388 0.0001\n",
      "Epoch 288: 0.0014994119992479682 0.03662484511733055 0.0001\n",
      "Epoch 289: 0.0014875393826514482 0.03660609573125839 0.0001\n",
      "Epoch 290: 0.0014759335899725556 0.0366102010011673 0.0001\n",
      "Epoch 291: 0.001459814957343042 0.03662783280014992 0.0001\n",
      "Epoch 292: 0.001446857932023704 0.03662583604454994 0.0001\n",
      "Epoch 293: 0.0014359590131789446 0.036621179431676865 0.0001\n",
      "Epoch 294: 0.001422273926436901 0.042856186628341675 0.0001\n",
      "Epoch 295: 0.0014224493643268943 0.03885453939437866 0.0001\n",
      "Epoch 296: 0.001409969525411725 0.04006921127438545 0.0001\n",
      "Epoch 297: 0.0013917475007474422 0.03872612118721008 0.0001\n",
      "Epoch 298: 0.0013789396034553647 0.037652965635061264 0.0001\n",
      "Epoch 299: 0.0013664704747498035 0.03959899768233299 0.0001\n",
      "Epoch 300: 0.0013610322494059801 0.03852792829275131 0.0001\n",
      "Epoch 301: 0.0013520584907382727 0.03956757113337517 0.0001\n",
      "Epoch 302: 0.001341913710348308 0.03928665444254875 0.0001\n",
      "Epoch 303: 0.0013293544761836529 0.039079587906599045 0.0001\n",
      "Epoch 304: 0.001315808854997158 0.03811167553067207 0.0001\n",
      "Epoch 305: 0.0013033058494329453 0.037501491606235504 0.0001\n",
      "Epoch 306: 0.0012896443950012326 0.037193119525909424 0.0001\n",
      "Epoch 307: 0.0012775369687005877 0.03693678602576256 0.0001\n",
      "Epoch 308: 0.0012664319947361946 0.03684360533952713 0.0001\n",
      "Epoch 309: 0.0012546651996672153 0.0368390753865242 0.0001\n",
      "Epoch 310: 0.0012436752440407872 0.03676433861255646 0.0001\n",
      "Epoch 311: 0.0012314098421484232 0.03676146641373634 0.0001\n",
      "Epoch 312: 0.0012211687862873077 0.03678206726908684 0.0001\n",
      "Epoch 313: 0.0012111896649003029 0.03671013563871384 0.0001\n",
      "Epoch 314: 0.0011999114649370313 0.036711569875478745 0.0001\n",
      "Epoch 315: 0.0011893264017999172 0.03671784698963165 0.0001\n",
      "Epoch 316: 0.0011789218988269567 0.0366826131939888 0.0001\n",
      "Epoch 317: 0.0011683284537866712 0.03668680414557457 0.0001\n",
      "Epoch 318: 0.0011584912426769733 0.03668847680091858 0.0001\n",
      "Epoch 319: 0.0011484212009236217 0.03672299161553383 0.0001\n",
      "Epoch 320: 0.0011389987776055932 0.03670969605445862 0.0001\n",
      "Epoch 321: 0.0011294697178527713 0.036689113825559616 0.0001\n",
      "Epoch 322: 0.0011199078289791942 0.03672119230031967 0.0001\n",
      "Epoch 323: 0.001111122197471559 0.03672740235924721 0.0001\n",
      "Epoch 324: 0.0011023350525647402 0.03670208528637886 0.0001\n",
      "Epoch 325: 0.00109360262285918 0.036749616265296936 0.0001\n",
      "Epoch 326: 0.001084693823941052 0.03755595162510872 0.0001\n",
      "Epoch 327: 0.0010794297559186816 0.03678617626428604 0.0001\n",
      "Epoch 328: 0.0010702399304136634 0.036794841289520264 0.0001\n",
      "Epoch 329: 0.0010561058297753334 0.036823004484176636 0.0001\n",
      "Epoch 330: 0.001047314377501607 0.03679386526346207 0.0001\n",
      "Epoch 331: 0.001038581714965403 0.0368223674595356 0.0001\n",
      "Epoch 332: 0.0010296676773577929 0.03683396428823471 0.0001\n",
      "Epoch 333: 0.0010207248851656914 0.03682491183280945 0.0001\n",
      "Epoch 334: 0.0010121801169589162 0.036880455911159515 0.0001\n",
      "Epoch 335: 0.0010042270878329873 0.0368342325091362 0.0001\n",
      "Epoch 336: 0.0009946620557457209 0.03682463988661766 0.0001\n",
      "Epoch 337: 0.0009862645529210567 0.03686002269387245 0.0001\n",
      "Epoch 338: 0.000977962277829647 0.03686857596039772 0.0001\n",
      "Epoch 339: 0.0009702305542305112 0.036866821348667145 0.0001\n",
      "Epoch 340: 0.000962414254900068 0.03689248487353325 0.0001\n",
      "Epoch 341: 0.000954869610723108 0.036855075508356094 9.900000000000001e-05\n",
      "Epoch 342: 0.0009466726914979517 0.0368797667324543 9.900000000000001e-05\n",
      "Epoch 343: 0.0009383966098539531 0.03689328581094742 9.900000000000001e-05\n",
      "Epoch 344: 0.0009305819985456765 0.03692300245165825 9.900000000000001e-05\n",
      "Epoch 345: 0.0009234537137672305 0.0368688739836216 9.900000000000001e-05\n",
      "Epoch 346: 0.0009159772889688611 0.03688843920826912 9.900000000000001e-05\n",
      "Epoch 347: 0.0009086753707379103 0.03690829500555992 9.900000000000001e-05\n",
      "Epoch 348: 0.00090171885676682 0.03692390024662018 9.900000000000001e-05\n",
      "Epoch 349: 0.0008940401603467762 0.0369117297232151 9.900000000000001e-05\n",
      "Epoch 350: 0.0008869654266163707 0.03693654388189316 9.900000000000001e-05\n",
      "Epoch 351: 0.0008806596742942929 0.036906249821186066 9.900000000000001e-05\n",
      "Epoch 352: 0.0008749677217565477 0.03696216270327568 9.900000000000001e-05\n",
      "Epoch 353: 0.0008669733651913702 0.036936141550540924 9.900000000000001e-05\n",
      "Epoch 354: 0.0008585838368162513 0.0369262658059597 9.900000000000001e-05\n",
      "Epoch 355: 0.0008518625982105732 0.03694987669587135 9.900000000000001e-05\n",
      "Epoch 356: 0.000844571681227535 0.03694191202521324 9.900000000000001e-05\n",
      "Epoch 357: 0.0008371633593924344 0.03696557506918907 9.900000000000001e-05\n",
      "Epoch 358: 0.0008303668582811952 0.036993809044361115 9.900000000000001e-05\n",
      "Epoch 359: 0.0008236936409957707 0.03696957975625992 9.900000000000001e-05\n",
      "Epoch 360: 0.0008176677511073649 0.03699752688407898 9.900000000000001e-05\n",
      "Epoch 361: 0.00081086769932881 0.03702796623110771 9.900000000000001e-05\n",
      "Epoch 362: 0.0008034619968384504 0.036978431046009064 9.900000000000001e-05\n",
      "Epoch 363: 0.00079720577923581 0.03700719028711319 9.900000000000001e-05\n",
      "Epoch 364: 0.0007907289545983076 0.037035852670669556 9.900000000000001e-05\n",
      "Epoch 365: 0.0007841602200642228 0.037023935467004776 9.900000000000001e-05\n",
      "Epoch 366: 0.0007787037175148726 0.03704327344894409 9.900000000000001e-05\n",
      "Epoch 367: 0.0007729842909611762 0.03702331334352493 9.900000000000001e-05\n",
      "Epoch 368: 0.00076723174424842 0.037058573216199875 9.900000000000001e-05\n",
      "Epoch 369: 0.0007629601750522852 0.03702690452337265 9.900000000000001e-05\n",
      "Epoch 370: 0.0007574919145554304 0.03709862381219864 9.900000000000001e-05\n",
      "Epoch 371: 0.000751336629036814 0.037039145827293396 9.900000000000001e-05\n",
      "Epoch 372: 0.0007442328496836126 0.037080176174640656 9.900000000000001e-05\n",
      "Epoch 373: 0.0007371233659796417 0.03708453103899956 9.900000000000001e-05\n",
      "Epoch 374: 0.0007308473577722907 0.037095122039318085 9.900000000000001e-05\n",
      "Epoch 375: 0.0007256424869410694 0.03709381818771362 9.900000000000001e-05\n",
      "Epoch 376: 0.0007200505933724344 0.03711334615945816 9.900000000000001e-05\n",
      "Epoch 377: 0.0007143706898204982 0.037113770842552185 9.900000000000001e-05\n",
      "Epoch 378: 0.0007080198847688735 0.03710408881306648 9.900000000000001e-05\n",
      "Epoch 379: 0.0007020683842711151 0.03715367987751961 9.900000000000001e-05\n",
      "Epoch 380: 0.0006967596127651632 0.0371210090816021 9.900000000000001e-05\n",
      "Epoch 381: 0.0006911379750818014 0.03711726516485214 9.900000000000001e-05\n",
      "Epoch 382: 0.0006860597641207278 0.037106096744537354 9.900000000000001e-05\n",
      "Epoch 383: 0.0006807558820582926 0.03716418519616127 9.900000000000001e-05\n",
      "Epoch 384: 0.0006758264498785138 0.03710823878645897 9.900000000000001e-05\n",
      "Epoch 385: 0.0006701952661387622 0.0371510311961174 9.900000000000001e-05\n",
      "Epoch 386: 0.0006652231095358729 0.03712105005979538 9.900000000000001e-05\n",
      "Epoch 387: 0.0006606286042369902 0.03714805096387863 9.900000000000001e-05\n",
      "Epoch 388: 0.0006557226879522204 0.03710968792438507 9.900000000000001e-05\n",
      "Epoch 389: 0.0006512418622151017 0.037174466997385025 9.900000000000001e-05\n",
      "Epoch 390: 0.0006459918222390115 0.037086524069309235 9.900000000000001e-05\n",
      "Epoch 391: 0.0006403867155313492 0.037172380834817886 9.900000000000001e-05\n",
      "Epoch 392: 0.0006347078597173095 0.037152647972106934 9.900000000000001e-05\n",
      "Epoch 393: 0.0006302249967120588 0.03717498481273651 9.900000000000001e-05\n",
      "Epoch 394: 0.0006267059361562133 0.03716803342103958 9.900000000000001e-05\n",
      "Epoch 395: 0.0006220841896720231 0.03718182072043419 9.900000000000001e-05\n",
      "Epoch 396: 0.0006171924178488553 0.03720115125179291 9.900000000000001e-05\n",
      "Epoch 397: 0.000611690164078027 0.03719798102974892 9.900000000000001e-05\n",
      "Epoch 398: 0.0006070753443054855 0.037242379039525986 9.900000000000001e-05\n",
      "Epoch 399: 0.0006031320663169026 0.037223778665065765 9.900000000000001e-05\n",
      "Epoch 400: 0.0005997536354698241 0.037263501435518265 9.900000000000001e-05\n",
      "Epoch 401: 0.0005941899144090712 0.03722906857728958 9.900000000000001e-05\n",
      "Epoch 402: 0.0005884587881155312 0.037227824330329895 9.900000000000001e-05\n",
      "Epoch 403: 0.0005844752304255962 0.03731147199869156 9.900000000000001e-05\n",
      "Epoch 404: 0.0005817831843160093 0.03720391169190407 9.900000000000001e-05\n",
      "Epoch 405: 0.0005776712787337601 0.037308089435100555 9.900000000000001e-05\n",
      "Epoch 406: 0.0005721964407712221 0.03723061457276344 9.900000000000001e-05\n",
      "Epoch 407: 0.0005673363339155912 0.03726420924067497 9.900000000000001e-05\n",
      "Epoch 408: 0.0005631322273984551 0.037226974964141846 9.900000000000001e-05\n",
      "Epoch 409: 0.0005589422071352601 0.037281136959791183 9.900000000000001e-05\n",
      "Epoch 410: 0.0005540364072658122 0.03724701330065727 9.900000000000001e-05\n",
      "Epoch 411: 0.0005498407408595085 0.03727306053042412 9.900000000000001e-05\n",
      "Epoch 412: 0.0005460642860271037 0.037268005311489105 9.900000000000001e-05\n",
      "Epoch 413: 0.0005420181551016867 0.03725205361843109 9.900000000000001e-05\n",
      "Epoch 414: 0.0005380091606639326 0.03726743534207344 9.900000000000001e-05\n",
      "Epoch 415: 0.0005331265274435282 0.037280723452568054 9.900000000000001e-05\n",
      "Epoch 416: 0.0005286858649924397 0.037265002727508545 9.900000000000001e-05\n",
      "Epoch 417: 0.0005249837995506823 0.03727119788527489 9.900000000000001e-05\n",
      "Epoch 418: 0.0005211925599724054 0.03724006935954094 9.900000000000001e-05\n",
      "Epoch 419: 0.0005173906101845205 0.03725425526499748 9.900000000000001e-05\n",
      "Epoch 420: 0.0005138258566148579 0.037276122719049454 9.900000000000001e-05\n",
      "Epoch 421: 0.000510369602125138 0.037261009216308594 9.900000000000001e-05\n",
      "Epoch 422: 0.0005073709762655199 0.037274304777383804 9.900000000000001e-05\n",
      "Epoch 423: 0.0005043139099143445 0.037277381867170334 9.900000000000001e-05\n",
      "Epoch 424: 0.0005010291934013367 0.03722864389419556 9.900000000000001e-05\n",
      "Epoch 425: 0.0004971817834302783 0.037309277802705765 9.900000000000001e-05\n",
      "Epoch 426: 0.0004937166813760996 0.03722449392080307 9.900000000000001e-05\n",
      "Epoch 427: 0.0004904975648969412 0.03733992576599121 9.900000000000001e-05\n",
      "Epoch 428: 0.0004890147247351706 0.03722744062542915 9.900000000000001e-05\n",
      "Epoch 429: 0.0004885672242380679 0.037330396473407745 9.900000000000001e-05\n",
      "Epoch 430: 0.00048156941193155944 0.03731758892536163 9.900000000000001e-05\n",
      "Epoch 431: 0.00047604378778487444 0.03739249333739281 9.900000000000001e-05\n",
      "Epoch 432: 0.0004721204750239849 0.037476714700460434 9.900000000000001e-05\n",
      "Epoch 433: 0.00046995547018013895 0.03745217248797417 9.900000000000001e-05\n",
      "Epoch 434: 0.00046719503006897867 0.03759972006082535 9.900000000000001e-05\n",
      "Epoch 435: 0.0004625532019417733 0.03753242641687393 9.900000000000001e-05\n",
      "Epoch 436: 0.0004587496514432132 0.03753247484564781 9.900000000000001e-05\n",
      "Epoch 437: 0.00045546673936769366 0.03755434602499008 9.900000000000001e-05\n",
      "Epoch 438: 0.0004513296007644385 0.03756609559059143 9.900000000000001e-05\n",
      "Epoch 439: 0.00044835027074441314 0.037562236189842224 9.900000000000001e-05\n",
      "Epoch 440: 0.00044456322211772203 0.037491463124752045 9.900000000000001e-05\n",
      "Epoch 441: 0.00044137571239843965 0.0375201590359211 9.900000000000001e-05\n",
      "Epoch 442: 0.00043845479376614094 0.03751097247004509 9.900000000000001e-05\n",
      "Epoch 443: 0.00043506352812983096 0.037478264421224594 9.900000000000001e-05\n",
      "Epoch 444: 0.00043153471779078245 0.037449005991220474 9.900000000000001e-05\n",
      "Epoch 445: 0.00042848166776821017 0.03748849406838417 9.900000000000001e-05\n",
      "Epoch 446: 0.00042539704008959234 0.037443384528160095 9.900000000000001e-05\n",
      "Epoch 447: 0.00042266209493391216 0.03743469715118408 9.900000000000001e-05\n",
      "Epoch 448: 0.0004206619050819427 0.037415310740470886 9.900000000000001e-05\n",
      "Epoch 449: 0.00041874274029396474 0.03742905706167221 9.900000000000001e-05\n",
      "Epoch 450: 0.00041734427213668823 0.03738616406917572 9.900000000000001e-05\n",
      "Epoch 451: 0.0004143918049521744 0.03740625083446503 9.900000000000001e-05\n",
      "Epoch 452: 0.0004104825493413955 0.03737847879528999 9.900000000000001e-05\n",
      "Epoch 453: 0.0004058307968080044 0.03742421790957451 9.900000000000001e-05\n",
      "Epoch 454: 0.0004025196540169418 0.03737429156899452 9.900000000000001e-05\n",
      "Epoch 455: 0.00039939326234161854 0.03744183108210564 9.900000000000001e-05\n",
      "Epoch 456: 0.0003969747922383249 0.037369854748249054 9.900000000000001e-05\n",
      "Epoch 457: 0.0003953915147576481 0.037435997277498245 9.900000000000001e-05\n",
      "Epoch 458: 0.0003930831444449723 0.037390004843473434 9.900000000000001e-05\n",
      "Epoch 459: 0.00039012651541270316 0.03740992769598961 9.900000000000001e-05\n",
      "Epoch 460: 0.00038627986214123666 0.037405770272016525 9.900000000000001e-05\n",
      "Epoch 461: 0.0003831811773125082 0.037454698234796524 9.900000000000001e-05\n",
      "Epoch 462: 0.00038170695188455284 0.037390705198049545 9.900000000000001e-05\n",
      "Epoch 463: 0.00038097446667961776 0.03743515536189079 9.900000000000001e-05\n",
      "Epoch 464: 0.00037935905857011676 0.03743257746100426 9.900000000000001e-05\n",
      "Epoch 465: 0.00037564258673228323 0.03744855895638466 9.900000000000001e-05\n",
      "Epoch 466: 0.0003714771301019937 0.03742942586541176 9.900000000000001e-05\n",
      "Epoch 467: 0.0003673335595522076 0.03747075796127319 9.900000000000001e-05\n",
      "Epoch 468: 0.0003644713433459401 0.03743186593055725 9.900000000000001e-05\n",
      "Epoch 469: 0.0003628672566264868 0.03747526928782463 9.900000000000001e-05\n",
      "Epoch 470: 0.00036103601451031864 0.03743593767285347 9.900000000000001e-05\n",
      "Epoch 471: 0.0003582702483981848 0.03748695179820061 9.900000000000001e-05\n",
      "Epoch 472: 0.0003553308779373765 0.038121093064546585 9.900000000000001e-05\n",
      "Epoch 473: 0.00035521568497642875 0.03768797963857651 9.900000000000001e-05\n",
      "Epoch 474: 0.00035470823058858514 0.03746602684259415 9.900000000000001e-05\n",
      "Epoch 475: 0.00035257148556411266 0.03738108277320862 9.900000000000001e-05\n",
      "Epoch 476: 0.00034987551043741405 0.03747851774096489 9.900000000000001e-05\n",
      "Epoch 477: 0.00034724315628409386 0.0374029278755188 9.900000000000001e-05\n",
      "Epoch 478: 0.00034299370599910617 0.03741578385233879 9.900000000000001e-05\n",
      "Epoch 479: 0.0003399361448828131 0.037467796355485916 9.900000000000001e-05\n",
      "Epoch 480: 0.0003371227066963911 0.03746674209833145 9.900000000000001e-05\n",
      "Epoch 481: 0.0003343861608300358 0.037471551448106766 9.900000000000001e-05\n",
      "Epoch 482: 0.0003322894626762718 0.03749619796872139 9.900000000000001e-05\n",
      "Epoch 483: 0.0003300729440525174 0.03753143176436424 9.900000000000001e-05\n",
      "Epoch 484: 0.0003284720878582448 0.03749461472034454 9.900000000000001e-05\n",
      "Epoch 485: 0.00032681727316230536 0.037575963884592056 9.900000000000001e-05\n",
      "Epoch 486: 0.0003243776154704392 0.037518564611673355 9.900000000000001e-05\n",
      "Epoch 487: 0.00032221354194916785 0.0375605970621109 9.900000000000001e-05\n",
      "Epoch 488: 0.0003189340641256422 0.037550803273916245 9.900000000000001e-05\n",
      "Epoch 489: 0.00031636905623599887 0.037592995911836624 9.900000000000001e-05\n",
      "Epoch 490: 0.0003152140125166625 0.03757116571068764 9.900000000000001e-05\n",
      "Epoch 491: 0.0003140093176625669 0.0375903844833374 9.900000000000001e-05\n",
      "Epoch 492: 0.00031252321787178516 0.03755313530564308 9.900000000000001e-05\n",
      "Epoch 493: 0.00031043446506373584 0.0375385656952858 9.900000000000001e-05\n",
      "Epoch 494: 0.0003072898252867162 0.03751346468925476 9.900000000000001e-05\n",
      "Epoch 495: 0.00030381366377696395 0.037558045238256454 9.900000000000001e-05\n",
      "Epoch 496: 0.0003011720546055585 0.03751659020781517 9.900000000000001e-05\n",
      "Epoch 497: 0.0002987984917126596 0.03754251450300217 9.900000000000001e-05\n",
      "Epoch 498: 0.0002970311907120049 0.037523508071899414 9.900000000000001e-05\n",
      "Epoch 499: 0.0002954293158836663 0.03754932060837746 9.900000000000001e-05\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "def train():\n",
    "    path = \"/home/cusps/python/ML_nanowire/disorder/delta_chem_dis/delta_chem_dis\"\n",
    "    \n",
    "    train_dataset = MyDataset1(input_file = path + '/train/train_data.npy', label_file= path + '/train/train_labels.npy')\n",
    "    vali_dataset = MyDataset1(input_file = path + '/vali/vali_data.npy', label_file= path + '/vali/vali_labels.npy')\n",
    "\n",
    "\n",
    "    net = MyNet().cuda()\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = 10000, shuffle=True, pin_memory=True)\n",
    "    vali_dataloader = DataLoader(vali_dataset, batch_size = 10000, shuffle=False, pin_memory=True)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.99, patience=100, threshold=1e-4, \n",
    "                                                           threshold_mode='rel',cooldown=100, min_lr=1e-20)\n",
    "\n",
    "\n",
    "\n",
    "    record_vali_loss = []\n",
    "    for epoch in range(500):\n",
    "        train_loss = []\n",
    "        vali_loss = []\n",
    "\n",
    "        net.train()\n",
    "        for batch in train_dataloader:\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            out = net(input)\n",
    "            loss = torch.nn.MSELoss()(out, batch['label'].cuda())\n",
    "            # loss = torch.nn.functional.l1_loss(out, batch['label'].cuda())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "            \n",
    "        net.eval()\n",
    "        for batch in vali_dataloader:\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            out = net(input)\n",
    "            loss = torch.nn.MSELoss()(out, batch['label'].cuda())\n",
    "            # loss = torch.nn.functional.l1_loss(out, batch['label'].cuda())\n",
    "            vali_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "            \n",
    "        print(f\"Epoch {epoch}: {np.mean(train_loss)} {np.mean(vali_loss)} {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "\n",
    "        writer.add_scalar(\"loss: \",np.mean(train_loss), global_step=epoch)\n",
    "        writer.add_scalar(\"learn rate: \",optimizer.state_dict()['param_groups'][0]['lr'], global_step=epoch)\n",
    "\n",
    "        scheduler.step(np.mean(vali_loss))\n",
    "        \n",
    "        record_vali_loss.append(np.mean(vali_loss))\n",
    "        writer.add_scalar(\"validation loss: \",np.mean(vali_loss), global_step=epoch)\n",
    "\n",
    "\n",
    "        \n",
    "        if  record_vali_loss[epoch] == min(record_vali_loss):\n",
    "            torch.save(net.state_dict(), 'model_weights.pth')\n",
    "            print(\"save model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7efd47",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd047b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk\n",
    "\n",
    "loss_file = '/home/cusps/python/ML_disorder/mlp/loss/Jun19_11-30-49_cusps/'\n",
    "\n",
    "# Read the CSV files\n",
    "train_loss = pd.read_csv(loss_file + 'train.csv')\n",
    "x1 = train_loss.iloc[:, 1]  \n",
    "y1 = train_loss.iloc[:, 2]  \n",
    "\n",
    "\n",
    "vali_loss = pd.read_csv(loss_file + 'vali.csv')\n",
    "x2 = vali_loss.iloc[:, 1]  \n",
    "y2 = vali_loss.iloc[:, 2]  \n",
    "\n",
    "\n",
    "# 绘制图形\n",
    "plt.plot(x1, y1,label='train')\n",
    "plt.plot(x2, y2,label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834bcbd-57b2-4bdc-ad78-436e0ef3c1b6",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b6ee5-778f-45b0-93a0-4e3e9304704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MyNet\n",
    "from dataset import MyDataset1\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def test():\n",
    "    # 加载测试数据集\n",
    "    path = \"/home/cusps/python/ML_disorder/new_data\"\n",
    "\n",
    "    test_dataset = MyDataset1(input_file = path + '/test/test_data.npy',\n",
    "                              label_file = path + '/test/test_labels.npy')\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=5000, shuffle=False)\n",
    "\n",
    "    # 初始化模型\n",
    "    net = MyNet(seq_num=test_dataset.in_shape[1], out_dim=test_dataset.out_shape[1], hidden_dim=1024).cuda().eval()\n",
    "    \n",
    "    # 加载训练好的权重\n",
    "    net.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "    \n",
    "    # 测试模型\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            predictions = net(input)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(batch['label'].numpy())\n",
    "    \n",
    "    # 合并所有批次的预测结果和真实标签\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "\n",
    "    mse_loss = np.mean((all_predictions - all_labels) ** 2)\n",
    "    print(f\"Test MSE Loss: {mse_loss}\")\n",
    "\n",
    "    # l1_loss = np.mean(np.abs(all_predictions - all_labels))\n",
    "    # print(f\"Test l1 Loss: {l1_loss}\")\n",
    "    \n",
    "    \n",
    "    # 可选：保存预测结果\n",
    "    np.save('test_predictions.npy', all_predictions)\n",
    "    np.save('test_labels.npy', all_labels)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4086251",
   "metadata": {},
   "source": [
    "# 衡量预测结果的好坏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c8878a-daf9-4619-8890-a6d19596243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''inner product fidelity'''\n",
    "\n",
    "%matplotlib tk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "exact_label = np.load(\"test_labels.npy\")\n",
    "prediction = np.load(\"test_predictions.npy\")\n",
    "\n",
    "\n",
    "F_list = []\n",
    "def calculate_F(A, B):\n",
    "    # return np.dot(A, B)/np.sqrt(np.dot(A, A) * np.dot(B, B))\n",
    "\n",
    "    return np.dot(A, B)/ (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "\n",
    "for i in range(exact_label.shape[0]):\n",
    "    F = calculate_F(exact_label[i], prediction[i])\n",
    "    F_list.append(F)\n",
    "\n",
    "print(min(F_list),max(F_list),np.mean(F_list))\n",
    "print(len(F_list),'\\n')\n",
    "\n",
    "\n",
    "'''所有F的分布'''\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(F_list)),F_list,'o')\n",
    "plt.xlim(0,len(F_list))\n",
    "plt.title('F distribution')\n",
    "plt.ylabel('F value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''R2'''\n",
    "def calculate_r2(label, pred):\n",
    "\n",
    "    label = np.array(label)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    ss_res = np.sum((label - pred) ** 2)\n",
    "    ss_tot = np.sum((label - np.mean(label, axis=0) ) ** 2)\n",
    "\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "r2 = calculate_r2(exact_label, prediction)\n",
    "print(f\"R2 = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f46e6-8532-47c7-b4bb-9a85ebb185b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''测试无disorder的表现情况'''\n",
    "\n",
    "from model import MyNet\n",
    "from dataset import MyDataset1\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def test():\n",
    "\n",
    "    # 初始化模型\n",
    "    net = MyNet(seq_num=test_dataset.in_shape[1], out_dim=test_dataset.out_shape[1], hidden_dim=1024).cuda().eval()\n",
    "    \n",
    "    # 加载训练好的权重\n",
    "    net.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "    \n",
    "    # 测试模型\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            predictions = net(input)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(batch['label'].numpy())\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kwant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
