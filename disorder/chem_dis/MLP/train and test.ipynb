{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6e665f-e083-4d42-8c4d-b2846146aab1",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a4324e-68a4-4f9c-9b38-abfdcbf27026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 5, 15, 15, 4)\n",
      "(8000, 5)\n",
      "(8000, 4500)\n",
      "(8000, 5)\n",
      "0 torch.Size([8000, 4500]) torch.Size([8000, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset1(Dataset):\n",
    "    def __init__(self, input_file='conductance_datasets.npy', label_file='Y_labels.npy'):\n",
    "        datas = np.load(input_file)\n",
    "        self.ori_in_shape = datas.shape\n",
    "        datas = datas.reshape(datas.shape[0], -1)\n",
    "        self.in_shape = datas.shape\n",
    "        self.datas = datas\n",
    "\n",
    "\n",
    "        labels = np.load(label_file)\n",
    "        self.ori_out_shape = labels.shape\n",
    "        labels = labels.reshape(labels.shape[0], -1)\n",
    "        self.out_shape = labels.shape\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input\": torch.from_numpy(self.datas[idx]).float(), \"label\":  torch.from_numpy(self.labels[idx]).float()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = \"/home/cusps/python/ML_DMF/300site /disorder/chem_dis/chem_dis\"\n",
    "\n",
    "    dataset = MyDataset1(input_file = path + '/train/train_data.npy', label_file= path + '/train/train_labels.npy')\n",
    "    # dataset = MyDataset1(input_file = path + '/test/test_data.npy', label_file= path + '/test/test_labels.npy')\n",
    "    # dataset = MyDataset1(input_file = path + '/vali/vali_data.npy', label_file= path + '/vali/vali_labels.npy')\n",
    "\n",
    "    print(dataset.ori_in_shape)\n",
    "    print(dataset.ori_out_shape)\n",
    "\n",
    "    print(dataset.in_shape)\n",
    "    print(dataset.out_shape)\n",
    "\n",
    "    data_dataloader = DataLoader(dataset, batch_size=10000, shuffle=True)\n",
    "    for i, d in enumerate(data_dataloader):\n",
    "        print(i, d['input'].shape, d['label'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb8eba-3c2b-4e0a-a811-88a3317266a5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86dc10e-a8e6-439e-870a-ec219cd59368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self, seq_num=4500, out_dim=5, hidden_dim=1024) -> None:\n",
    "        super(MyNet, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(seq_num, hidden_dim), torch.nn.ReLU(), torch.nn.BatchNorm1d(hidden_dim),#torch.nn.Dropout(0.1),\n",
    "                                       torch.nn.Linear(hidden_dim, hidden_dim // 2), torch.nn.ReLU(), torch.nn.BatchNorm1d(hidden_dim // 2),#torch.nn.Dropout(0.1),\n",
    "                                       torch.nn.Linear(hidden_dim // 2, hidden_dim // 4), torch.nn.ReLU(), torch.nn.BatchNorm1d(hidden_dim // 4),#torch.nn.Dropout(0.1),\n",
    "                                       torch.nn.Linear(hidden_dim // 4, hidden_dim // 8), torch.nn.ReLU(), torch.nn.BatchNorm1d(hidden_dim // 8),\n",
    "                                       torch.nn.Linear(hidden_dim // 8, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x.squeeze())\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = MyNet().cuda()\n",
    "    input = torch.rand([16, 4500]).cuda()\n",
    "    out = net(input)\n",
    "    print(out.shape)\n",
    "\n",
    "    # # 输出各层的参数数量\n",
    "    # for name, param in net.named_parameters():\n",
    "    #     print(f\"Layer: {name}, Parameters: {param.numel()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cc707-b33a-47ec-a6b5-ccfbb81a01dd",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216d735d-b160-4d80-baa0-0883e890561c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.621051549911499 0.25437214970588684 0.0001\n",
      "save model\n",
      "Epoch 1: 0.332367867231369 0.2533501982688904 0.0001\n",
      "save model\n",
      "Epoch 2: 0.27765536308288574 0.25301316380500793 0.0001\n",
      "save model\n",
      "Epoch 3: 0.22882363200187683 0.25291749835014343 0.0001\n",
      "save model\n",
      "Epoch 4: 0.18649666011333466 0.25296732783317566 0.0001\n",
      "Epoch 5: 0.15902408957481384 0.25308915972709656 0.0001\n",
      "Epoch 6: 0.1415587067604065 0.2530660331249237 0.0001\n",
      "Epoch 7: 0.1283089518547058 0.25286030769348145 0.0001\n",
      "save model\n",
      "Epoch 8: 0.1169227734208107 0.2525079846382141 0.0001\n",
      "save model\n",
      "Epoch 9: 0.10636693984270096 0.2519555985927582 0.0001\n",
      "save model\n",
      "Epoch 10: 0.09692874550819397 0.25108587741851807 0.0001\n",
      "save model\n",
      "Epoch 11: 0.08908429741859436 0.24985052645206451 0.0001\n",
      "save model\n",
      "Epoch 12: 0.0828445628285408 0.2482749968767166 0.0001\n",
      "save model\n",
      "Epoch 13: 0.07759243249893188 0.24642272293567657 0.0001\n",
      "save model\n",
      "Epoch 14: 0.07291198521852493 0.24434418976306915 0.0001\n",
      "save model\n",
      "Epoch 15: 0.06869753450155258 0.24205197393894196 0.0001\n",
      "save model\n",
      "Epoch 16: 0.06484226137399673 0.2395678609609604 0.0001\n",
      "save model\n",
      "Epoch 17: 0.06130730360746384 0.23684100806713104 0.0001\n",
      "save model\n",
      "Epoch 18: 0.05798640102148056 0.23386377096176147 0.0001\n",
      "save model\n",
      "Epoch 19: 0.05495140329003334 0.23063476383686066 0.0001\n",
      "save model\n",
      "Epoch 20: 0.05225050076842308 0.2271670401096344 0.0001\n",
      "save model\n",
      "Epoch 21: 0.04984230920672417 0.22336600720882416 0.0001\n",
      "save model\n",
      "Epoch 22: 0.04766266047954559 0.21926625072956085 0.0001\n",
      "save model\n",
      "Epoch 23: 0.04560051113367081 0.2148188352584839 0.0001\n",
      "save model\n",
      "Epoch 24: 0.043614134192466736 0.20994824171066284 0.0001\n",
      "save model\n",
      "Epoch 25: 0.041715752333402634 0.20455971360206604 0.0001\n",
      "save model\n",
      "Epoch 26: 0.03993970900774002 0.1987096071243286 0.0001\n",
      "save model\n",
      "Epoch 27: 0.03828935697674751 0.19241662323474884 0.0001\n",
      "save model\n",
      "Epoch 28: 0.03674735128879547 0.1859140247106552 0.0001\n",
      "save model\n",
      "Epoch 29: 0.035310376435518265 0.1792883425951004 0.0001\n",
      "save model\n",
      "Epoch 30: 0.03396151587367058 0.17260825634002686 0.0001\n",
      "save model\n",
      "Epoch 31: 0.03269551321864128 0.16592372953891754 0.0001\n",
      "save model\n",
      "Epoch 32: 0.03148970380425453 0.15928712487220764 0.0001\n",
      "save model\n",
      "Epoch 33: 0.030329283326864243 0.1527373194694519 0.0001\n",
      "save model\n",
      "Epoch 34: 0.02922094613313675 0.14618900418281555 0.0001\n",
      "save model\n",
      "Epoch 35: 0.02816949412226677 0.13958843052387238 0.0001\n",
      "save model\n",
      "Epoch 36: 0.02717716619372368 0.13294431567192078 0.0001\n",
      "save model\n",
      "Epoch 37: 0.026242772117257118 0.12634611129760742 0.0001\n",
      "save model\n",
      "Epoch 38: 0.025361036881804466 0.11994867771863937 0.0001\n",
      "save model\n",
      "Epoch 39: 0.024521270766854286 0.11393875628709793 0.0001\n",
      "save model\n",
      "Epoch 40: 0.02370792254805565 0.10843995958566666 0.0001\n",
      "save model\n",
      "Epoch 41: 0.022940095514059067 0.10344491899013519 0.0001\n",
      "save model\n",
      "Epoch 42: 0.022207818925380707 0.09884001314640045 0.0001\n",
      "save model\n",
      "Epoch 43: 0.021504582837224007 0.09451223164796829 0.0001\n",
      "save model\n",
      "Epoch 44: 0.02082863077521324 0.09039708971977234 0.0001\n",
      "save model\n",
      "Epoch 45: 0.020251715555787086 0.08653904497623444 0.0001\n",
      "save model\n",
      "Epoch 46: 0.019583959132432938 0.08292206376791 0.0001\n",
      "save model\n",
      "Epoch 47: 0.01898760162293911 0.07954347878694534 0.0001\n",
      "save model\n",
      "Epoch 48: 0.0184449702501297 0.07640725374221802 0.0001\n",
      "save model\n",
      "Epoch 49: 0.017898468300700188 0.0735674574971199 0.0001\n",
      "save model\n",
      "Epoch 50: 0.017389927059412003 0.07106347382068634 0.0001\n",
      "save model\n",
      "Epoch 51: 0.01689746044576168 0.06882481276988983 0.0001\n",
      "save model\n",
      "Epoch 52: 0.016422390937805176 0.06669933348894119 0.0001\n",
      "save model\n",
      "Epoch 53: 0.015973057597875595 0.0646791085600853 0.0001\n",
      "save model\n",
      "Epoch 54: 0.015542687848210335 0.06279833614826202 0.0001\n",
      "save model\n",
      "Epoch 55: 0.0151376873254776 0.061115000396966934 0.0001\n",
      "save model\n",
      "Epoch 56: 0.014720425009727478 0.05963961035013199 0.0001\n",
      "save model\n",
      "Epoch 57: 0.014334925450384617 0.05835757404565811 0.0001\n",
      "save model\n",
      "Epoch 58: 0.013961663469672203 0.05722837895154953 0.0001\n",
      "save model\n",
      "Epoch 59: 0.013603966683149338 0.05623979493975639 0.0001\n",
      "save model\n",
      "Epoch 60: 0.013258611783385277 0.05542388930916786 0.0001\n",
      "save model\n",
      "Epoch 61: 0.01292309258133173 0.05477409064769745 0.0001\n",
      "save model\n",
      "Epoch 62: 0.012597342021763325 0.0542299747467041 0.0001\n",
      "save model\n",
      "Epoch 63: 0.012283604592084885 0.05373772978782654 0.0001\n",
      "save model\n",
      "Epoch 64: 0.011981803923845291 0.053283825516700745 0.0001\n",
      "save model\n",
      "Epoch 65: 0.011690599843859673 0.05289028212428093 0.0001\n",
      "save model\n",
      "Epoch 66: 0.011430833488702774 0.052638646215200424 0.0001\n",
      "save model\n",
      "Epoch 67: 0.011132931336760521 0.05247464403510094 0.0001\n",
      "save model\n",
      "Epoch 68: 0.010868865996599197 0.05238107591867447 0.0001\n",
      "save model\n",
      "Epoch 69: 0.010613375343382359 0.05230718106031418 0.0001\n",
      "save model\n",
      "Epoch 70: 0.010366120375692844 0.05220740661025047 0.0001\n",
      "save model\n",
      "Epoch 71: 0.010126122273504734 0.05204647034406662 0.0001\n",
      "save model\n",
      "Epoch 72: 0.009893953800201416 0.05186072364449501 0.0001\n",
      "save model\n",
      "Epoch 73: 0.009668702259659767 0.05171578377485275 0.0001\n",
      "save model\n",
      "Epoch 74: 0.009450693614780903 0.0516633540391922 0.0001\n",
      "save model\n",
      "Epoch 75: 0.00923924520611763 0.05167757347226143 0.0001\n",
      "Epoch 76: 0.009034272283315659 0.05169835686683655 0.0001\n",
      "Epoch 77: 0.008835099637508392 0.05170712247490883 0.0001\n",
      "Epoch 78: 0.008641441352665424 0.051705993711948395 0.0001\n",
      "Epoch 79: 0.00845301616936922 0.051704827696084976 0.0001\n",
      "Epoch 80: 0.00827087089419365 0.05174323543906212 0.0001\n",
      "Epoch 81: 0.008093961514532566 0.05181058123707771 0.0001\n",
      "Epoch 82: 0.007921481505036354 0.05188228562474251 0.0001\n",
      "Epoch 83: 0.007753787096589804 0.05192970484495163 0.0001\n",
      "Epoch 84: 0.007590731605887413 0.05194941163063049 0.0001\n",
      "Epoch 85: 0.007431935053318739 0.05197359621524811 0.0001\n",
      "Epoch 86: 0.0072778379544615746 0.05201678350567818 0.0001\n",
      "Epoch 87: 0.007127787452191114 0.05208512395620346 0.0001\n",
      "Epoch 88: 0.006981391925364733 0.05212685465812683 0.0001\n",
      "Epoch 89: 0.006839014124125242 0.05219162628054619 0.0001\n",
      "Epoch 90: 0.006700044963508844 0.05223562940955162 0.0001\n",
      "Epoch 91: 0.006564370356500149 0.05226573348045349 0.0001\n",
      "Epoch 92: 0.006432665977627039 0.05229504406452179 0.0001\n",
      "Epoch 93: 0.006304190494120121 0.05230676755309105 0.0001\n",
      "Epoch 94: 0.006178977899253368 0.05234389007091522 0.0001\n",
      "Epoch 95: 0.006056675221771002 0.05239139124751091 0.0001\n",
      "Epoch 96: 0.005937438923865557 0.052423618733882904 0.0001\n",
      "Epoch 97: 0.005821087397634983 0.05243895202875137 0.0001\n",
      "Epoch 98: 0.00570798572152853 0.05246802419424057 0.0001\n",
      "Epoch 99: 0.0055967275984585285 0.052488066256046295 0.0001\n",
      "Epoch 100: 0.0054887025617063046 0.052488915622234344 0.0001\n",
      "Epoch 101: 0.005383296404033899 0.05248691886663437 0.0001\n",
      "Epoch 102: 0.005279975943267345 0.05246976017951965 0.0001\n",
      "Epoch 103: 0.005179314874112606 0.05248957499861717 0.0001\n",
      "Epoch 104: 0.005080546252429485 0.052509766072034836 0.0001\n",
      "Epoch 105: 0.00498452503234148 0.0524846650660038 0.0001\n",
      "Epoch 106: 0.004890906624495983 0.05249987542629242 0.0001\n",
      "Epoch 107: 0.004798696842044592 0.05248736962676048 0.0001\n",
      "Epoch 108: 0.004708907566964626 0.052460167557001114 0.0001\n",
      "Epoch 109: 0.00462129432708025 0.05245516076683998 0.0001\n",
      "Epoch 110: 0.004535818938165903 0.05243360623717308 0.0001\n",
      "Epoch 111: 0.00445192027837038 0.052470799535512924 0.0001\n",
      "Epoch 112: 0.004369373898953199 0.052505847066640854 0.0001\n",
      "Epoch 113: 0.00428947014734149 0.053666844964027405 0.0001\n",
      "Epoch 114: 0.004320409614592791 0.05252114683389664 0.0001\n",
      "Epoch 115: 0.004137074109166861 0.05252997204661369 0.0001\n",
      "Epoch 116: 0.004061894956976175 0.05247873440384865 0.0001\n",
      "Epoch 117: 0.003987859468907118 0.052525583654642105 0.0001\n",
      "Epoch 118: 0.003915061708539724 0.052574120461940765 0.0001\n",
      "Epoch 119: 0.003844769671559334 0.05256729573011398 0.0001\n",
      "Epoch 120: 0.003775255521759391 0.052576810121536255 0.0001\n",
      "Epoch 121: 0.003707434283569455 0.0525054931640625 0.0001\n",
      "Epoch 122: 0.003640898736193776 0.053808752447366714 0.0001\n",
      "Epoch 123: 0.003685993840917945 0.05246290937066078 0.0001\n",
      "Epoch 124: 0.0035174975637346506 0.052379000931978226 0.0001\n",
      "Epoch 125: 0.00345435319468379 0.05229669064283371 0.0001\n",
      "Epoch 126: 0.0033926109317690134 0.05229389667510986 0.0001\n",
      "Epoch 127: 0.0033327231649309397 0.05227665975689888 0.0001\n",
      "Epoch 128: 0.0032732984982430935 0.05226195976138115 0.0001\n",
      "Epoch 129: 0.003215470351278782 0.052266210317611694 0.0001\n",
      "Epoch 130: 0.0031591998413205147 0.052195675671100616 0.0001\n",
      "Epoch 131: 0.0031034040730446577 0.05212290585041046 0.0001\n",
      "Epoch 132: 0.0030494052916765213 0.05209067091345787 0.0001\n",
      "Epoch 133: 0.002995998365804553 0.052043408155441284 0.0001\n",
      "Epoch 134: 0.0029441046062856913 0.05202696472406387 0.0001\n",
      "Epoch 135: 0.0028929486870765686 0.051981858909130096 0.0001\n",
      "Epoch 136: 0.002842952962964773 0.05196543410420418 0.0001\n",
      "Epoch 137: 0.0027936305850744247 0.05197908729314804 0.0001\n",
      "Epoch 138: 0.0027456621173769236 0.05195091292262077 0.0001\n",
      "Epoch 139: 0.0026983253192156553 0.05196407809853554 0.0001\n",
      "Epoch 140: 0.0026522602420300245 0.0519595630466938 0.0001\n",
      "Epoch 141: 0.0026068626902997494 0.051948901265859604 0.0001\n",
      "Epoch 142: 0.0025623119436204433 0.051969870924949646 0.0001\n",
      "Epoch 143: 0.0025185653939843178 0.05194073170423508 0.0001\n",
      "Epoch 144: 0.002475922228768468 0.051909033209085464 0.0001\n",
      "Epoch 145: 0.0024339789524674416 0.05191042274236679 0.0001\n",
      "Epoch 146: 0.002392727881669998 0.05190214887261391 0.0001\n",
      "Epoch 147: 0.00235234247520566 0.051876794546842575 0.0001\n",
      "Epoch 148: 0.002312542637810111 0.05188480019569397 0.0001\n",
      "Epoch 149: 0.002273899968713522 0.05187249183654785 0.0001\n",
      "Epoch 150: 0.0022358004935085773 0.05188606306910515 0.0001\n",
      "Epoch 151: 0.0021981995087116957 0.051842838525772095 0.0001\n",
      "Epoch 152: 0.0021615561563521624 0.05181065574288368 0.0001\n",
      "Epoch 153: 0.0021253242157399654 0.05181503668427467 0.0001\n",
      "Epoch 154: 0.002090031048282981 0.05179639905691147 0.0001\n",
      "Epoch 155: 0.002055178862065077 0.051757603883743286 0.0001\n",
      "Epoch 156: 0.002020971616730094 0.051744066178798676 0.0001\n",
      "Epoch 157: 0.001987715717405081 0.0517265610396862 0.0001\n",
      "Epoch 158: 0.001954837469384074 0.05170298367738724 0.0001\n",
      "Epoch 159: 0.001922763418406248 0.051733165979385376 0.0001\n",
      "Epoch 160: 0.0018910735379904509 0.05169052630662918 0.0001\n",
      "Epoch 161: 0.0018600673647597432 0.05169157683849335 0.0001\n",
      "Epoch 162: 0.0018294802866876125 0.051652103662490845 0.0001\n",
      "save model\n",
      "Epoch 163: 0.0017993483925238252 0.0516437292098999 0.0001\n",
      "save model\n",
      "Epoch 164: 0.001769979833625257 0.05161932110786438 0.0001\n",
      "save model\n",
      "Epoch 165: 0.0017409351421520114 0.054612692445516586 0.0001\n",
      "Epoch 166: 0.0017693151021376252 0.05244198441505432 0.0001\n",
      "Epoch 167: 0.0017267012735828757 0.05301547795534134 0.0001\n",
      "Epoch 168: 0.0016869681421667337 0.05277348682284355 0.0001\n",
      "Epoch 169: 0.001653157058171928 0.052484165877103806 0.0001\n",
      "Epoch 170: 0.0016223754500970244 0.05227786302566528 0.0001\n",
      "Epoch 171: 0.0015938150463625789 0.05212714150547981 0.0001\n",
      "Epoch 172: 0.0015663510421290994 0.05194849893450737 0.0001\n",
      "Epoch 173: 0.0015395494410768151 0.051829587668180466 0.0001\n",
      "Epoch 174: 0.0015136171132326126 0.05172232538461685 0.0001\n",
      "Epoch 175: 0.001488701906055212 0.051659103482961655 0.0001\n",
      "Epoch 176: 0.0014638989232480526 0.05165131762623787 0.0001\n",
      "Epoch 177: 0.0014394110767170787 0.05163344740867615 0.0001\n",
      "Epoch 178: 0.0014158864505589008 0.05160947144031525 0.0001\n",
      "save model\n",
      "Epoch 179: 0.001392769394442439 0.05157987028360367 0.0001\n",
      "save model\n",
      "Epoch 180: 0.0013696140376850963 0.051623400300741196 0.0001\n",
      "Epoch 181: 0.0013473419239744544 0.05154018476605415 0.0001\n",
      "save model\n",
      "Epoch 182: 0.0013254813384264708 0.05146968737244606 0.0001\n",
      "save model\n",
      "Epoch 183: 0.0013034656876698136 0.051414135843515396 0.0001\n",
      "save model\n",
      "Epoch 184: 0.0012828659964725375 0.05138532444834709 0.0001\n",
      "save model\n",
      "Epoch 185: 0.0012618425535038114 0.05137953534722328 0.0001\n",
      "save model\n",
      "Epoch 186: 0.0012411880306899548 0.05136637017130852 0.0001\n",
      "save model\n",
      "Epoch 187: 0.0012217825278639793 0.05133383721113205 0.0001\n",
      "save model\n",
      "Epoch 188: 0.00120157387573272 0.05127221718430519 0.0001\n",
      "save model\n",
      "Epoch 189: 0.0011822219239547849 0.051260076463222504 0.0001\n",
      "save model\n",
      "Epoch 190: 0.0011635252740234137 0.05126458778977394 0.0001\n",
      "Epoch 191: 0.0011443814728409052 0.051298633217811584 0.0001\n",
      "Epoch 192: 0.0011263747001066804 0.0513153076171875 0.0001\n",
      "Epoch 193: 0.0011083808494731784 0.051279161125421524 0.0001\n",
      "Epoch 194: 0.0010906042298302054 0.0512160025537014 0.0001\n",
      "save model\n",
      "Epoch 195: 0.0010736507829278708 0.05121568590402603 0.0001\n",
      "save model\n",
      "Epoch 196: 0.0010564380791038275 0.051204778254032135 0.0001\n",
      "save model\n",
      "Epoch 197: 0.0010399711318314075 0.0511934868991375 0.0001\n",
      "save model\n",
      "Epoch 198: 0.0010235416702926159 0.051170140504837036 0.0001\n",
      "save model\n",
      "Epoch 199: 0.0010072584263980389 0.05117391049861908 0.0001\n",
      "Epoch 200: 0.0009916028939187527 0.05116219446063042 0.0001\n",
      "save model\n",
      "Epoch 201: 0.0009759128442965448 0.05117137357592583 0.0001\n",
      "Epoch 202: 0.0009607101092115045 0.05118938535451889 0.0001\n",
      "Epoch 203: 0.0009456967236474156 0.05722316727042198 0.0001\n",
      "Epoch 204: 0.0009708476718515158 0.05909254774451256 0.0001\n",
      "Epoch 205: 0.0009459382854402065 0.05368243157863617 0.0001\n",
      "Epoch 206: 0.0009226201800629497 0.052139684557914734 0.0001\n",
      "Epoch 207: 0.0009044971084222198 0.051841191947460175 0.0001\n",
      "Epoch 208: 0.0008878118242137134 0.05169232562184334 0.0001\n",
      "Epoch 209: 0.0008720085024833679 0.0515846312046051 0.0001\n",
      "Epoch 210: 0.0008574571111239493 0.051424119621515274 0.0001\n",
      "Epoch 211: 0.0008438783697783947 0.05130825191736221 0.0001\n",
      "Epoch 212: 0.0008306369418278337 0.051303815096616745 0.0001\n",
      "Epoch 213: 0.0008171340450644493 0.05123100429773331 0.0001\n",
      "Epoch 214: 0.0008039295207709074 0.051146987825632095 0.0001\n",
      "save model\n",
      "Epoch 215: 0.000790998456068337 0.051062796264886856 0.0001\n",
      "save model\n",
      "Epoch 216: 0.0007784774643369019 0.05104348808526993 0.0001\n",
      "save model\n",
      "Epoch 217: 0.000765928125474602 0.05110646039247513 0.0001\n",
      "Epoch 218: 0.0007538484642282128 0.051117099821567535 0.0001\n",
      "Epoch 219: 0.0007419550674967468 0.05111079663038254 0.0001\n",
      "Epoch 220: 0.0007300304132513702 0.05109309405088425 0.0001\n",
      "Epoch 221: 0.0007187179289758205 0.05110054835677147 0.0001\n",
      "Epoch 222: 0.000707217666786164 0.05456940084695816 0.0001\n",
      "Epoch 223: 0.0007259878912009299 0.05425245687365532 0.0001\n",
      "Epoch 224: 0.0007071084110066295 0.0530802346765995 0.0001\n",
      "Epoch 225: 0.0006896859267726541 0.05205484479665756 0.0001\n",
      "Epoch 226: 0.0006758985109627247 0.05150798335671425 0.0001\n",
      "Epoch 227: 0.0006629031267948449 0.05129297822713852 0.0001\n",
      "Epoch 228: 0.0006510649691335857 0.051195934414863586 0.0001\n",
      "Epoch 229: 0.0006404267041943967 0.051182299852371216 0.0001\n",
      "Epoch 230: 0.0006304793641902506 0.051193978637456894 0.0001\n",
      "Epoch 231: 0.0006205413374118507 0.051175571978092194 0.0001\n",
      "Epoch 232: 0.0006105541251599789 0.05115722492337227 0.0001\n",
      "Epoch 233: 0.0006008972995914519 0.051128700375556946 0.0001\n",
      "Epoch 234: 0.000591500080190599 0.051108360290527344 0.0001\n",
      "Epoch 235: 0.0005822131643071771 0.051128238439559937 0.0001\n",
      "Epoch 236: 0.0005731193232350051 0.05112481117248535 0.0001\n",
      "Epoch 237: 0.000564176996704191 0.05109449103474617 0.0001\n",
      "Epoch 238: 0.0005553331575356424 0.05106217414140701 0.0001\n",
      "Epoch 239: 0.000546855793800205 0.051053836941719055 0.0001\n",
      "Epoch 240: 0.0005382903036661446 0.051031697541475296 0.0001\n",
      "save model\n",
      "Epoch 241: 0.0005300394259393215 0.053425222635269165 0.0001\n",
      "Epoch 242: 0.000547991250641644 0.05681738257408142 0.0001\n",
      "Epoch 243: 0.0005339424824342132 0.05229418724775314 0.0001\n",
      "Epoch 244: 0.0005236822762526572 0.05221646651625633 0.0001\n",
      "Epoch 245: 0.0005088816396892071 0.051919665187597275 0.0001\n",
      "Epoch 246: 0.0005015254719182849 0.051532864570617676 0.0001\n",
      "Epoch 247: 0.0004913443117402494 0.05131586268544197 0.0001\n",
      "Epoch 248: 0.00048477144446223974 0.051281969994306564 0.0001\n",
      "Epoch 249: 0.0004764799086842686 0.0512210987508297 0.0001\n",
      "Epoch 250: 0.00046928119263611734 0.05110783129930496 0.0001\n",
      "Epoch 251: 0.00046105936053209007 0.051042065024375916 0.0001\n",
      "Epoch 252: 0.00045410130405798554 0.051014095544815063 0.0001\n",
      "save model\n",
      "Epoch 253: 0.0004468559636734426 0.050963256508111954 0.0001\n",
      "save model\n",
      "Epoch 254: 0.0004395351861603558 0.050865575671195984 0.0001\n",
      "save model\n",
      "Epoch 255: 0.0004330521041993052 0.05084741860628128 0.0001\n",
      "save model\n",
      "Epoch 256: 0.0004260693385731429 0.05089164897799492 0.0001\n",
      "Epoch 257: 0.0004197072994429618 0.050855740904808044 0.0001\n",
      "Epoch 258: 0.0004128491273149848 0.05082886666059494 0.0001\n",
      "save model\n",
      "Epoch 259: 0.0004069660499226302 0.050872232764959335 0.0001\n",
      "Epoch 260: 0.00040032705874182284 0.05091999098658562 0.0001\n",
      "Epoch 261: 0.0003947767545469105 0.05088887736201286 0.0001\n",
      "Epoch 262: 0.0003884413745254278 0.050856925547122955 0.0001\n",
      "Epoch 263: 0.0003828296030405909 0.050894446671009064 0.0001\n",
      "Epoch 264: 0.00037681846879422665 0.05093109607696533 0.0001\n",
      "Epoch 265: 0.00037143201916478574 0.05095933750271797 0.0001\n",
      "Epoch 266: 0.0003657991474028677 0.05096053704619408 0.0001\n",
      "Epoch 267: 0.0003603561199270189 0.05097072944045067 0.0001\n",
      "Epoch 268: 0.00035515218041837215 0.05093517154455185 0.0001\n",
      "Epoch 269: 0.0003498639853205532 0.050918418914079666 0.0001\n",
      "Epoch 270: 0.000344843661878258 0.050929710268974304 0.0001\n",
      "Epoch 271: 0.0003396902175154537 0.051540713757276535 0.0001\n",
      "Epoch 272: 0.00034530344419181347 0.05098205432295799 0.0001\n",
      "Epoch 273: 0.00033394742058590055 0.05094305798411369 0.0001\n",
      "Epoch 274: 0.00032694110996089876 0.050932932645082474 0.0001\n",
      "Epoch 275: 0.0003231590089853853 0.050968803465366364 0.0001\n",
      "Epoch 276: 0.0003173641161993146 0.05096115916967392 0.0001\n",
      "Epoch 277: 0.00031341423164121807 0.05097854882478714 0.0001\n",
      "Epoch 278: 0.0003082323237322271 0.051022641360759735 0.0001\n",
      "Epoch 279: 0.00030408668681047857 0.05103982612490654 0.0001\n",
      "Epoch 280: 0.00029947090661153197 0.05100962147116661 0.0001\n",
      "Epoch 281: 0.0002950446796603501 0.05105079710483551 0.0001\n",
      "Epoch 282: 0.0002909788745455444 0.0511273629963398 0.0001\n",
      "Epoch 283: 0.0002864619600586593 0.05100395902991295 0.0001\n",
      "Epoch 284: 0.00028253416530787945 0.05091294273734093 0.0001\n",
      "Epoch 285: 0.00027837982634082437 0.0508752204477787 0.0001\n",
      "Epoch 286: 0.0002744529629126191 0.050815630704164505 0.0001\n",
      "save model\n",
      "Epoch 287: 0.0002705163788050413 0.050776492804288864 0.0001\n",
      "save model\n",
      "Epoch 288: 0.00026674827677197754 0.050745077431201935 0.0001\n",
      "save model\n",
      "Epoch 289: 0.00026293168775737286 0.05077415332198143 0.0001\n",
      "Epoch 290: 0.00025924204965122044 0.050749991089105606 0.0001\n",
      "Epoch 291: 0.00025549178826622665 0.05073966085910797 0.0001\n",
      "save model\n",
      "Epoch 292: 0.0002518875407986343 0.05234205350279808 0.0001\n",
      "Epoch 293: 0.0002578072599135339 0.05074843764305115 0.0001\n",
      "Epoch 294: 0.00026158938999287784 0.050745293498039246 0.0001\n",
      "Epoch 295: 0.00024409379693679512 0.05074481666088104 0.0001\n",
      "Epoch 296: 0.0002410843299003318 0.05079362541437149 0.0001\n",
      "Epoch 297: 0.0002371071750530973 0.050806187093257904 0.0001\n",
      "Epoch 298: 0.00023369464906863868 0.050759103149175644 0.0001\n",
      "Epoch 299: 0.0002303840301465243 0.050760574638843536 0.0001\n",
      "Epoch 300: 0.0002268076059408486 0.0508003905415535 0.0001\n",
      "Epoch 301: 0.00022391154197975993 0.050831910222768784 0.0001\n",
      "Epoch 302: 0.00022013748821336776 0.05078073963522911 0.0001\n",
      "Epoch 303: 0.00021762942196801305 0.05074775591492653 0.0001\n",
      "Epoch 304: 0.00021387655579019338 0.05077365040779114 0.0001\n",
      "Epoch 305: 0.00021147851657588035 0.05081482231616974 0.0001\n",
      "Epoch 306: 0.00020799884805455804 0.0507863387465477 0.0001\n",
      "Epoch 307: 0.00020533909264486283 0.0507611520588398 0.0001\n",
      "Epoch 308: 0.00020230391237419099 0.050770316272974014 0.0001\n",
      "Epoch 309: 0.00019962732039857656 0.05081891268491745 0.0001\n",
      "Epoch 310: 0.00019679113756865263 0.050831444561481476 0.0001\n",
      "Epoch 311: 0.0001940725342137739 0.05086015537381172 0.0001\n",
      "Epoch 312: 0.00019146509293932468 0.05083654820919037 0.0001\n",
      "Epoch 313: 0.00018871582869905978 0.050844356417655945 0.0001\n",
      "Epoch 314: 0.0001861707423813641 0.050849877297878265 0.0001\n",
      "Epoch 315: 0.0001835852162912488 0.05086301267147064 0.0001\n",
      "Epoch 316: 0.00018108250515069813 0.050862230360507965 0.0001\n",
      "Epoch 317: 0.00017855274199973792 0.05087810754776001 0.0001\n",
      "Epoch 318: 0.0001761745661497116 0.0508737675845623 0.0001\n",
      "Epoch 319: 0.00017372365982737392 0.05086946487426758 0.0001\n",
      "Epoch 320: 0.00017142100841738284 0.050897516310214996 0.0001\n",
      "Epoch 321: 0.00016910162230487913 0.0508989654481411 0.0001\n",
      "Epoch 322: 0.00016686506569385529 0.05092085897922516 0.0001\n",
      "Epoch 323: 0.0001647231838433072 0.0510161854326725 0.0001\n",
      "Epoch 324: 0.0001645255833864212 0.05095558241009712 0.0001\n",
      "Epoch 325: 0.0001625075819902122 0.05096524953842163 0.0001\n",
      "Epoch 326: 0.00015824091678950936 0.050994206219911575 0.0001\n",
      "Epoch 327: 0.00015614154108334333 0.050990715622901917 0.0001\n",
      "Epoch 328: 0.00015402250573970377 0.050979580730199814 0.0001\n",
      "Epoch 329: 0.00015194359002634883 0.050987303256988525 0.0001\n",
      "Epoch 330: 0.00014993299555499107 0.050994448363780975 0.0001\n",
      "Epoch 331: 0.00014797084440942854 0.05101444572210312 0.0001\n",
      "Epoch 332: 0.00014602189185097814 0.05099596455693245 0.0001\n",
      "Epoch 333: 0.00014409252617042512 0.05102068930864334 0.0001\n",
      "Epoch 334: 0.00014219711010809988 0.05101737752556801 0.0001\n",
      "Epoch 335: 0.0001402676134603098 0.05102742835879326 0.0001\n",
      "Epoch 336: 0.00013840754400007427 0.05102187395095825 0.0001\n",
      "Epoch 337: 0.0001365603820886463 0.0510258711874485 0.0001\n",
      "Epoch 338: 0.00013474248407874256 0.05100322142243385 0.0001\n",
      "Epoch 339: 0.00013297297118697315 0.05101259425282478 0.0001\n",
      "Epoch 340: 0.00013120555377099663 0.05100211873650551 0.0001\n",
      "Epoch 341: 0.00012951352982781827 0.05101553723216057 0.0001\n",
      "Epoch 342: 0.00012782767589669675 0.05099964886903763 0.0001\n",
      "Epoch 343: 0.00012615436571650207 0.051022909581661224 0.0001\n",
      "Epoch 344: 0.00012451781367417425 0.0510123111307621 0.0001\n",
      "Epoch 345: 0.00012288357538636774 0.05102375149726868 0.0001\n",
      "Epoch 346: 0.00012130980030633509 0.05101008340716362 0.0001\n",
      "Epoch 347: 0.00011971764615736902 0.05103285238146782 0.0001\n",
      "Epoch 348: 0.00011813568562502041 0.0510224923491478 0.0001\n",
      "Epoch 349: 0.0001165869107353501 0.05152380093932152 0.0001\n",
      "Epoch 350: 0.00011969641491305083 0.051909323781728745 0.0001\n",
      "Epoch 351: 0.0001286570040974766 0.05102520063519478 0.0001\n",
      "Epoch 352: 0.00012392798089422286 0.05102066323161125 0.0001\n",
      "Epoch 353: 0.0001159871753770858 0.05103575438261032 0.0001\n",
      "Epoch 354: 0.00011239556624786928 0.05105284973978996 0.0001\n",
      "Epoch 355: 0.00011245645873714238 0.05107656866312027 0.0001\n",
      "Epoch 356: 0.00010915427264990285 0.051077306270599365 0.0001\n",
      "Epoch 357: 0.00010838486923603341 0.05102061107754707 0.0001\n",
      "Epoch 358: 0.00010647970339050516 0.05103393271565437 0.0001\n",
      "Epoch 359: 0.00010481785284355283 0.05107053369283676 0.0001\n",
      "Epoch 360: 0.00010371369717177004 0.05105917900800705 0.0001\n",
      "Epoch 361: 0.00010141384700546041 0.05102929100394249 0.0001\n",
      "Epoch 362: 0.00010078094055643305 0.051099710166454315 0.0001\n",
      "Epoch 363: 9.870195935945958e-05 0.05111033469438553 0.0001\n",
      "Epoch 364: 9.77090239757672e-05 0.051043733954429626 0.0001\n",
      "Epoch 365: 9.614355076337233e-05 0.05103873088955879 0.0001\n",
      "Epoch 366: 9.461776062380522e-05 0.05108816921710968 0.0001\n",
      "Epoch 367: 9.375700756208971e-05 0.05108397826552391 0.0001\n",
      "Epoch 368: 9.195454913424328e-05 0.05105309560894966 0.0001\n",
      "Epoch 369: 9.122076153289527e-05 0.05106223002076149 0.0001\n",
      "Epoch 370: 8.967561734607443e-05 0.05103704333305359 0.0001\n",
      "Epoch 371: 8.861227979650721e-05 0.05104365572333336 0.0001\n",
      "Epoch 372: 8.744552178541198e-05 0.051054392009973526 0.0001\n",
      "Epoch 373: 8.622626046417281e-05 0.05108840763568878 0.0001\n",
      "Epoch 374: 8.506266021868214e-05 0.05107305943965912 0.0001\n",
      "Epoch 375: 8.396702469326556e-05 0.05106636509299278 0.0001\n",
      "Epoch 376: 8.278181485366076e-05 0.05105661600828171 0.0001\n",
      "Epoch 377: 8.174183312803507e-05 0.05106191337108612 0.0001\n",
      "Epoch 378: 8.06205061962828e-05 0.05106436461210251 0.0001\n",
      "Epoch 379: 7.961061055539176e-05 0.051069475710392 0.0001\n",
      "Epoch 380: 7.854343130020425e-05 0.05109081417322159 0.0001\n",
      "Epoch 381: 7.756784179946408e-05 0.05109425261616707 0.0001\n",
      "Epoch 382: 7.659269613213837e-05 0.05109839513897896 0.0001\n",
      "Epoch 383: 7.561894744867459e-05 0.05106295645236969 0.0001\n",
      "Epoch 384: 7.46906953281723e-05 0.051092248409986496 0.0001\n",
      "Epoch 385: 7.370181992882863e-05 0.05110704153776169 0.0001\n",
      "Epoch 386: 7.277708209585398e-05 0.05110491067171097 0.0001\n",
      "Epoch 387: 7.181685941759497e-05 0.05109000951051712 0.0001\n",
      "Epoch 388: 7.08695879438892e-05 0.05110697075724602 0.0001\n",
      "Epoch 389: 6.998929166002199e-05 0.051104117184877396 0.0001\n",
      "Epoch 390: 6.910174852237105e-05 0.051114968955516815 0.0001\n",
      "Epoch 391: 6.821305578341708e-05 0.05110752582550049 0.0001\n",
      "Epoch 392: 6.735436909366399e-05 0.05109720677137375 0.0001\n",
      "Epoch 393: 6.651200965279713e-05 0.05109654366970062 9.900000000000001e-05\n",
      "Epoch 394: 6.565129297086969e-05 0.0511002354323864 9.900000000000001e-05\n",
      "Epoch 395: 6.486636266345158e-05 0.051120441406965256 9.900000000000001e-05\n",
      "Epoch 396: 6.406191096175462e-05 0.05112301558256149 9.900000000000001e-05\n",
      "Epoch 397: 6.325537106022239e-05 0.051133979111909866 9.900000000000001e-05\n",
      "Epoch 398: 6.249787838896737e-05 0.05114201083779335 9.900000000000001e-05\n",
      "Epoch 399: 6.173561996547505e-05 0.05115606263279915 9.900000000000001e-05\n",
      "Epoch 400: 6.098946323618293e-05 0.051146604120731354 9.900000000000001e-05\n",
      "Epoch 401: 6.029077121638693e-05 0.051166828721761703 9.900000000000001e-05\n",
      "Epoch 402: 5.956228051218204e-05 0.05115527659654617 9.900000000000001e-05\n",
      "Epoch 403: 5.881825563847087e-05 0.05117011442780495 9.900000000000001e-05\n",
      "Epoch 404: 5.805189357488416e-05 0.05115926265716553 9.900000000000001e-05\n",
      "Epoch 405: 5.731675628339872e-05 0.051166899502277374 9.900000000000001e-05\n",
      "Epoch 406: 5.658140071318485e-05 0.0511636883020401 9.900000000000001e-05\n",
      "Epoch 407: 5.587962732533924e-05 0.05108753591775894 9.900000000000001e-05\n",
      "Epoch 408: 5.58788342459593e-05 0.051164690405130386 9.900000000000001e-05\n",
      "Epoch 409: 5.4849231673870236e-05 0.051164496690034866 9.900000000000001e-05\n",
      "Epoch 410: 5.406541094998829e-05 0.051170092076063156 9.900000000000001e-05\n",
      "Epoch 411: 5.3364525228971615e-05 0.05117128789424896 9.900000000000001e-05\n",
      "Epoch 412: 5.275781586533412e-05 0.051170989871025085 9.900000000000001e-05\n",
      "Epoch 413: 5.205111301620491e-05 0.0511452741920948 9.900000000000001e-05\n",
      "Epoch 414: 5.141808287589811e-05 0.051147136837244034 9.900000000000001e-05\n",
      "Epoch 415: 5.081489507574588e-05 0.051155462861061096 9.900000000000001e-05\n",
      "Epoch 416: 5.01972135680262e-05 0.051172852516174316 9.900000000000001e-05\n",
      "Epoch 417: 4.9611135182203725e-05 0.05116359516978264 9.900000000000001e-05\n",
      "Epoch 418: 4.904636080027558e-05 0.05117795616388321 9.900000000000001e-05\n",
      "Epoch 419: 4.8398262151749805e-05 0.05116948112845421 9.900000000000001e-05\n",
      "Epoch 420: 4.779261871590279e-05 0.05117088928818703 9.900000000000001e-05\n",
      "Epoch 421: 4.717726551461965e-05 0.051162105053663254 9.900000000000001e-05\n",
      "Epoch 422: 4.657962927012704e-05 0.05116027221083641 9.900000000000001e-05\n",
      "Epoch 423: 4.599945532390848e-05 0.051145538687705994 9.900000000000001e-05\n",
      "Epoch 424: 4.544968032860197e-05 0.05150400102138519 9.900000000000001e-05\n",
      "Epoch 425: 4.6049295633565634e-05 0.051151782274246216 9.900000000000001e-05\n",
      "Epoch 426: 4.644422006094828e-05 0.05118675157427788 9.900000000000001e-05\n",
      "Epoch 427: 4.4156018702778965e-05 0.0511997789144516 9.900000000000001e-05\n",
      "Epoch 428: 4.363475090940483e-05 0.05117626488208771 9.900000000000001e-05\n",
      "Epoch 429: 4.311494194553234e-05 0.05116647481918335 9.900000000000001e-05\n",
      "Epoch 430: 4.2551750084385276e-05 0.051197607070207596 9.900000000000001e-05\n",
      "Epoch 431: 4.196524969302118e-05 0.051170095801353455 9.900000000000001e-05\n",
      "Epoch 432: 4.1462793888058513e-05 0.05117988586425781 9.900000000000001e-05\n",
      "Epoch 433: 4.098164936294779e-05 0.05118802934885025 9.900000000000001e-05\n",
      "Epoch 434: 4.045094829052687e-05 0.051193468272686005 9.900000000000001e-05\n",
      "Epoch 435: 3.9901740819914266e-05 0.051198527216911316 9.900000000000001e-05\n",
      "Epoch 436: 3.9437556552002206e-05 0.05120934918522835 9.900000000000001e-05\n",
      "Epoch 437: 3.895710324286483e-05 0.051194217056035995 9.900000000000001e-05\n",
      "Epoch 438: 3.8451111322501674e-05 0.05120041221380234 9.900000000000001e-05\n",
      "Epoch 439: 3.794912117882632e-05 0.051201555877923965 9.900000000000001e-05\n",
      "Epoch 440: 3.749237657757476e-05 0.051192063838243484 9.900000000000001e-05\n",
      "Epoch 441: 3.705862764036283e-05 0.05118964612483978 9.900000000000001e-05\n",
      "Epoch 442: 3.659512003650889e-05 0.051192425191402435 9.900000000000001e-05\n",
      "Epoch 443: 3.61481070285663e-05 0.051197350025177 9.900000000000001e-05\n",
      "Epoch 444: 3.571074194042012e-05 0.05119451880455017 9.900000000000001e-05\n",
      "Epoch 445: 3.530934191076085e-05 0.051204513758420944 9.900000000000001e-05\n",
      "Epoch 446: 3.4927335946122184e-05 0.05117829516530037 9.900000000000001e-05\n",
      "Epoch 447: 3.455426121945493e-05 0.0512041375041008 9.900000000000001e-05\n",
      "Epoch 448: 3.407856638659723e-05 0.05119903385639191 9.900000000000001e-05\n",
      "Epoch 449: 3.363542055012658e-05 0.05120643228292465 9.900000000000001e-05\n",
      "Epoch 450: 3.320030009490438e-05 0.051183395087718964 9.900000000000001e-05\n",
      "Epoch 451: 3.281636236351915e-05 0.05118679255247116 9.900000000000001e-05\n",
      "Epoch 452: 3.24070242641028e-05 0.05118852108716965 9.900000000000001e-05\n",
      "Epoch 453: 3.204199674655683e-05 0.05118517205119133 9.900000000000001e-05\n",
      "Epoch 454: 3.166469105053693e-05 0.05118926241993904 9.900000000000001e-05\n",
      "Epoch 455: 3.131839548586868e-05 0.05118069797754288 9.900000000000001e-05\n",
      "Epoch 456: 3.0942988814786077e-05 0.051192037761211395 9.900000000000001e-05\n",
      "Epoch 457: 3.0568146030418575e-05 0.05117914453148842 9.900000000000001e-05\n",
      "Epoch 458: 3.018411916855257e-05 0.05119749903678894 9.900000000000001e-05\n",
      "Epoch 459: 2.9805067242705263e-05 0.051201652735471725 9.900000000000001e-05\n",
      "Epoch 460: 2.943597428384237e-05 0.0511956661939621 9.900000000000001e-05\n",
      "Epoch 461: 2.9076474675093777e-05 0.051200609654188156 9.900000000000001e-05\n",
      "Epoch 462: 2.873926132451743e-05 0.05118662863969803 9.900000000000001e-05\n",
      "Epoch 463: 2.8420721719157882e-05 0.051204003393650055 9.900000000000001e-05\n",
      "Epoch 464: 2.812846469169017e-05 0.05118339881300926 9.900000000000001e-05\n",
      "Epoch 465: 2.785795550153125e-05 0.05121869966387749 9.900000000000001e-05\n",
      "Epoch 466: 2.7588033844949678e-05 0.05117657408118248 9.900000000000001e-05\n",
      "Epoch 467: 2.729299376369454e-05 0.051217272877693176 9.900000000000001e-05\n",
      "Epoch 468: 2.6902835088549182e-05 0.05119297280907631 9.900000000000001e-05\n",
      "Epoch 469: 2.654953277669847e-05 0.0512048564851284 9.900000000000001e-05\n",
      "Epoch 470: 2.625658453325741e-05 0.05120581388473511 9.900000000000001e-05\n",
      "Epoch 471: 2.6041130695375614e-05 0.05120173841714859 9.900000000000001e-05\n",
      "Epoch 472: 2.5768062187125906e-05 0.05121687054634094 9.900000000000001e-05\n",
      "Epoch 473: 2.5467732484685257e-05 0.05120031535625458 9.900000000000001e-05\n",
      "Epoch 474: 2.5065368390642107e-05 0.05120164155960083 9.900000000000001e-05\n",
      "Epoch 475: 2.471415609761607e-05 0.05120979994535446 9.900000000000001e-05\n",
      "Epoch 476: 2.4437071260763332e-05 0.051185667514801025 9.900000000000001e-05\n",
      "Epoch 477: 2.4198045139200985e-05 0.051219042390584946 9.900000000000001e-05\n",
      "Epoch 478: 2.3945802240632474e-05 0.051186442375183105 9.900000000000001e-05\n",
      "Epoch 479: 2.3615171812707558e-05 0.051210351288318634 9.900000000000001e-05\n",
      "Epoch 480: 2.3271024474524893e-05 0.05120234191417694 9.900000000000001e-05\n",
      "Epoch 481: 2.2943713702261448e-05 0.051209915429353714 9.900000000000001e-05\n",
      "Epoch 482: 2.266700721520465e-05 0.05120878294110298 9.900000000000001e-05\n",
      "Epoch 483: 2.2437487132265233e-05 0.05120297893881798 9.900000000000001e-05\n",
      "Epoch 484: 2.2242396880756132e-05 0.05120866745710373 9.900000000000001e-05\n",
      "Epoch 485: 2.2099355192040093e-05 0.051205869764089584 9.900000000000001e-05\n",
      "Epoch 486: 2.1998917873133905e-05 0.05119318887591362 9.900000000000001e-05\n",
      "Epoch 487: 2.191818020946812e-05 0.0512269027531147 9.900000000000001e-05\n",
      "Epoch 488: 2.1676656615454704e-05 0.05119038745760918 9.900000000000001e-05\n",
      "Epoch 489: 2.1262039808789268e-05 0.05121738091111183 9.900000000000001e-05\n",
      "Epoch 490: 2.0863390091108158e-05 0.05121108144521713 9.900000000000001e-05\n",
      "Epoch 491: 2.052191848633811e-05 0.05121266469359398 9.900000000000001e-05\n",
      "Epoch 492: 2.028242488449905e-05 0.05122265964746475 9.900000000000001e-05\n",
      "Epoch 493: 2.0106768715777434e-05 0.05119467154145241 9.900000000000001e-05\n",
      "Epoch 494: 1.9973200323875062e-05 0.051227159798145294 9.900000000000001e-05\n",
      "Epoch 495: 1.9801887901849113e-05 0.05119507759809494 9.900000000000001e-05\n",
      "Epoch 496: 1.954621620825492e-05 0.05122154951095581 9.900000000000001e-05\n",
      "Epoch 497: 1.9188748410670087e-05 0.05120233818888664 9.900000000000001e-05\n",
      "Epoch 498: 1.8858161638490856e-05 0.05122269317507744 9.900000000000001e-05\n",
      "Epoch 499: 1.861488817667123e-05 0.05122105032205582 9.900000000000001e-05\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "def train():\n",
    "    path = \"/home/cusps/python/ML_DMF/300site /disorder/chem_dis/chem_dis\"\n",
    "    \n",
    "    train_dataset = MyDataset1(input_file = path + '/train/train_data.npy', label_file= path + '/train/train_labels.npy')\n",
    "    vali_dataset = MyDataset1(input_file = path + '/vali/vali_data.npy', label_file= path + '/vali/vali_labels.npy')\n",
    "\n",
    "\n",
    "    net = MyNet().cuda()\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = 10000, shuffle=True, pin_memory=True)\n",
    "    vali_dataloader = DataLoader(vali_dataset, batch_size = 10000, shuffle=False, pin_memory=True)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.99, patience=100, threshold=1e-4, \n",
    "                                                           threshold_mode='rel',cooldown=100, min_lr=1e-20)\n",
    "\n",
    "\n",
    "\n",
    "    record_vali_loss = []\n",
    "    for epoch in range(500):\n",
    "        train_loss = []\n",
    "        vali_loss = []\n",
    "\n",
    "        net.train()\n",
    "        for batch in train_dataloader:\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            out = net(input)\n",
    "            loss = torch.nn.MSELoss()(out, batch['label'].cuda())\n",
    "            # loss = torch.nn.functional.l1_loss(out, batch['label'].cuda())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "            \n",
    "        net.eval()\n",
    "        for batch in vali_dataloader:\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            out = net(input)\n",
    "            loss = torch.nn.MSELoss()(out, batch['label'].cuda())\n",
    "            # loss = torch.nn.functional.l1_loss(out, batch['label'].cuda())\n",
    "            vali_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "            \n",
    "        print(f\"Epoch {epoch}: {np.mean(train_loss)} {np.mean(vali_loss)} {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "\n",
    "        writer.add_scalar(\"loss: \",np.mean(train_loss), global_step=epoch)\n",
    "        writer.add_scalar(\"learn rate: \",optimizer.state_dict()['param_groups'][0]['lr'], global_step=epoch)\n",
    "\n",
    "        scheduler.step(np.mean(vali_loss))\n",
    "        \n",
    "        record_vali_loss.append(np.mean(vali_loss))\n",
    "        writer.add_scalar(\"validation loss: \",np.mean(vali_loss), global_step=epoch)\n",
    "\n",
    "\n",
    "        \n",
    "        if  record_vali_loss[epoch] == min(record_vali_loss):\n",
    "            torch.save(net.state_dict(), 'model_weights.pth')\n",
    "            print(\"save model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7efd47",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd047b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk\n",
    "\n",
    "loss_file = '/home/cusps/python/ML_DMF/300site /disorder/chem_dis/MLP/runs/Jun30_09-46-00_cusps/loss/'\n",
    "# Read the CSV files\n",
    "train_loss = pd.read_csv(loss_file + 'train.csv')\n",
    "x1 = train_loss.iloc[:, 1]  \n",
    "y1 = train_loss.iloc[:, 2]  \n",
    "\n",
    "\n",
    "vali_loss = pd.read_csv(loss_file + 'vali.csv')\n",
    "x2 = vali_loss.iloc[:, 1]  \n",
    "y2 = vali_loss.iloc[:, 2]  \n",
    "\n",
    "\n",
    "# 绘制图形\n",
    "plt.plot(x1, y1,label='train')\n",
    "plt.plot(x2, y2,label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834bcbd-57b2-4bdc-ad78-436e0ef3c1b6",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "334b6ee5-778f-45b0-93a0-4e3e9304704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 0.045831348747015\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def test():\n",
    "    # 加载测试集\n",
    "    path = \"/home/cusps/python/ML_DMF/300site /disorder/chem_dis/chem_dis\"\n",
    "\n",
    "    test_dataset = MyDataset1(input_file = path + '/test/test_data.npy',\n",
    "                              label_file = path + '/test/test_labels.npy')\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=5000, shuffle=False)\n",
    "\n",
    "    # 初始化模型\n",
    "    net = MyNet(seq_num=test_dataset.in_shape[1], out_dim=test_dataset.out_shape[1], hidden_dim=1024).cuda().eval()\n",
    "    \n",
    "    # 加载训练好的权重\n",
    "    net.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "    \n",
    "    # 测试模型\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            predictions = net(input)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(batch['label'].numpy())\n",
    "    \n",
    "    # 合并所有批次的预测结果和真实标签\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "\n",
    "    mse_loss = np.mean((all_predictions - all_labels) ** 2)\n",
    "    print(f\"Test MSE Loss: {mse_loss}\")\n",
    "\n",
    "    # l1_loss = np.mean(np.abs(all_predictions - all_labels))\n",
    "    # print(f\"Test l1 Loss: {l1_loss}\")\n",
    "    \n",
    "    \n",
    "    # 可选：保存预测结果\n",
    "    np.save('test_predictions.npy', all_predictions)\n",
    "    np.save('test_labels.npy', all_labels)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4086251",
   "metadata": {},
   "source": [
    "# 衡量预测结果的好坏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c8878a-daf9-4619-8890-a6d19596243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''inner product fidelity'''\n",
    "\n",
    "%matplotlib tk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "exact_label = np.load(\"test_labels.npy\")\n",
    "prediction = np.load(\"test_predictions.npy\")\n",
    "\n",
    "\n",
    "F_list = []\n",
    "def calculate_F(A, B):\n",
    "    # return np.dot(A, B)/np.sqrt(np.dot(A, A) * np.dot(B, B))\n",
    "\n",
    "    return np.dot(A, B)/ (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "\n",
    "for i in range(exact_label.shape[0]):\n",
    "    F = calculate_F(exact_label[i], prediction[i])\n",
    "    F_list.append(F)\n",
    "\n",
    "print(min(F_list),max(F_list),np.mean(F_list))\n",
    "print(len(F_list),'\\n')\n",
    "\n",
    "\n",
    "'''所有F的分布'''\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(F_list)),F_list,'o')\n",
    "plt.xlim(0,len(F_list))\n",
    "plt.title('F distribution')\n",
    "plt.ylabel('F value')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''R2'''\n",
    "def calculate_r2(label, pred):\n",
    "\n",
    "    label = np.array(label)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    ss_res = np.sum((label - pred) ** 2)\n",
    "    ss_tot = np.sum((label - np.mean(label, axis=0) ) ** 2)\n",
    "\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "r2 = calculate_r2(exact_label, prediction)\n",
    "print(f\"R2 = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f46e6-8532-47c7-b4bb-9a85ebb185b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''测试无disorder的表现情况'''\n",
    "\n",
    "from model import MyNet\n",
    "from dataset import MyDataset1\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def test():\n",
    "\n",
    "    # 初始化模型\n",
    "    net = MyNet(seq_num=test_dataset.in_shape[1], out_dim=test_dataset.out_shape[1], hidden_dim=1024).cuda().eval()\n",
    "    \n",
    "    # 加载训练好的权重\n",
    "    net.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "    \n",
    "    # 测试模型\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input = batch['input'].unsqueeze(-1).cuda()\n",
    "            predictions = net(input)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(batch['label'].numpy())\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kwant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
